{"cells":[{"cell_type":"markdown","source":["## LIBRARIES"],"metadata":{"id":"7_tnlLjBsuAc"}},{"cell_type":"code","source":["!pip install contractions\n","!pip install gensim==\"4.1.2\"\n","!pip install sentence-transformers\n","!pip install scikit-learn-extra"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I-Mv8Qm1stp-","executionInfo":{"status":"ok","timestamp":1648822077904,"user_tz":-120,"elapsed":39167,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}},"outputId":"9bf38992-8ecc-413a-dba9-5408e596f819"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting contractions\n","  Downloading contractions-0.1.68-py2.py3-none-any.whl (8.1 kB)\n","Collecting textsearch>=0.0.21\n","  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n","Collecting pyahocorasick\n","  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n","\u001b[K     |████████████████████████████████| 106 kB 7.0 MB/s \n","\u001b[?25hCollecting anyascii\n","  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n","\u001b[K     |████████████████████████████████| 284 kB 38.6 MB/s \n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.0 contractions-0.1.68 pyahocorasick-1.4.4 textsearch-0.0.21\n","Collecting gensim==4.1.2\n","  Downloading gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n","\u001b[K     |████████████████████████████████| 24.1 MB 1.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.21.5)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (5.2.1)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.4.1)\n","Installing collected packages: gensim\n","  Attempting uninstall: gensim\n","    Found existing installation: gensim 3.6.0\n","    Uninstalling gensim-3.6.0:\n","      Successfully uninstalled gensim-3.6.0\n","Successfully installed gensim-4.1.2\n","Collecting sentence-transformers\n","  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 3.3 MB/s \n","\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 11.0 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.63.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 42.3 MB/s \n","\u001b[?25hCollecting huggingface-hub\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 3.9 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 64.2 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 54.7 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n","Collecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 50.8 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.7.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=29d98a348d57f18b6830f3b331393836e81e4f24eee10d12006beb5299e8f639\n","  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n","Successfully built sentence-transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.11.6 transformers-4.17.0\n","Collecting scikit-learn-extra\n","  Downloading scikit_learn_extra-0.2.0-cp37-cp37m-manylinux2010_x86_64.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-extra) (1.21.5)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-extra) (1.4.1)\n","Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-extra) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.1.0)\n","Installing collected packages: scikit-learn-extra\n","Successfully installed scikit-learn-extra-0.2.0\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"-RwwmSN1seAC","executionInfo":{"status":"ok","timestamp":1648822092378,"user_tz":-120,"elapsed":14480,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"outputs":[],"source":["import string\n","from nltk.util import ngrams\n","from tqdm import tqdm\n","import csv\n","import re\n","import string\n","import operator\n","from datetime import datetime\n","import glob\n","import unicodedata\n","import re\n","import contractions\n","from contractions import contractions_dict\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.util import ngrams\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","import pandas as pd\n","from tqdm import tqdm\n","from yellowbrick.text import FreqDistVisualizer\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","import numpy as np\n","import spacy\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","from gensim.models.coherencemodel import CoherenceModel\n","from gensim.models.nmf import Nmf\n","import pickle\n","from sentence_transformers import SentenceTransformer\n","from sklearn_extra.cluster import KMedoids\n","import keras\n","from keras.layers import Input, Dense\n","from keras.models import Model\n","import os\n","from keras import backend as K\n","import random as rn\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.decomposition import PCA\n","# import plotly.express as px\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","from gensim import corpora\n","import gensim\n","import numpy as np\n","import tensorflow as tf\n","import joblib\n","import warnings\n","warnings.filterwarnings('ignore', category=Warning)\n","\n","os.environ['PYTHONHASHSEED']='0'\n","os.environ['CUDA_VISIBLE_DEVICES']=''\n","os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n","os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\""]},{"cell_type":"code","execution_count":3,"metadata":{"id":"EhwQ3vpiseAI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648822094111,"user_tz":-120,"elapsed":1741,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}},"outputId":"373f9692-99fa-4bc6-be5e-6c98d17f781a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}],"source":["nltk.download('wordnet')\n","nltk.download('stopwords')  # GUARDARE PIU AVANZATE -> NN\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('words')"]},{"cell_type":"code","source":["pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n","pd.set_option('expand_frame_repr', False)"],"metadata":{"id":"ChP9te28s23L","executionInfo":{"status":"ok","timestamp":1648822094112,"user_tz":-120,"elapsed":7,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wV9VkMwNs4WF","executionInfo":{"status":"ok","timestamp":1648822117055,"user_tz":-120,"elapsed":22949,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}},"outputId":"075971ef-eb62-4f4d-c1bf-f12162307e17"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["!ls /content/gdrive/My\\ Drive/Colab\\ Notebooks/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h1ZQJRT3s6jm","executionInfo":{"status":"ok","timestamp":1648822117641,"user_tz":-120,"elapsed":591,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}},"outputId":"f4734ea6-2687-4ed0-a9f5-aeaabf3a682e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["'0 - RESULTS'   GEA    IL    OLD_ProdLDA      PIPELINE_SPLIT.ipynb   TEST.ipynb\n"," data_dir       GEAC   IM    OTHER\t      ProdLDA\t\t     WORD2VEC\n"," DeepLog        HW2    IoT   PIPE\t      T2V\n"," DSL\t        HW3    MML   PIPELINE.ipynb   Tesi\n"]}]},{"cell_type":"code","source":["%cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FcFUyUz2s8rH","executionInfo":{"status":"ok","timestamp":1648822117642,"user_tz":-120,"elapsed":6,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}},"outputId":"07dab825-eb59-4869-b900-d4c0aad5e632"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/Colab Notebooks\n"]}]},{"cell_type":"code","source":["pathname=\"/content/gdrive/MyDrive/Colab Notebooks/\""],"metadata":{"id":"2JZZQlEzs-Yo","executionInfo":{"status":"ok","timestamp":1648822117643,"user_tz":-120,"elapsed":6,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zf2DqR97seAJ"},"source":["## INPUT VARIABLES\n"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"M0IDfW3qseAL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648822761372,"user_tz":-120,"elapsed":20074,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}},"outputId":"164bc75c-312a-4c77-c523-cb63c65c4627"},"outputs":[{"name":"stdout","output_type":"stream","text":["Inser dataset name: Spark\n","Inset path input file: /content/gdrive/MyDrive/Colab Notebooks/PIPE/INPUT/Spark_input.log\n","Change path? [y/n]n\n","COMPLETE PATH: /content/gdrive/MyDrive/Colab Notebooks//content/gdrive/MyDrive/Colab Notebooks/PIPE/INPUT/Spark_input.log\n","Process with ORIGIN: n\n"]}],"source":["var=None\n","while var not in [\"Spark\",\"HDFS\"]:\n","    var = input(f\"Inser dataset name: \")\n","if var==\"Spark\":\n","    name=\"Spark\"\n","    model=None\n","else:\n","    name=\"HDFS\"\n","    model=None\n","\n","var=None\n","flag=True\n","path=None\n","\n","while flag:\n","  var = input(f\"Inset path input file: \")\n","  path=[var]\n","  var2= input(\"Change path? [y/n]\")\n","  if var2 in [\"no\",\"n\"]:\n","    flag=False  \n","print(f\"COMPLETE PATH: {pathname+var}\")\n","Origin=True\n","\n","\n","var=None\n","while var not in [\"yes\",\"no\",\"y\",\"n\"]:\n","    var = input(f\"Process with ORIGIN: \")\n","\n","if var in [\"yes\",\"y\"]:\n","    Origin=True\n","    folder = \"ORIGIN+MESSAGE\"\n","else:\n","    Origin=False\n","    folder = \"MESSAGE\""]},{"cell_type":"markdown","source":["/content/gdrive/MyDrive/Colab Notebooks/PIPE/INPUT/Spark_input.log"],"metadata":{"id":"hOaZQISBQrn-"}},{"cell_type":"markdown","metadata":{"id":"5-AvRxrKseAT"},"source":["## FUNCTIONS"]},{"cell_type":"markdown","source":["### DATA MANIPULATION"],"metadata":{"id":"ykIZLfnytLYo"}},{"cell_type":"markdown","source":["#### PROCESSING FUNCTIONS"],"metadata":{"id":"-F9dIrV7xD0e"}},{"cell_type":"code","source":["def ImportFiles():\n","    print(\"\\tIMPORT FILES\")\n","    filepath = f\"C:/Users/david/Desktop/Tesi/Dataset/{name}/{name}/*.log\"\n","    paths = glob.glob(filepath)\n","    # print(len(paths))\n","    if len(paths) == 0:\n","        filepath = f\"C:/Users/david/Desktop/Tesi/Dataset/{name}/{name}/*/*.log\"\n","        paths = glob.glob(filepath)\n","        # print(len(paths))\n","    # for i in paths:\n","    #     print(i)\n","    return paths"],"metadata":{"id":"OSBhyIUztT9V","executionInfo":{"status":"ok","timestamp":1648822765239,"user_tz":-120,"elapsed":278,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","execution_count":34,"metadata":{"id":"Pc-nrG7kseAW","executionInfo":{"status":"ok","timestamp":1648822766061,"user_tz":-120,"elapsed":467,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"outputs":[],"source":["def ReadFile(paths, index):\n","    #print(\"\\tREAD FILE\")\n","    path0 = paths[index]  # PER ORA CONSIDERO SOLO UN FILE DI LOG\n","    print(f\"\\t{path0}\")\n","    file = open(path0, 'r')\n","    lines = file.read().splitlines()\n","    file.close()\n","    i = 0\n","    outread = []\n","    for ln, line in enumerate(lines):\n","        if (len(line) > 0):\n","            if (line.startswith((' ', \"\\t\"))) or (line.startswith(('STARTUP_MSG', \"/*\", \"**\", \"SHUTDOWN_MSG\"))):\n","                None\n","            else:\n","                outread.append(line)\n","                i += 1\n","    return outread"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"YuJB2TjEseAY","executionInfo":{"status":"ok","timestamp":1648822766063,"user_tz":-120,"elapsed":9,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"outputs":[],"source":["def MakeDataframe(outread):\n","    #print(\"\\tMAKE PANDAS DATAFRAME\")\n","    records = []\n","    for i, line in enumerate(outread):\n","        tmp = []\n","        counter = 0\n","        substring = \"\"\n","        firstchar = True\n","        errorline = False\n","        for char in line:\n","            if firstchar:\n","                firstchar = False\n","                if char.isalpha():  # FIRST CHAR AT EACH LINE IS NUMBER OF THE DATE\n","                    print(f\"FIRST CHAR ERROR LINE: {i} - {char}\")\n","                    errorline = True\n","                    break\n","            if char == \" \" and counter < 4:\n","                tmp.append(substring)\n","                substring = \"\"\n","                counter += 1\n","            else:\n","                substring += char\n","        if counter >= 4:\n","            tmp.append(substring)\n","        if errorline:\n","            splits = line.split(\":\")\n","            print(len(splits))\n","            orig = splits[0]\n","            if len(splits) > 1:\n","                ms = \":\".join(splits[1:])\n","            else:\n","                ms = orig\n","                orig = \"\"\n","            tmp = [\" \", \" \", \" \", orig, ms]\n","            print(f\"ERROR LINE: {i} - {tmp}\\n\")\n","        if len(tmp) != 5:\n","            print(\"ERRORE\")\n","            print(line)\n","            print(f\"{tmp}\\n\")\n","        records.append(tmp)\n","    df = pd.DataFrame(records, columns=[\"DATE\", \"TIMESTAMP\", \"TYPE\", \"ORIGIN\", \"MESSAGE\"])\n","    if Origin:\n","        #print(\"\\t\\tWITH ORIGIN\")\n","        df2 = df.loc[:, [\"ORIGIN\", \"MESSAGE\"]]\n","        df2[\"STRING\"] = df2[\"ORIGIN\"] + \" \" + df2[\"MESSAGE\"]  # PROVARE SOLO MESSAGE -> NO DIFFERENCES(TO_SAVE)\n","        df2 = df2.loc[:, [\"STRING\"]]\n","    else:\n","        #print(\"\\t\\tWITHOUT ORIGIN\")\n","        df2 = df.loc[:, [\"MESSAGE\"]]\n","    # print(df2.head())\n","    # origins = df2[\"ORIGIN\"].tolist()\n","    # messages=df2[\"MESSAGE\"].tolist()\n","    # input_preproc=[]\n","    # for o,m in zip(origins, messages):\n","    #     row=o+\" \"+m\n","    #     input_preproc.append(row)\n","    input_preproc = df2.values.tolist()\n","    return input_preproc"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"Q1DPvB3VseAa","executionInfo":{"status":"ok","timestamp":1648822766063,"user_tz":-120,"elapsed":8,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"outputs":[],"source":["def RemoveAccent(input_preproc):\n","    def remove_accented_chars(text):\n","        text = (unicodedata\n","                .normalize('NFKD', text)\n","                .encode('ascii', 'ignore')\n","                .decode('utf-8', 'ignore'))\n","        return text\n","    print(\"\\tREMOVE POSSIBLE ACCENTED CHARACTERS\")\n","    no_accent = []\n","    for i, el in (enumerate(input_preproc)):\n","        tmp = remove_accented_chars(el[0])\n","        # if el!=tmp:\n","        #     print(\"MODIFIED\")\n","        #     print(f\"{i} - {el[0]}\")\n","        #     print(f\"{i} - {tmp}\\n\")\n","        no_accent.append(tmp)  # LOWERCASE, CONSIDERAZIONE CAMEL CAPITALIZER\n","    return no_accent"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"Z4_GnL--seAb","executionInfo":{"status":"ok","timestamp":1648822766442,"user_tz":-120,"elapsed":386,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"outputs":[],"source":["def RemoveSpecial(ccs):\n","    set_upp = set()\n","    def remove_special_characters(text, i):  # TRA 2 /\n","        brackets = False\n","        first_len = len(text)\n","        text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \" \", text)  # PARANTESI TONDE E QUADRE\n","        second_len = len(text)\n","        if first_len != second_len:\n","            brackets = True\n","        build = []\n","        possible_path = []\n","        SOURCE = True\n","        for el in text.split(\" \"):  # RIMOZIONE PATH\n","            if not SOURCE:  # PRIMO ELEMENTO SOURCE\n","                if \"/\" in el:\n","                    tmp_split = el.split(\"/\")\n","                    str1 = \"\".join([c for c in tmp_split[0] if c not in string.punctuation])\n","                    str2 = \"\".join([c for c in tmp_split[1] if c not in string.punctuation])\n","                    if (len(tmp_split) == 2 and (len(str1) == 1 or len(str2) == 1)):\n","                        if len(str1) == 1:\n","                            build.append(tmp_split[1])\n","                        else:\n","                            build.append(tmp_split[0])\n","                    else:\n","                        possible_path.append(el)\n","                else:\n","                    build.append(el)\n","            else:\n","                build.append(el)\n","            SOURCE = False\n","        text = \" \".join(build)\n","        text = re.sub(' +', ' ', text)\n","        build = []\n","        word_with_digit = []\n","        SOURCE = True\n","        for el in text.split(\" \"):\n","            if not SOURCE:\n","                array = re.findall(r'[0-9]+', el)\n","                if len(array) == 0:\n","                    build.append(el)\n","                else:\n","                    word_with_digit.append(el)\n","            else:\n","                build.append(el)\n","            SOURCE = False\n","        text = \" \".join(build)\n","        text = re.sub(r'[@|;|$]+', ' ', text)  # SPLIT BY TOKENS\n","        r1 = re.compile(r\"(([\\w]+(:|=))+(-|)([\\w]+))\")  # ([0-9]+|[A-Za-z-]+[0-9]+))\")  # npid=261268536, #pid=null\n","        build = []\n","        possible_word_eq_dp_num = []\n","        for el in text.split(\" \"):\n","            if r1.match(el):\n","                possible_word_eq_dp_num.append(el)\n","            else:\n","                build.append(el)\n","        text = \" \".join(build)\n","\n","        # text = re.sub('[^\\'a-zA-Z\\s]', ' ', text) #TENGO SOLO CARATTERI\n","        # r4 = re.compile(r\"'[a-zA-Z]+'\")\n","        # output = []\n","        # word_with_apex=[]\n","        # for word in text.split():\n","        #     if r4.match(word):\n","        #         word_with_apex.append(word)\n","        #         output.append(word.replace(\"'\", \"\"))\n","        #     else:\n","        #         output.append(word)\n","        # text=\" \".join(output)\n","        r4 = re.compile(r\"'[.\\w]+'\")\n","        output = []\n","        word_with_apex = []\n","        for word in text.split():\n","            if r4.match(word):\n","                if len(word) < 20:\n","                    word_with_apex.append(word)\n","                    output.append(word.replace(\"'\", \"\"))\n","            else:\n","                output.append(word)\n","        text = \" \".join(output)\n","        text = re.sub('[^\\'a-zA-Z\\s]', ' ', text)  # TENGO SOLO CARATTERI\n","        text = re.sub(r'[\\r|\\n|\\r\\n_|\\r\\n/]+', ' ', text)  # TOLGO POTENZIALI SIMBOLI RIMASTI\n","        text = text.replace(\"[\", \"\")\n","        text = text.replace(\"]\", \"\")\n","        text = re.sub(' +', ' ', text.strip())  # TOLGO SPAZI AGGIUNTIVI\n","        res = []\n","        split_upper = []\n","        for el in text.split(\" \"):\n","            res.append(el)\n","            prevUpp = 0\n","            uppstring = \"\"\n","            for char in el:\n","                if char.isupper():\n","                    prevUpp += 1\n","                    uppstring += char\n","                else:\n","                    if prevUpp >= 3:\n","                        uppstring = uppstring[0:len(uppstring) - 1]\n","                        tmp = el\n","                        lowstirng = tmp.replace(uppstring, \"\")\n","                        res.remove(el)\n","                        res.append(uppstring)\n","                        res.append(lowstirng)\n","                        split_upper.append(uppstring)\n","                        split_upper.append(lowstirng)\n","                        set_upp.add(el)\n","                    prevUpp = 0\n","                    uppstring = \"\"\n","        out = \" \".join(res)\n","        return out, possible_path, possible_word_eq_dp_num, word_with_digit, brackets, word_with_apex, split_upper\n","    print(\"\\tREMOVE SPECIAL\")\n","    no_special = []\n","    for i, el in (enumerate(ccs)):\n","        tmp, path, wen, wwd, br, wwa, su = remove_special_characters(el, i)\n","        #if len(su) > 0:  # and len(path)>0  and len(wwd)>0:\n","            #print(f\"{i} - {el}\")\n","            #print(f\"{i} - {tmp}\")\n","            #     print(f\"{i} - PATHS: {path}\")\n","            #     print(f\"{i} - W=:N: {wen}\")\n","            #     print(f\"{i} - WWA: {wwa}\")\n","            #print(f\"{i} - SU: {su}\")\n","        #     print(f\"{i} - WWD: {wwd}\\n\")\n","        no_special.append(tmp)\n","    return no_special"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"gE3KjQgQseAe","executionInfo":{"status":"ok","timestamp":1648822766443,"user_tz":-120,"elapsed":7,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"outputs":[],"source":["def CamelCase(no_accent):\n","    tot_words = set()\n","    def camel_case_split(str):\n","        words = [[str[0]]]\n","        for c in str[1:]:\n","            if words[-1][-1].islower() and c.isupper():\n","                words.append(list(c))\n","            else:\n","                words[-1].append(c)\n","        inner_tmp = [''.join(word) for word in words]\n","        for ww in inner_tmp:\n","            tot_words.add(ww)\n","        out_tmp = ' '.join(inner_tmp)\n","        return out_tmp\n","    print(\"\\tCAMEL CASE SPLIT\")\n","    ccs = []\n","    for i, el in (enumerate(no_accent)):\n","        tmp = camel_case_split(el)\n","        tmp = tmp.lower()\n","        #if i < 69:\n","            #if el != tmp:\n","                #print(f\"{i} - {el}\")\n","                #print(f\"{i} - {tmp}\")\n","        #     if el!=tmp:\n","        #         print(\"MODIFIED!\\n\")\n","        #     else:\n","        #         print(\"\\n\")\n","        ccs.append(tmp)\n","    return ccs"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"Xh9TeNDOseAf","executionInfo":{"status":"ok","timestamp":1648822766443,"user_tz":-120,"elapsed":7,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"outputs":[],"source":["def CorrectCamel(normalized):\n","    mergedwords = set()  ############################################################################### TODO A PRIORI\n","    AllWord = set()\n","    for el in (normalized):  # ALL DATA\n","        for wrd in el.split(\" \"):\n","            AllWord.add(wrd)\n","    #print(len(AllWord))\n","    def create_merged_words(str):\n","        List_w = str.split(\" \")\n","        List_w = [x for x in List_w if len(x) > 0]\n","        j = 0\n","        for i in range(len(List_w)):\n","            if j >= len(List_w):\n","                break\n","            if (j + 1) < len(List_w):\n","                tmp_w = List_w[j] + List_w[j + 1]\n","                if tmp_w in AllWord:\n","                    if \"block\" not in tmp_w:\n","                        mergedwords.add(tmp_w)\n","                    j += 2\n","                else:\n","                    j += 1\n","            else:\n","                j += 1\n","    def wrong_camel_case_split(str):\n","        List_w = [x for x in str.split(\" \") if len(x) > 0]\n","        row_set = set(List_w)\n","        row_list = list(row_set)\n","        output = []\n","        j = 0\n","        flag = 0\n","        for i in range(len(List_w)):\n","            if j >= len(List_w):\n","                break\n","            if (j + 1) < len(List_w):\n","                tmp_w = List_w[j] + List_w[j + 1]\n","                if tmp_w in row_list or tmp_w in mergedwords:\n","                    flag = 1\n","                    output.append(tmp_w)\n","                    j += 2\n","                else:\n","                    output.append(List_w[j])\n","                    j += 1\n","            else:\n","                output.append(List_w[j])\n","                j += 1\n","        out = \" \".join(output)\n","        return out, flag\n","    print(\"\\tCONCATENATION WRONG CAMEL CASE\")\n","    w_css = []\n","    for i, el in (enumerate(normalized)):\n","        create_merged_words(el)\n","    #print(f\"MERGED WORDS: {mergedwords}\")\n","    for i, el in (enumerate(normalized)):\n","        tmp, flag = wrong_camel_case_split(el)\n","        #if i < 69:\n","            #if flag:\n","                #print(f\"{i} - {el}\")\n","                #print(f\"{i} - {tmp}\")\n","        #     if flag:\n","        #         print(\"MODIFIED!\\n\")\n","        #     else:\n","        #         print(\"\\n\")\n","        w_css.append(tmp)\n","    return w_css"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"pSf66vTEseAh","executionInfo":{"status":"ok","timestamp":1648822766444,"user_tz":-120,"elapsed":7,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"outputs":[],"source":["def ExpandContraction(w_css):\n","    def expand_contractions(text):\n","        expanded_words = []\n","        for word in text.split():\n","            fix_w = contractions.fix(word)\n","            expanded_words.append(fix_w)\n","        out_tmp = ' '.join(expanded_words)\n","        return out_tmp\n","    print(\"\\tEXPAND POSSIBLE CONTRACTIONS\")\n","    expand = []\n","    for i, el in (enumerate(w_css)):\n","        tmp = expand_contractions(el)\n","        #if el != tmp:\n","            #print(\"MODIFIED!\\n\")\n","            #print(f\"{i} - {el}\")\n","            #print(f\"{i} - {tmp}\\n\")\n","        expand.append(tmp)\n","    return expand"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"84quyYdGseAi","executionInfo":{"status":"ok","timestamp":1648822766444,"user_tz":-120,"elapsed":7,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"outputs":[],"source":["def Tokenization(expand):\n","    print(\"\\tTOKENIZATION\")\n","    tokenize = []\n","    for i, el in (enumerate(expand)):\n","        tmp = word_tokenize(el)\n","        tokenize.append(tmp)\n","    return tokenize"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"4EkxzNPZseAi","executionInfo":{"status":"ok","timestamp":1648822766445,"user_tz":-120,"elapsed":7,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"outputs":[],"source":["def RemoveStopwords(tokenize):\n","    print(\"\\tREMOVE POSSIBLE STOPWORDS\")\n","    stop = stopwords.words('english')\n","    stop.append(name.lower())  # DATASET NAME\n","    stop.remove(\"not\")\n","    stop.remove(\"no\")\n","    stop.remove(\"nor\")\n","    stop.remove(\"down\")\n","    no_stop = []\n","    for i, el in (enumerate(tokenize)):\n","        tmp = []\n","        for x in el:\n","            if x not in stop:\n","                if x == \"no\" or x == \"nor\":\n","                    tmp.append(\"not\")\n","                else:\n","                    tmp.append(x)\n","\n","        # if i < 69:\n","        #     print(f\"{i} - {el}\")\n","        #     print(f\"{i} - {tmp}\")\n","        #     if len(el)!=len(tmp):\n","        #         print(f\"MODIFIED! REMOVED: {[y for y in el if y not in tmp]}\\n\")\n","        #     else:\n","        #         print(\"\\n\")\n","        no_stop.append(tmp)\n","    return no_stop"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"BjKyE_epseAj","executionInfo":{"status":"ok","timestamp":1648822766445,"user_tz":-120,"elapsed":7,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"outputs":[],"source":["def PosTagChunk(no_stop):\n","    print(\"\\tPOS TAGGING & CHUNKING\")\n","    tagg = []\n","    for i, el in (enumerate(no_stop)):\n","        tmp = pos_tag(el)\n","        # if i < 100:\n","        #     print(f\"{i} - {el}\")\n","        #     print(f\"{i} - {tmp}\\n\")\n","        tagg.append(tmp)\n","    return tagg"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"ehuctsrKseAj","executionInfo":{"status":"ok","timestamp":1648822766854,"user_tz":-120,"elapsed":415,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"outputs":[],"source":["def StemmLemm(tagg):  ################################################################################ TODO USER AWARE WORD WITH MULTIPLE TAGS\n","    print(\"\\tLEMMING\")\n","    admitted_tag = [\"JJ\", \"JJR\", \"JJS\", \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n","    stemm = []\n","    wl = WordNetLemmatizer()\n","    diz_word = {}\n","    wrong_word_set = set()\n","    word_ing = set()\n","    wrong_word_idx = []\n","    to_keep_wrong_tag = [\"not\", \"filter\", \"namenode\", \"second\", \"timeout\", \"util\", \"down\", \"server\", \"driver\"]\n","    def get_wordnet_pos(tokens, idx=-1):\n","        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN,\n","                    \"V\": wordnet.VERB}  # , \"R\": wordnet.ADV} #EXTEND???\n","        tmp = []\n","        for token in tokens:\n","            # print(token)\n","            word = token[0]\n","            tag = token[1]\n","            if word.endswith(\"ing\") and tag in admitted_tag and tag[0] != \"V\":\n","                word_ing.add((word, tag))\n","                tag = \"VB\"\n","            if tag[0] != \"V\" and word.endswith(\"ed\"):\n","                wrong_word_set.add(word)\n","                if idx < 0:\n","                    wrong_word_idx.append(idx)\n","                tag = \"VB\"\n","            if tag in admitted_tag or word in to_keep_wrong_tag:\n","                # diz_word[word]=tag\n","                tmp.append((word, tag_dict.get(tag[0], wordnet.NOUN)))\n","        # print(tmp)\n","        return tmp\n","    word_3 = set()\n","    to_keep = [\"max\", \"new\", \"net\", \"msg\", \"bad\", \"os\", \"ipc\", \"not\", \"eof\", \"dfs\", 'io', \"up\", \"map\", \"use\", \"bp\",\n","               \"nodename\", \"nn\", \"no\", \"src\", \"dst\", \"end\", \"rdd\", \"server\"]\n","    avoid_lemm_error = [\"dest\", \"os\", \"not\", 'io', \"server\"]\n","    my_lemma = {\"rescanning\": \"rescan\", \"rescanned\": \"rescan\", \"dst\": \"dest\", \"remoting\": \"remote\",\n","                \"deprecation\": \"deprecate\"}\n","    # java, io, eof, reset, socket, file, org, datanode, user, ipc, impl, storage, bpid, block, bytes, nn, src, dest, write, id, finalize, interval, total, fsdataset, replicas\n","    # TODO USER INPUT\n","    for i, el in (enumerate(tagg)):\n","        for w, t in el:\n","            if w not in diz_word.keys():\n","                diz_word[w] = set()\n","                t_wn = get_wordnet_pos([(w, t)])\n","                if len(t_wn) > 0:\n","                    diz_word[w].add(t_wn[0][1])\n","                if len(diz_word[w]) == 0:\n","                    print(f\"\\t\\t{w} NOT added since {t} NOT admitted\")\n","                    var = \"\"\n","                    while var not in [\"y\", \"n\"]:\n","                        var = input(f\"\\t\\tDo you want to change the TAG for {t} (prev {t}): \")\n","                        print(\"\\t\\tYou entered: \" + var)\n","                        if var in [\"y\", \"n\"]:\n","                            if var == \"y\":\n","                                var2 = ''\n","                                while var2 not in ['a', 'v', 'n']:\n","                                    var2 = input(f\"\\t\\tChoose tag between 'a','v' and 'n':\")\n","                                    if var2 in ['a', 'b', 'n']:\n","                                        t_wn = get_wordnet_pos([(w, var2)])\n","                                        if len(t_wn) > 0:\n","                                            diz_word[w].add(t_wn[0][1])\n","                            else:\n","                                print(f\"\\t\\tElement Discarded\")\n","                        else:\n","                            print(f'\\t\\tInput not acceptable, choose between: {[\"y\", \"n\"]}')\n","                    print(\"\\n\")\n","            else:\n","                t_wn = get_wordnet_pos([(w, t)])\n","                if len(t_wn) > 0:\n","                    diz_word[w].add(t_wn[0][1])\n","    word_tags = {}\n","    tot_err = 0\n","    for k, v in diz_word.items():\n","        v = list(v)\n","        if len(v) > 1:\n","            tot_err += 1\n","\n","    print(f\"\\t\\tThere are {tot_err} words with more than one tag\")\n","\n","    tag_wn = {\"a\": \"ADJECTIVE\", \"n\": \"NOUN\", \"v\": \"VERB\", \"d\": \"DISCARDA TAG AGGREGATION ANALYSIS FOR THAT WORD\"}\n","    to_pop = []\n","    for k, v in diz_word.items():\n","        if len(v) == 0:\n","            to_pop.append(k)\n","        else:\n","            v = list(v)\n","            if len(v) > 1:\n","                kb = \"\"\n","                for char in k:\n","                    kb = kb + \"\\u0332\" + char + \"\\u0332\"\n","                print(f\"\\t\\tConflict for: {kb}, TAGS found: {v}\")\n","                set_row = set()\n","                for orig in no_special:\n","                    ispresent = False\n","                    for wrd in orig.split(\" \"):\n","                        if wrd.lower() == k:\n","                            ispresent = True\n","                    if ispresent:\n","                        tmp = []\n","                        for wrd in orig.split(\" \"):\n","                            if wrd.lower() == k:\n","                                tmpwrd = \"\"\n","                                for char in wrd:\n","                                    tmpwrd = tmpwrd + \"\\u0332\" + char + \"\\u0332\"\n","                                tmp.append(tmpwrd)\n","                            else:\n","                                tmp.append(wrd)\n","                        toadd = \" \".join(tmp)\n","                        set_row.add(toadd)\n","                var = None\n","                print(f\"\\t\\tHere the first 5 row to decide the TAG\")\n","                it_tmp = 0\n","                for el in set_row:\n","                    if it_tmp < 5:\n","                        print(f\"{it_tmp} - {el}\")\n","                    else:\n","                        break\n","                    it_tmp += 1\n","                # v.append(\"d\")\n","                while var not in v:\n","                    guideline = {}\n","                    for el in v:\n","                        if el == \"a\":\n","                            guideline[el] = \"ADJECTIVE\"\n","                        if el == \"n\":\n","                            guideline[el] = \"NOUN\"\n","                        if el == \"v\":\n","                            guideline[el] = \"VERB\"\n","                    guideline[\"d\"] = \"DISCARDA TAG AGGREGATION ANALYSIS FOR THAT WORD\"\n","                    var = input(f\"\\t\\tPlease choose the desidered TAG between {v} {guideline}: \")\n","                    print(\"\\t\\tYou entered: \" + var)\n","                    if var not in v:\n","                        if var in tag_wn.keys():\n","                            var2 = input(f\"\\t\\tYou have inserted another acceptable TAG: {var}. Do you confirm it? [y/n]\")\n","                            if var2 == \"y\":\n","                                word_tags[k] = var\n","                                print(f\"\\t\\tModification Accepted!\\n\")\n","                                break\n","                            if var2 == \"n\":\n","                                print(f\"\\t\\tModification Discarded!\")\n","                        else:\n","                            print(f\"\\t\\tInput not acceptable, choose between: {v}\")\n","                    else:\n","                        word_tags[k] = var\n","                        print(f\"\\t\\tInput Valid!\\n\")\n","            else:\n","                word_tags[k] = v\n","    for el in to_pop:\n","        diz_word.pop(el, None)\n","    #print(\"Start Lemmization\")\n","\n","    for i, el in (enumerate(tagg)):\n","        tmp = []\n","        res = get_wordnet_pos(el, i)\n","        for w in res:\n","            if len(w[0]) < 4 and w[0] not in to_keep:\n","                word_3.add(w[0])\n","        for w1, w2 in zip(el, res):\n","            if w2[0] in avoid_lemm_error:\n","                tmp.append(w2[0])\n","            else:\n","                if (w2[1] == \"v\" and len(w2[0]) > 2) or (w2[0] in to_keep) or (len(w2[0]) > 3):\n","                    if w2[0] in list(my_lemma.keys()):\n","                        tmp.append(my_lemma[w2[0]])\n","                    else:\n","                        if word_tags[w2[0]] != \"d\":\n","                            tmp.append(wl.lemmatize(w2[0], word_tags[w2[0]][0]))\n","                        else:\n","                            tmp.append(wl.lemmatize(w2[0], w2[1]))\n","        #if el != tmp:\n","         #   print(f\"{i} - {el}\")\n","          #  print(f\"{i} - {tmp}\")\n","           # print(\"MODIFIED\\n\")\n","        #else:\n","            #print(\"\\n\")\n","        stemm.append(tmp)\n","    #print(word_3)\n","    #print(wrong_word_set)\n","    #print(wrong_word_idx)\n","    #print(word_ing)\n","    return stemm, diz_word"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"KyZp1oCOseAl","executionInfo":{"status":"ok","timestamp":1648822766854,"user_tz":-120,"elapsed":5,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"outputs":[],"source":["def Regex(shorty):\n","    print(\"\\tREGEX\")\n","    r1r = re.compile(r\"([bcdfghjklmnpqrstvwxz]*[aeiou]{3}[bcdfghjklmnpqrstvwxz]*)+\")\n","    r2r = re.compile(r\"([aeiou]*[bcdfghjklmnpqrstvwxz]{4}[aeiou]*)+\")\n","    strange_w = set()\n","    r4 = set()\n","    r5 = set()\n","    to_keep = [\"queue\", \"http\"]\n","    def cleaning(text):\n","        output = []\n","        for wrd in text:\n","            if r1r.match(wrd) and len(wrd) < 6 and wrd not in to_keep:\n","                strange_w.add(wrd)\n","                r5.add(wrd)\n","            else:\n","                if r2r.match(wrd) and len(wrd) < 6 and wrd not in to_keep:\n","                    # print(wrd)\n","                    strange_w.add(wrd)\n","                    r4.add(wrd)\n","                else:\n","                    output.append(wrd)\n","        return output\n","    final = []\n","    for i, el in (enumerate(shorty)):  # ngrs\n","        tmp = cleaning(el)\n","       # if el != tmp:\n","           # print(f\"{i} - {el}\")\n","            #print(f\"{i} - {tmp}\")\n","          #  diff = [w for w in el if w not in tmp]\n","           # print(f\"{i} - {diff}\")\n","            #print(\"MODIFIED!\\n\")\n","        final.append(tmp)\n","    #print(f\"\\tStrange Words: {strange_w}\")\n","    #print(f\"{len(r4), len(r5)}\")\n","    return final"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"nC2VaSoHseAm","executionInfo":{"status":"ok","timestamp":1648822766855,"user_tz":-120,"elapsed":5,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"outputs":[],"source":["def RemoveFinalS(input):\n","    TotWord = []\n","    for el in (enumerate(input)):  # ALL DATA\n","        for wrd in el[1]:\n","            TotWord.append(wrd)\n","    tot_w = list(set(TotWord))\n","    #print(len(tot_w), len(TotWord))\n","    new_row_set = []\n","    new_final = []\n","    count_modiication = 0\n","    for row in input:\n","        new_row = []\n","        for word in row:\n","            word_trunc = \"\"\n","            last_char = \"\"\n","            for i, char in enumerate(word):\n","                if i < len(word) - 1:\n","                    word_trunc += char\n","                else:\n","                    last_char = char\n","            if word_trunc in tot_w and last_char == \"s\":\n","                new_row.append(word_trunc)\n","            else:\n","                new_row.append(word)\n","        if row != new_row:\n","            count_modiication += 1\n","        if row != new_row and new_row not in new_row_set:\n","            #print(\"MODIFIED!\")\n","           # print(row)\n","           # print(new_row)\n","            new_row_set.append(new_row)\n","        new_final.append(new_row)\n","    #print(len(input), (len(new_final)), count_modiication)\n","    return new_final"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"QjFhq7e8seAn","executionInfo":{"status":"ok","timestamp":1648822766855,"user_tz":-120,"elapsed":5,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"outputs":[],"source":["def NGRAMS(final, no_mono):\n","    print(\"\\tMAKE NGRAMS\")\n","    ngrs = []\n","    total_bigrams = set()  # NON NECESSARIO FARLO A PRIORI, OGNI RIGA SI BASA PER LE CORREZIONE SUI BIGRAMMI DELLE RIGHE PRECEDENTI\n","    total_trigrams = set()\n","    for i, el in enumerate(final):\n","        #print(el)\n","        tmp1_t = list(map(\" \".join, list(ngrams(el, 1))))\n","        tmp1 = [el for el in tmp1_t if el not in no_mono]\n","        tmp2 = list(map(\" \".join, list(ngrams(el, 2))))\n","        tmp3 = list(map(\" \".join, list(ngrams(el, 3))))\n","        #print(tmp2)\n","        #print(tmp3)\n","        tmp2_3 = []\n","        for idx, ngrams_list in enumerate([tmp2, tmp3]):  # PRIMA BI E POI TRIGRAMMI\n","            inner_tmp = []\n","            for concat_words in ngrams_list:\n","                inner_words = concat_words.split(\" \")\n","                single_words = set(inner_words)\n","                if len(single_words) == len(\n","                        inner_words):  # NON SONO PRESENTI ALMENO 2 PAROLE UGUALI -> SOLO PAROLE DISTINTE\n","                    if idx == 0:  # SE BIGRAM\n","                        # word1 = inner_words[0] + \" \" + inner_words[1]\n","                        word_reversed = inner_words[1] + \" \" + inner_words[0]\n","                        if word_reversed in total_bigrams:\n","                            #print(f\"{i} - REVERSE ALREADY EXISTS: {word_reversed} of {concat_words}\")\n","                            inner_tmp.append(word_reversed)\n","                            total_bigrams.add(word_reversed)\n","                        else:\n","                            inner_tmp.append(concat_words)\n","                            total_bigrams.add(concat_words)\n","                    else:\n","                        to_add = concat_words\n","                        inner_tmp.append(to_add)\n","                        total_trigrams.add(to_add)\n","                else:\n","                    None\n","                    #print(f\"{i} - NOT ADDED: {concat_words}\")\n","            tmp2_3 += inner_tmp\n","        tmp4 = tmp1 + tmp2_3  # tmp2 + tmp3 + tmp1\n","        tmp = tmp4\n","        #print(f\"{i} - {el}\")\n","        #print(f\"{i} - {tmp}\\n\")\n","        ngrs.append(tmp)\n","    return ngrs"]},{"cell_type":"markdown","source":["#### RUNNING FUNCTIONS"],"metadata":{"id":"4FehVkyBtOaK"}},{"cell_type":"code","source":["def Data():\n","  start_time = datetime.now()\n","  outread = ReadFile(path, 0)\n","  input_preproc = MakeDataframe(outread)\n","  if Origin:\n","      f=open(f\"PIPE/{name}/{folder}/{name}_Input_ORIGIN+MESSAGE.txt\",\"w\")\n","  else:\n","      f=open(f\"PIPE/{name}/{folder}/{name}_Input_MESSAGE.txt\",\"w\")\n","  for el in input_preproc:\n","      print(el[0], file=f)\n","  f.close()\n","  no_accent = RemoveAccent(input_preproc)\n","  no_special = RemoveSpecial(no_accent)\n","  tmp = no_special\n","  tmp_u = np.unique(np.array(tmp))\n","  #print(len(tmp), len(tmp_u))\n","  # %%\n","  if Origin:\n","      f = open(f\"PIPE/{name}/{folder}/{name}_Clean_ORIGIN+MESSAGE.txt\", \"w\")\n","  else:\n","      f = open(f\"PIPE/{name}/{folder}/{name}_Clean_MESSAGE.txt\", \"w\")\n","  for el in tmp_u:\n","      print(el, file=f)\n","  f.close()\n","  # %%\n","  ccs = CamelCase(no_special)\n","  # %%\n","  w_css = CorrectCamel(ccs)\n","  # %%\n","  expand = ExpandContraction(w_css)\n","  tokenize = Tokenization(expand)\n","  no_stop = RemoveStopwords(tokenize)\n","  # %%\n","  tmp = np.array(no_stop)\n","  tmpu, tmpind = np.unique(tmp, return_index=True)\n","  no_stop = tmpu.tolist()\n","  print(f\"\\tBEFORE: {len(tmp)}, AFTER: {len(no_stop)}, DIFF: {len(tmp) - len(no_stop)}, %: {100 * ((len(tmp) - len(no_stop)) / len(tmp))}%\")\n","  tagg = PosTagChunk(no_stop)\n","  stemm2, diz_word = StemmLemm(tagg)\n","  final2 = Regex(stemm2)\n","  new_final2 = RemoveFinalS(final2)\n","  # %%\n","  nf2 = [\" \".join(el) for el in new_final2]\n","  # %%\n","  nf = CorrectCamel(nf2)\n","  new_final2 = [el.split(\" \") for el in nf]\n","  new_final = new_final2\n","  new_final = np.array(new_final)\n","  new_final_unique_max_f1 = []\n","  for row in new_final:\n","      r = np.array(row)\n","      ru = np.unique(r)\n","      new_final_unique_max_f1.append(ru)\n","  new_final_unique_max_f1 = np.array(new_final_unique_max_f1)\n","  # %%\n","  TotWordfinal_unique_max_f1 = []\n","  for el in (enumerate(new_final_unique_max_f1)):  # ALL DATA\n","      for wrd in el[1]:\n","          TotWordfinal_unique_max_f1.append(wrd)\n","  TotWordfinal_unique_max_f1 = np.array(TotWordfinal_unique_max_f1)\n","  TotWord_set = set(TotWordfinal_unique_max_f1)\n","  # %%\n","  diz_word_doc2 = {}\n","  for word in TotWord_set:\n","      diz_word_doc2[word] = 0\n","  for word in TotWord_set:\n","      for row in new_final_unique_max_f1:\n","          if word in row:\n","              diz_word_doc2[word] += 1\n","  sorted_d = dict(sorted(diz_word_doc2.items(), key=operator.itemgetter(1), reverse=True))\n","  keys = list(sorted_d.keys())\n","  values = list(sorted_d.values())\n","  # %%\n","  iter = 0\n","  for k, v in zip(keys, values):\n","      #print(iter, k, v)\n","      iter += 1\n","  # %%\n","  corpus = []\n","  for row in new_final_unique_max_f1:\n","      r = \" \".join(row)\n","      #print(r)\n","      corpus.append(r)\n","  #print(len(corpus))\n","  # vectorizer = CountVectorizer(max_df=0.8)# min_df=2)#, ngram_range=(1, 3))\n","  # X = vectorizer.fit_transform(corpus)\n","\n","  # features1 = vectorizer.get_feature_names()\n","  # diz_word_doc = {}\n","  # for word in features1:\n","  #     diz_word_doc[word] = 0\n","  # for word in features1:\n","  #     for row in new_final_unique_max_f1:\n","  #         if word in row:\n","  #             diz_word_doc[word] += 1\n","  # sorted_d = dict(sorted(diz_word_doc.items(), key=operator.itemgetter(1), reverse=True))\n","  # keys = list(sorted_d.keys())\n","  # values = list(sorted_d.values())\n","  # tot_len = len(new_final_unique_max_f1)\n","  # no_mono = []\n","  # for k, v in zip(keys, values):\n","  #     f = v / tot_len\n","  #     if f >= 0.2 and f < 0.8 and v!=1:\n","  #         no_mono.append(k)\n","  #     #print(k, v, f)\n","  f=open(f\"PIPE/INPUT/{name}_drop.txt\", \"r\")\n","  lines=f.readlines()\n","  to_drop=[]\n","  for el in lines:\n","    to_drop.append(el.rstrip(\"\\n\"))\n","\n","  f=open(f\"PIPE/INPUT/{name}_nomono.txt\", \"r\")\n","  lines=f.readlines()\n","  no_mono=[]\n","  for el in lines:\n","    no_mono.append(el.rstrip(\"\\n\"))\n","\n","  if len(no_mono)>0:\n","    print(f\"\\tWORDS TO TAKE ONLY THEIR BIGRAM/TRIGRAM INSIDE LOG ROWS: {no_mono}\")\n","\n","  in_data = []\n","  dropped_words = set()\n","  for row in (new_final):\n","      tmp = []\n","      for el in row:\n","          if el not in to_drop:\n","              tmp.append(el)\n","          else:\n","              dropped_words.add(el)\n","      in_data.append(tmp)\n","  if len(dropped_words)>0:\n","    print(f\"\\tDROPPED WORDS THAT ARE TO FREQUENT {dropped_words}\")\n","  # %%\n","  input_data = NGRAMS(in_data, no_mono)\n","  # %%\n","  i = 0\n","  for el, el2 in zip(input_data, in_data):\n","      eligible = False\n","      eligible2 = False\n","      for w in el2:\n","          if w in dropped_words:\n","              eligible = True\n","          if w in no_mono:\n","              eligible2 = True\n","      #if eligible or eligible2:\n","          #print(i, el2, el)\n","      i += 1\n","  # %%\n","  end_time = datetime.now()\n","  #print('Duration: {}'.format(end_time - start_time))\n","  # %%\n","  if Origin:\n","      f = open(f\"PIPE/{name}/{folder}/{name}_Input_Output_ORIGIN+MESSAGE.txt\", \"w\")\n","      f2 = open(f\"PIPE/{name}/{folder}/{name}_Raw_ORIGIN+MESSAGE.txt\", \"w\")\n","  else:\n","      f = open(f\"PIPE/{name}/{folder}/{name}_Input_Output_MESSAGE.txt\", \"w\")\n","      f2 = open(f\"PIPE/{name}/{folder}/{name}_Raw_MESSAGE.txt\", \"w\")\n","\n","  iter = 0\n","  for ind in tmpind:\n","      print(f\"{iter} - {no_accent[ind]}\", file=f)\n","      print(f\"{no_accent[ind]}\", file=f2)\n","      print(f\"{iter} - {input_data[iter]}\\n\", file=f)  # SpaCy\n","      iter += 1\n","  f.close()\n","  f2.close()\n","  # %%\n","  #print(\"SAVE DATA\")\n","  if Origin:\n","      fname = f\"PIPE/{name}/{folder}/{name}_InputData_ORIGIN+MESSAGE.txt\"\n","  else:\n","      fname = f\"PIPE/{name}/{folder}/{name}_InputData_MESSAGE.txt\"\n","\n","  f = open(fname, \"w\")\n","  for el in input_data:\n","      print(el, file=f)\n","  #print(\"END SAVE DATA\")\n","  f.close()"],"metadata":{"id":"eYisx_uDvVjO","executionInfo":{"status":"ok","timestamp":1648822767144,"user_tz":-120,"elapsed":293,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":["### TOPIC MODELS\n"],"metadata":{"id":"-s3eZTx2wzMC"}},{"cell_type":"markdown","source":["#### PROCESSING FUNCTIONS"],"metadata":{"id":"rute0bTixHh_"}},{"cell_type":"code","source":["def get_topic_words(token_lists, labels, k=None, topk=20):\n","    if k is None:\n","        k = len(np.unique(labels))\n","    topics = ['' for _ in range(k)]\n","    for i, c in enumerate(token_lists):\n","        topics[labels[i]] += (' ' + ' '.join(c))\n","    word_counts = list(map(lambda x: Counter(x.split()).items(), topics))\n","    # get sorted word counts\n","    word_counts = list(map(lambda x: sorted(x, key=lambda x: x[1], reverse=True),word_counts))\n","    # get topics\n","    topics = list(map(lambda x: list(map(lambda x: x[0], x[:topk])), word_counts))\n","    return topics\n","def get_coherence(model, token_list, measure='c_v', topk=20):\n","    topics = get_topic_words(token_list, model.cluster_model.labels_, topk=20)\n","    cm = CoherenceModel(topics=topics, texts = token_list, corpus=model.corpus, dictionary=model.dictionary, coherence = measure)\n","    return cm.get_coherence()"],"metadata":{"id":"vbYha1NVxPI-","executionInfo":{"status":"ok","timestamp":1648822767148,"user_tz":-120,"elapsed":8,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["class Autoencoder:\n","    def __init__(self, latent_dim = 32, activation='relu', epochs=200, batch_size=128):\n","        self.latent_dim = latent_dim\n","        self.activation = activation\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.autoencoder = None\n","        self.encoder = None\n","        self.decoder = None\n","        self.his = None\n","        self.Xtrain=None\n","        self.Xtest=None\n","    def _compile(self, input_dim):\n","        input_vec = Input(shape=(input_dim,))\n","        encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)\n","        decoded = Dense(input_dim, activation=self.activation)(encoded)\n","        self.autoencoder = Model(input_vec, decoded)\n","        self.encoder = Model(input_vec, encoded)\n","        encoded_input = Input(shape=(self.latent_dim,))\n","        decoded_layer = self.autoencoder.layers[-1]\n","        self.decoder = Model(encoded_input, self.autoencoder.layers[-1](encoded_input))\n","        self.autoencoder.compile(optimizer='adam', loss=keras.losses.binary_crossentropy) #ADAM + MEAN ABS ERROR\n","    def fit(self, X):\n","        if not self.autoencoder:\n","            self._compile(X.shape[1])\n","        X_train, X_test = train_test_split(X, random_state=42)\n","        self.Xtrain=X_train\n","        self.Xtest=X_test\n","        self.his = self.autoencoder.fit(X_train, X_train, epochs=self.epochs, batch_size=self.batch_size, shuffle=False, validation_data=(X_test, X_test), verbose=0, workers=1)"],"metadata":{"id":"DT-pzx3FxS5e","executionInfo":{"status":"ok","timestamp":1648822767556,"user_tz":-120,"elapsed":415,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import StandardScaler,MinMaxScaler\n","from sklearn.decomposition import PCA\n","#import plotly.express as px\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","from gensim import corpora\n","import gensim\n","import numpy as np\n","#from Autoencoder import *\n","#from preprocess import *\n","pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n","pd.set_option('expand_frame_repr', False)\n","from datetime import datetime\n","\n","# define model object\n","class Topic_Model:\n","    def __init__(self, k=10, method='BERT', gamma=15, delta=15, epsilon=15, AE_latent_dim=32, AE_epochs=100, modelw2v=None, cluster_model=None, cluster_metric=None, scale=None, reduce=None, DEBUG=False):\n","\n","        if method not in {'BERT', 'BERT_LDA', \"BERT_LDA_W2V\", \"W2V\", \"W2V_LDA\", \"BERT_W2V\", \"BERT_NMF\", \"BERT_NMF_W2V\", \"W2V_NMF\"}:\n","            raise Exception('Invalid method!')\n","        self.k = k\n","        self.dictionary = None\n","        self.corpus = None\n","        self.ldamodel = None\n","        self.nmfmodel = None\n","        self.modelw2v = modelw2v\n","        self.vec = {}\n","        self.gamma = gamma #15  # parameter for reletive importance of lda\n","        self.delta = delta\n","        self.epsilon = epsilon\n","        self.method = method\n","        self.AE = None\n","        self.AE_latent_dim=AE_latent_dim\n","        self.AE_epochs=AE_epochs\n","        self.ldavec=None\n","        self.nmfvec=None\n","        self.bertvec=None\n","        self.w2vvec=None\n","        self.cm=cluster_model\n","        self.scale=scale\n","        self.cluster_metric=cluster_metric\n","        self.cluster_model=None\n","        self.reduce=reduce\n","        self.DEBUG=DEBUG\n","        self.tw=None\n","        #self.cluster_model=None\n","        print(f\"Method:{self.method}, Gamma:{self.gamma}, Delta:{self.delta}, Epsilon:{self.epsilon}, AE_latent_dim:{self.AE_latent_dim}, AE_epochs:{self.AE_epochs}, Cluster: {self.cm}, Metric: {self.cluster_metric}, Scale: {self.scale}, Reduce: {self.reduce}\")\n","\n","    def vectorize(self, sentences, token_lists, method=None):\n","        # Default method\n","        if method is None:\n","            method = self.method\n","\n","        if self.dictionary is None:\n","          print(\"\\tMAKE DICTIONARY\")\n","          self.dictionary = corpora.Dictionary(token_lists)\n","        if method==self.method:\n","          print(\"\\tMAKE CORPUS\")\n","          self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n","\n","        if method == 'LDA':\n","            print(\"LDA\")\n","            print(\"\\tLEN CORPUS\",len(self.corpus))\n","            if self.ldamodel is None:\n","                print(\"\\tTRAIN LDA MODEL\")\n","                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary, passes=10, iterations=100)\n","\n","            def get_vec_lda(model, corpus, k):\n","                n_doc = len(corpus)\n","                vec_lda = np.zeros((n_doc, k))\n","                for i in range(n_doc):\n","                    # get the distribution for the i-th document in corpus\n","                    for topic, prob in model.get_document_topics(corpus[i]):\n","                        vec_lda[i, topic] = prob\n","                return vec_lda\n","\n","            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n","            return vec\n","\n","        elif method == 'NMF':\n","            print(\"NMF\")\n","            print(\"\\tLEN CORPUS\",len(self.corpus))\n","            if self.nmfmodel is None:\n","                print(\"\\tTRAIN NMF MODEL\")\n","                self.nmfmodel = Nmf(self.corpus, num_topics=self.k, id2word=self.dictionary, passes=20, eval_every=100)\n","\n","            def get_vec_nmf(model, corpus, k):\n","                n_doc = len(corpus)\n","                vec_nmf = np.zeros((n_doc, k))\n","                for i in range(n_doc):\n","                    # get the distribution for the i-th document in corpus\n","                    for topic, prob in model.get_document_topics(corpus[i]):\n","                        vec_nmf[i, topic] = prob\n","\n","                return vec_nmf\n","\n","            vec = get_vec_nmf(self.nmfmodel, self.corpus, self.k)\n","            return vec\n","\n","        elif method == 'BERT':\n","            print(\"BERT\")\n","\n","            model = SentenceTransformer('all-mpnet-base-v2')#bert-base-nli-max-tokens')\n","            vec_bert = np.array(model.encode(sentences, show_progress_bar=False))\n","\n","            return vec_bert\n","             \n","        elif method == 'BERT_LDA':\n","            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n","            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n","            self.ldavec=vec_lda\n","            self.bertvec=vec_bert\n","\n","            if self.gamma>0 and self.delta>0:\n","                vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert*self.delta]\n","            elif self.gamma>0:\n","                vec_ldabert=vec_lda*self.gamma\n","            else:\n","                vec_ldabert=vec_bert*self.delta\n","\n","            print(vec_lda.shape, vec_bert.shape, vec_ldabert.shape)\n","\n","            return vec_ldabert\n","\n","        elif method == 'BERT_NMF':\n","            vec_nmf = self.vectorize(sentences, token_lists, method='NMF')\n","            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n","            self.nmfvec=vec_nmf\n","            self.bertvec=vec_bert\n","\n","            vec_nmfbert = np.c_[vec_nmf * self.gamma, vec_bert*self.delta]\n","\n","            print(vec_nmf.shape, vec_bert.shape, vec_nmfbert.shape)\n","\n","            return vec_nmfbert\n","        \n","        elif method == \"BERT_LDA_W2V\":\n","            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n","            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n","            vec_w2v = self.vectorize(sentences, token_lists, method='W2V')\n","            self.ldavec=vec_lda\n","            self.bertvec=vec_bert\n","            self.w2vvec=vec_w2v\n","\n","            if self.gamma>0 and self.delta>0 and self.epsilon>0:\n","                vec_bertldaw2v = np.c_[vec_lda * self.gamma, vec_bert*self.delta, vec_w2v*self.epsilon]\n","            \n","            if self.gamma>0 and self.delta>0 and self.epsilon==0:\n","                vec_bertldaw2v = np.c_[vec_lda * self.gamma, vec_bert*self.delta]\n","\n","            if self.gamma>0 and self.delta==0 and self.epsilon>0:\n","                vec_bertldaw2v = np.c_[vec_lda * self.gamma, vec_w2v*self.epsilon]\n","            \n","            if self.gamma==0 and self.delta>0 and self.epsilon>0:\n","                vec_bertldaw2v = np.c_[vec_bert*self.delta, vec_w2v*self.epsilon]\n","            \n","            if self.gamma==0 and self.delta==0 and self.epsilon>0:\n","                vec_bertldaw2v = vec_w2v*self.epsilon\n","            \n","            if self.gamma==0 and self.delta>0 and self.epsilon==0:\n","                vec_bertldaw2v = vec_bert*self.delta\n","\n","            print(vec_lda.shape, vec_bert.shape, vec_w2v.shape, vec_bertldaw2v.shape)\n","\n","            \n","            return vec_bertldaw2v\n","        \n","        elif method == \"BERT_NMF_W2V\":\n","            vec_nmf = self.vectorize(sentences, token_lists, method='NMF')\n","            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n","            vec_w2v = self.vectorize(sentences, token_lists, method='W2V')\n","            self.nmfvec=vec_nmf\n","            self.bertvec=vec_bert\n","            self.w2vvec=vec_w2v\n","\n","            vec_bertnmfw2v = np.c_[vec_nmf * self.gamma, vec_bert*self.delta, vec_w2v*self.epsilon]\n","\n","            print(vec_nmf.shape, vec_bert.shape, vec_w2v.shape, vec_bertnmfw2v.shape)\n","\n","            return vec_bertnmfw2v\n","        \n","        elif method == \"W2V\":\n","            print(\"W2V\")\n","            vec_w2v=np.zeros((len(token_lists),300))\n","            for i,tl in enumerate(token_lists):\n","                curr_v=np.zeros(300)\n","                num_words=0\n","                for el in tl:\n","                    word_vec=self.modelw2v[el]\n","                    curr_v+=word_vec\n","                    num_words+=1\n","                curr_v=curr_v/num_words\n","                vec_w2v[i]=curr_v\n","\n","            return vec_w2v\n","        \n","        elif method == \"W2V_LDA\":\n","            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n","            vec_w2v = self.vectorize(sentences, token_lists, method='W2V')\n","            self.ldavec=vec_lda\n","            self.w2vvec=vec_w2v\n","\n","            if self.gamma>0 and self.epsilon>0:\n","                vec_ldaw2v = np.c_[vec_lda * self.gamma, vec_w2v*self.epsilon]\n","            \n","            if self.gamma==0 and self.delta>0:\n","                vec_ldaw2v = vec_w2v*self.epsilon\n","            \n","            print(vec_lda.shape, vec_w2v.shape, vec_ldaw2v.shape)\n","            \n","            return vec_ldaw2v\n","        \n","        elif method == \"W2V_NMF\":\n","            vec_nmf = self.vectorize(sentences, token_lists, method='NMF')\n","            vec_w2v = self.vectorize(sentences, token_lists, method='W2V')\n","            self.nmfvec=vec_nmf\n","            self.w2vvec=vec_w2v\n","            vec_nmfw2v = np.c_[vec_nmf * self.gamma, vec_w2v*self.epsilon]\n","            \n","            print(vec_nmf.shape, vec_w2v.shape, vec_nmfw2v.shape)\n","            \n","            return vec_nmfw2v\n","        \n","        elif method == \"BERT_W2V\":\n","            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n","            vec_w2v = self.vectorize(sentences, token_lists, method='W2V')\n","            self.bertvec=vec_bert\n","            self.w2vvec=vec_w2v\n","\n","            if self.delta>0 and self.epsilon>0:\n","                vec_bertw2v = np.c_[vec_bert*self.delta, vec_w2v*self.epsilon]\n","            \n","            if self.delta>0 and self.epsilon==0:\n","                vec_bertw2v = vec_bert * self.delta\n","\n","            if self.delta==0 and self.epsilon>0:\n","                vec_bertw2v = vec_w2v*self.epsilon\n","            \n","\n","            print(vec_bert.shape, vec_w2v.shape, vec_bertw2v.shape)\n","\n","            return vec_bertw2v\n","\n","    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n","\n","        if method is None:\n","            method = self.method\n","\n","        if m_clustering is None:\n","            if self.cm==\"KMeans\": \n","                print(f\"CLUSTER MODEL: {self.cm}\")\n","                m_clustering = KMeans\n","                self.cluster_model = m_clustering(n_clusters=self.k, random_state=42)\n","            if self.cm==\"KMedoids\":\n","                print(f\"CLUSTER MODEL: {self.cm}, CLUSTER METRIC: {self.cluster_metric}\")\n","                m_clustering = KMedoids\n","                self.cluster_model = m_clustering(n_clusters=self.k, metric=self.cluster_metric, random_state=42, init='k-medoids++', method='pam')\n","\n","        # if not self.dictionary:\n","        #     self.dictionary = corpora.Dictionary(token_lists)\n","        #     self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n","        \n","        \n","        self.vec[method] = self.vectorize(sentences, token_lists, method)\n","        \n","        # if self.scale:\n","        #     print(\"SCALE DATA\")\n","        #     scaler = MinMaxScaler()\n","        #     normalized_data=scaler.fit_transform(self.vec[method])\n","        #     self.vec[method]=normalized_data   \n","            \n","        if self.reduce:\n","            print(\"REDUCE DATA\")\n","            if self.DEBUG:\n","                print(\"### DEBUG ###\")\n","                row_mean=np.mean(self.vec[method],1)\n","                row_std=np.std(self.vec[method],1)\n","                col_mean=np.mean(self.vec[method],0)\n","                col_std=np.std(self.vec[method],0)\n","                print(f\"MEAN: {np.mean(self.vec[method])}, STD: {np.std(self.vec[method])}\")\n","                print(f\"ROW - MEAN: {row_mean}\")\n","                print(f\"ROW - STD: {row_std}\")\n","                print(f\"COL - MEAN: {col_mean}\")\n","                print(f\"COL - STD: {col_std}\")\n","                for i in range(10):\n","                    print(self.vec[method][i])\n","                \n","            if not self.AE:\n","                self.AE = Autoencoder(epochs=self.AE_epochs, latent_dim=self.AE_latent_dim)\n","                self.AE.fit(self.vec[method])\n","            vec = self.AE.encoder.predict(self.vec[method])\n","            self.vec[method]=vec\n","            \n","            if self.DEBUG:\n","                print(\"### DEBUG ###\")\n","                row_mean=np.mean(vec,1)\n","                row_std=np.std(vec,1)\n","                col_mean=np.mean(vec,0)\n","                col_std=np.std(vec,0)\n","                print(f\"MEAN: {np.mean(vec)}, STD: {np.std(vec)}\")\n","                print(f\"ROW - MEAN: {row_mean}\")\n","                print(f\"ROW - STD: {row_std}\")\n","                print(f\"COL - MEAN: {col_mean}\")\n","                print(f\"COL - STD: {col_std}\")\n","                df=pd.DataFrame(self.vec[method])\n","                print(df)\n","        \n","        if self.DEBUG:\n","            print(\"BEFORE NORMALIZATION\")\n","            pca = PCA(n_components=2)\n","            print(self.vec[method].shape)\n","            components = pca.fit_transform(self.vec[method])\n","            components=np.array(components)\n","            plt.scatter(components[:,0],components[:,1])\n","            plt.show()\n","                \n","        if self.scale:\n","            print(\"SCALE DATA\")\n","            scaler = StandardScaler()\n","            normalized_data=scaler.fit_transform(self.vec[method])\n","            self.vec[method]=normalized_data    \n","                         \n","        if self.DEBUG:\n","            print(\"AFTER NORMALIZATION\")\n","            pca = PCA(n_components=2)\n","            print(self.vec[method].shape)\n","            components=np.array(components)\n","            plt.scatter(components[:,0],components[:,1])\n","            plt.show()\n","        \n","        if self.DEBUG:\n","            print(\"### DEBUG ###\")\n","            row_mean=np.mean(vec,1)\n","            row_std=np.std(vec,1)\n","            col_mean=np.mean(vec,0)\n","            col_std=np.std(vec,0)\n","            print(f\"MEAN: {np.mean(vec)}, STD: {np.std(vec)}\")\n","            print(f\"ROW - MEAN: {row_mean}\")\n","            print(f\"ROW - STD: {row_std}\")\n","            print(f\"COL - MEAN: {col_mean}\")\n","            print(f\"COL - STD: {col_std}\")\n","            df=pd.DataFrame(self.vec[method])\n","            print(df)\n","        \n","        print(\"CLUSTER INPUT DIM:\",self.vec[method].shape)\n","        self.cluster_model.fit(self.vec[method])\n","\n","    def predict(self, sentences, token_lists):\n","        print(\"PREDICT\")\n","        other_corpus=[self.dictionary.doc2bow(text) for text in token_lists]\n","        self.corpus=other_corpus\n","        vec = self.vectorize(sentences, token_lists, self.method)\n","        vec = self.AE.encoder.predict(vec)\n","        lbs = self.cluster_model.predict(vec)\n","        return lbs\n"],"metadata":{"id":"zEZQfB1cxVbj","executionInfo":{"status":"ok","timestamp":1648822769020,"user_tz":-120,"elapsed":1468,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":["#### RUNNING FUNCTIONS"],"metadata":{"id":"OnfFOCg5xMVu"}},{"cell_type":"code","source":["def Topic():\n","  print(\"\\n\")\n","  print(\"TOPIC MODEL...\")\n","  f=open(f\"PIPE/{name}/{folder}/{name}_InputData_{folder}.txt\",\"r\")\n","  lines=f.readlines()\n","  sentences=[]\n","  token_lists=[]\n","  idx_in=[]\n","  for i,line in enumerate(lines):\n","      line=line.rstrip(\"\\n\")\n","        #print(i,type(line),line)\n","      row_tk=[]\n","      for el in line.split(\", \"):\n","          el=el.replace(\"[\",\"\")\n","          el=el.replace(\"]\",\"\")\n","          el=el.replace(\"'\",\"\")\n","          el=el.replace(\" \",\"_\")\n","          row_tk.append(el)\n","        #print(i,type(row_tk),row_tk)\n","      sent=\" \".join(row_tk)\n","        #print(i,sent,\"\\n\")\n","      sentences.append(sent)\n","      token_lists.append(row_tk)\n","      idx_in.append(i)\n","  f.close()\n","  print(\"\\tLOADED\",len(sentences),\"INPUT SENTENCES\")\n","\n","  f=open(f\"PIPE/{name}/{folder}/{name}_Raw_{folder}.txt\",\"r\")\n","  lines=f.readlines()\n","  originals=[]\n","  for i,line in enumerate(lines):\n","      line=line.rstrip(\"\\n\")\n","      originals.append(line)\n","  f.close()\n","  print(\"\\tLOADED\",len(originals),\"RAW SENTENCES\")\n","\n","  print(\"\\tLOAD MODEL\")\n","\n","  tm=pickle.load(open(f\"PIPE/INPUT/{name}_{folder}.model\", \"rb\"))\n","\n","  topics_words=tm.tw\n","  print(len(topics_words))\n","\n","  predictions=tm.predict(sentences, token_lists)\n","  print(len(predictions))\n","  print(predictions)\n","\n","  print(\"\\tWRITE PREDICTIONS\")\n","  f = open(f\"PIPE/RESULT/{name}_{folder}_result.csv\", \"w\")\n","  for idx in range(len(sentences)):\n","      tw = topics_words[predictions[idx]]\n","      tw_s = \" \".join(tw)\n","      org = originals[idx].replace(\",\", \" \")\n","      output_str = str(predictions[idx]) + \", \" + tw_s + \", \" + org + \", \" + sentences[idx]\n","      print(output_str, file=f)\n","  f.close()\n","  print(\"... TOPIC MODEL\")"],"metadata":{"id":"zfxFLAmJxYm6","executionInfo":{"status":"ok","timestamp":1648822769021,"user_tz":-120,"elapsed":6,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":["## PIPELINE"],"metadata":{"id":"s5NliRdWxra7"}},{"cell_type":"code","source":["def Main():\n","  Data()\n","  Topic()"],"metadata":{"id":"bgF62wBgxsmb","executionInfo":{"status":"ok","timestamp":1648822773409,"user_tz":-120,"elapsed":3,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["Main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uUR2vaTgxuMx","executionInfo":{"status":"ok","timestamp":1648822793931,"user_tz":-120,"elapsed":19412,"user":{"displayName":"davide napolitano","userId":"10513136351490050850"}},"outputId":"2316b445-2df7-4215-da06-29f06c7458d2"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["\t/content/gdrive/MyDrive/Colab Notebooks/PIPE/INPUT/Spark_input.log\n","\tREMOVE POSSIBLE ACCENTED CHARACTERS\n","\tREMOVE SPECIAL\n","\tCAMEL CASE SPLIT\n","\tCONCATENATION WRONG CAMEL CASE\n","\tEXPAND POSSIBLE CONTRACTIONS\n","\tTOKENIZATION\n","\tREMOVE POSSIBLE STOPWORDS\n","\tBEFORE: 3, AFTER: 3, DIFF: 0, %: 0.0%\n","\tPOS TAGGING & CHUNKING\n","\tLEMMING\n","\t\tinstead NOT added since RB NOT admitted\n","\t\tDo you want to change the TAG for RB (prev RB): n\n","\t\tYou entered: n\n","\t\tElement Discarded\n","\n","\n","\t\tThere are 0 words with more than one tag\n","\tREGEX\n","\tCONCATENATION WRONG CAMEL CASE\n","\tWORDS TO TAKE ONLY THEIR BIGRAM/TRIGRAM INSIDE LOG ROWS: ['manager', 'executor', 'storage', 'block']\n","\tMAKE NGRAMS\n","\n","\n","TOPIC MODEL...\n","\tLOADED 3 INPUT SENTENCES\n","\tLOADED 3 RAW SENTENCES\n","\tLOAD MODEL\n","WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n","WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n","39\n","PREDICT\n","\tMAKE CORPUS\n","NMF\n","\tLEN CORPUS 3\n","BERT\n","W2V\n","(3, 39) (3, 768) (3, 300) (3, 1107)\n","3\n","[25  1  6]\n","\tWRITE PREDICTIONS\n","... TOPIC MODEL\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"cifyFKeCTQtF"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"PIPELINE.ipynb","provenance":[],"collapsed_sections":["7_tnlLjBsuAc","-F9dIrV7xD0e"],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}