{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psutil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu |grep 'Model name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu | grep 'Socket(s):'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu | grep 'Core(s) per socket:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gensim==4.1.2#3.8.3 #ISSUES->SOLVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install lda2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yellowbrick "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import unicodedata\n",
    "import re\n",
    "import csv\n",
    "import pickle \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import yellowbrick\n",
    "from yellowbrick.text import FreqDistVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "#from gensim.models.wrappers import DtmModel\n",
    "#from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models.nmf import Nmf\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords') #GUARDARE PIU AVANZATE -> NN\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import gensim.downloader\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from sklearn.tree import export_graphviz\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Origin=True\n",
    "name=\"HDFS\"\n",
    "\n",
    "data_origin=[]\n",
    "set_word=set()\n",
    "for tipo in [\"MESSAGE\",\"ORIGIN+MESSAGE\",\"SPLIT\"]:\n",
    "    print(\"LOAD DATA\")\n",
    "    filename = f\"{name}_InputData_{tipo}.txt\"\n",
    "\n",
    "    print(f\"READ FILE: {filename}\")\n",
    "\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        rows = csv.reader(csvfile, delimiter='\\t')\n",
    "        for i,row in enumerate(rows):\n",
    "            tmp = row[0].replace(\"[\", \"\")\n",
    "            tmp = tmp.replace(\"]\", \"\")\n",
    "            tmp = tmp.replace(\"'\", \"\")\n",
    "            row_l = list(tmp.split(\", \"))\n",
    "\n",
    "            data_origin.append(row_l)\n",
    "            for el in row_l:\n",
    "                set_word.add(el)\n",
    "    print(\"END LOAD DATA\")\n",
    "    \n",
    "print(len(data_origin))\n",
    "list_word=list(set_word)\n",
    "print(len(list_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Origin=True\n",
    "name=\"Spark\"\n",
    "\n",
    "data_origin2=[]\n",
    "set_word2=set()\n",
    "for tipo in [\"MESSAGE\",\"ORIGIN+MESSAGE\",\"SPLIT\"]:\n",
    "    print(\"LOAD DATA\")\n",
    "    filename = f\"{name}_InputData_{tipo}.txt\"\n",
    "\n",
    "    print(f\"READ FILE: {filename}\")\n",
    "\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        rows = csv.reader(csvfile, delimiter='\\t')\n",
    "        for i,row in enumerate(rows):\n",
    "            tmp = row[0].replace(\"[\", \"\")\n",
    "            tmp = tmp.replace(\"]\", \"\")\n",
    "            tmp = tmp.replace(\"'\", \"\")\n",
    "            row_l = list(tmp.split(\", \"))\n",
    "            #if i<30:\n",
    "            #print(i, row_l)\n",
    "            data_origin2.append(row_l)\n",
    "            for el in row_l:\n",
    "                set_word2.add(el)\n",
    "    print(\"END LOAD DATA\")\n",
    "    \n",
    "print(len(data_origin2))\n",
    "list_word2=list(set_word2)\n",
    "print(len(list_word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_word=list_word+list_word2\n",
    "tot_word_u=np.unique(tot_word)\n",
    "all_word=tot_word_u.tolist()\n",
    "print(len(tot_word), len(all_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_g= api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "for w in all_word:\n",
    "    try:\n",
    "        vec = model_g[w]\n",
    "    except KeyError:\n",
    "        #print(f\"The word <{w}> does not appear in this model\")\n",
    "        counter+=1\n",
    "print(\"Not Known\",counter, \"over\", len(all_word), \"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v=[]\n",
    "for el in all_word:\n",
    "    w2v.append([el.replace(\" \",\"_\")])\n",
    "print(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "tot=0\n",
    "unique_word=set()\n",
    "for el in w2v:\n",
    "    for w in el:\n",
    "        tot+=1\n",
    "        unique_word.add(w)\n",
    "\n",
    "print(tot)\n",
    "for w in unique_word:\n",
    "    try:\n",
    "        vec = model_g[w]\n",
    "    except KeyError:\n",
    "        #print(f\"The word <{w}> does not appear in this model\")\n",
    "        counter+=1\n",
    "print(\"Not Known\",counter, \"over\", len(unique_word), \"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_kti=[list(model_g.key_to_index)]\n",
    "#print(len(g_kti),type(g_kti),g_kti[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(g_kti[0]),type(g_kti[0]),g_kti[0][113])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_kti[0].remove(\"supergroup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(g_kti[0]),type(g_kti[0]),g_kti[0][113])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(w2v)} - W2V\")\n",
    "model = Word2Vec(min_count=1, vector_size=300, epochs=15)\n",
    "model.build_vocab(w2v)\n",
    "\n",
    "\n",
    "print(0,len(model.wv))\n",
    "model.wv.vectors_lockf = np.ones(len(model.wv))\n",
    "training_examples_count = model.corpus_count\n",
    "print(1,training_examples_count)\n",
    "# below line will make it 1, so saving it before\n",
    "model.build_vocab(g_kti, update=True)\n",
    "print(2,len(model.wv))\n",
    "model.wv.vectors_lockf = np.ones(len(model.wv))\n",
    "print(3,len(model.wv))\n",
    "\n",
    "\n",
    "model.wv.intersect_word2vec_format(\"GoogleNews-vectors-negative300.bin\",binary=True) #lockf=1 ->further train g-vectors\n",
    "print(4,model.epochs)\n",
    "model.train(w2v, total_examples=training_examples_count, epochs=model.epochs)\n",
    "print(5,len(model.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diz_w2v={}\n",
    "for el in list_word:\n",
    "    el=el.replace(\" \",\"_\")\n",
    "    vec=model.wv[el]\n",
    "    diz_w2v[el]=vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=[\"supergroup\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=[\"hdfsgroup\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=[\"supergroup_hdfsgroup\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('supergroup','supergroup_hdfsgroup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1=['supergroup']\n",
    "v2=['supergroup','supergroup_hdfsgroup','hdfsgroup']\n",
    "\n",
    "curr_v=np.zeros(300)\n",
    "num_words=0\n",
    "for el in v1:\n",
    "    word_vec=model.wv[el]\n",
    "    curr_v+=word_vec\n",
    "    num_words+=1\n",
    "curr_v=curr_v/num_words\n",
    "\n",
    "\n",
    "act_v=np.zeros(300)\n",
    "num_words=0\n",
    "for el in v2:\n",
    "    word_vec=model.wv[el]\n",
    "    act_v+=word_vec\n",
    "    num_words+=1\n",
    "act_v=act_v/num_words\n",
    "\n",
    "similarity=spatial.distance.cosine(curr_v, act_v)\n",
    "similarity=round(1-similarity,4)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=np.inner(curr_v, act_v)\n",
    "print(num)\n",
    "den1=np.linalg.norm(act_v)\n",
    "den2=np.linalg.norm(curr_v)\n",
    "print(den1)\n",
    "print(den2)\n",
    "den=den1*den2\n",
    "print(den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.n_similarity(['supergroup'],['supergroup','supergroup_hdfsgroup','hdfsgroup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_words=set()\n",
    "for el in tw:\n",
    "    for w in el:\n",
    "        tw_words.add(w)\n",
    "print(len(tw_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v=[]\n",
    "for el in tw_words:\n",
    "    w2v.append([el.replace(\" \",\"_\")])\n",
    "print(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp=data_origin#tw\n",
    "similarities=[]\n",
    "for i,top_wrd in enumerate(tp):\n",
    "    #if i<2:\n",
    "    #    print(f\"{i+1} - {top_wrd}\")\n",
    "    t1=[el.replace(\" \", \"_\") for el in top_wrd]\n",
    "    tmp=tp\n",
    "    tmp_sim=[]\n",
    "    for j,other_topic in enumerate(tmp):\n",
    "        t2=[el.replace(\" \", \"_\") for el in other_topic]\n",
    "        similarity=model.wv.n_similarity(t1,t2)\n",
    "        #if i<2:\n",
    "            #print(f\"\\t{j+1} - {other_topic}\")\n",
    "            #print(f\"\\t\\t{len(top_wrd)} - {len(other_topic)}\")\n",
    "            #print(f\"\\t\\tSimilarity: {similarity}\")\n",
    "        similarity=round(similarity,3)\n",
    "        tmp_sim.append(similarity)\n",
    "    similarities.append(tmp_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2=np.array(similarities)\n",
    "print(s2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_print=[]\n",
    "for i,row in enumerate(s2):\n",
    "    row[i]=0\n",
    "    if i not in to_print and np.max(row)>0.99:\n",
    "        print(i,np.max(row),np.argmax(row))\n",
    "        print(\"MAIN\",data_origin[i])\n",
    "        print(\"OTHER\",data_origin[np.argmax(row)])\n",
    "        to_print.append(i)\n",
    "        to_print.append(np.argmax(row))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_print=s2[33]\n",
    "e1=to_print[19]\n",
    "e2=to_print[33]\n",
    "e3=to_print[60]\n",
    "print(e1,e2,e3)\n",
    "print(tw[19])\n",
    "print(tw[33])\n",
    "print(tw[60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"darkgrid\"):\n",
    "    mask = np.triu(np.ones_like(similarities, dtype=bool),k=1)\n",
    "    plt.figure(figsize=(50,50), dpi=80)\n",
    "    cmap=sns.diverging_palette(240, 10, n=9)\n",
    "    sns.heatmap(similarities,annot=True,mask=mask,\n",
    "              cmap=cmap,\n",
    "              linewidth=2,edgecolor=\"k\",\n",
    "              #square=True\n",
    "              vmin=-1,vmax=1,center=0\n",
    "              )\n",
    "    plt.title(\"Similarity between Topics - DOCUMENT\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_origin[77])\n",
    "print(data_origin[83])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"Raw_Spark_SPLIT.txt\",\"r\")\n",
    "lines=f.readlines()\n",
    "originals=[]\n",
    "for i,line in enumerate(lines):\n",
    "      line=line.rstrip(\"\\n\")\n",
    "      #print(line)\n",
    "      originals.append(line)\n",
    "f.close()\n",
    "print(len(originals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sources=set()\n",
    "list_unique=[]\n",
    "idx=0\n",
    "main_source=set()\n",
    "for row in originals:\n",
    "    print(idx,row)\n",
    "    source=row.split(\" \")[0]\n",
    "    unique_sources.add(source)\n",
    "    first=source.split(\".\")[0]\n",
    "    main_source.add(first)\n",
    "    list_unique.append(first)\n",
    "    idx+=1\n",
    "print(len(unique_sources))\n",
    "print(unique_sources)\n",
    "print(len(main_source))\n",
    "print(main_source)\n",
    "c=Counter(list_unique)\n",
    "print(c)\n",
    "idx=0\n",
    "list_ids=[]\n",
    "for row in originals:\n",
    "    source=row.split(\" \")[0]\n",
    "    first=source.split(\".\")[0]\n",
    "    if c[first]==1:\n",
    "        list_ids.append(idx)\n",
    "    idx+=1\n",
    "print(list_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sim=[]\n",
    "P=0\n",
    "W=0\n",
    "idx=0\n",
    "for v1,v2 in zip(token_list, topic_words):\n",
    "    similarity=model.wv.n_similarity(v1,v2)\n",
    "    #print(similarity)\n",
    "    if similarity<0.65:\n",
    "        W+=1\n",
    "    if similarity==1 and idx in list_ids:\n",
    "        P+=1\n",
    "    score_sim.append(similarity)\n",
    "    idx+=1\n",
    "mean=np.mean(score_sim)\n",
    "print(f\"AVERAGE SIMILARITY INPUT-TOPIC: {mean}\")\n",
    "print(P,W)\n",
    "myscore=mean*(P-W)/(P+W)\n",
    "print(f\"My Score: {myscore}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOC2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v=token_list\n",
    "print(len(w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl=tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.append([\"supergroup\",\"hdfsgroup\"])\n",
    "tl.append([\"supergroup\",\"hdfsgroup\",\"supergroup_hdfsgroup\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus=[TaggedDocument(el,[i]) for i,el in enumerate(tl)]\n",
    "print(len(train_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_corpus[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelD = Doc2Vec(vector_size=100, min_count=1, epochs=20, window=5)\n",
    "modelD.build_vocab(train_corpus)\n",
    "modelD.train(train_corpus, total_examples=modelD.corpus_count, epochs=modelD.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLUSTER COMPARISON\n",
    "tp=tl\n",
    "similarities=[]\n",
    "for i,top_wrd in enumerate(tp):\n",
    "    tmp=tp\n",
    "    tmp_sim=[]\n",
    "    current_v=modelD.infer_vector(top_wrd)\n",
    "    for j,other_topic in enumerate(tmp):\n",
    "\n",
    "        tmp_v=modelD.infer_vector(other_topic)\n",
    "        #similarity=modelD.dv.distance(top_wrd,other_topic)\n",
    "        similarity=spatial.distance.cosine(current_v, tmp_v)\n",
    "        #if i<2:\n",
    "            #print(f\"\\t{j+1} - {other_topic}\")\n",
    "            #print(f\"\\t\\t{len(top_wrd)} - {len(other_topic)}\")\n",
    "            #print(f\"\\t\\tSimilarity: {similarity}\")\n",
    "        tmp_sim.append(1-similarity)\n",
    "    similarities.append(tmp_sim)\n",
    "    #print(f\"{i+1} - {len(tmp_sim)} - {len(top_wrd)} - {top_wrd}\")\n",
    "#print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similaritiesD=np.array(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_print=[]\n",
    "for i,row in enumerate(similaritiesD):\n",
    "    row[i]=0\n",
    "    if i not in to_print and np.max(row)>0.99:\n",
    "        print(i,np.max(row),np.argmax(row))\n",
    "        print(tw[i])\n",
    "        print(tw[np.argmax(row)])\n",
    "        to_print.append(i)\n",
    "        to_print.append(np.argmax(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"darkgrid\"):\n",
    "    mask = np.triu(np.ones_like(similarities, dtype=bool),k=1)\n",
    "    plt.figure(figsize=(50,50), dpi=80)\n",
    "    cmap=sns.diverging_palette(240, 10, n=9)\n",
    "    sns.heatmap(similarities,annot=True,mask=mask,\n",
    "              cmap=cmap,\n",
    "              linewidth=2,edgecolor=\"k\",\n",
    "              #square=True\n",
    "              vmin=-1,vmax=1,center=0\n",
    "              )\n",
    "    plt.title(\"Similarity between Topics - DOCUMENT\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tw[39])\n",
    "print(tw[24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_g= api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v=topics_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(min_count=1, vector_size=300)\n",
    "model.build_vocab(w2v)\n",
    "print(len(model.wv))\n",
    "model.wv.vectors_lockf = np.ones(len(model.wv))\n",
    "training_examples_count = model.corpus_count\n",
    "print(training_examples_count)\n",
    "# below line will make it 1, so saving it before\n",
    "model.build_vocab([list(model_g.key_to_index)], update=True)\n",
    "model.wv.vectors_lockf = np.ones(len(model.wv))\n",
    "print(len(model.wv))\n",
    "model.wv.intersect_word2vec_format(\"GoogleNews-vectors-negative300.bin\",binary=True) #lockf=1 ->further train g-vectors\n",
    "print(model.epochs)\n",
    "model.train(w2v, total_examples=training_examples_count, epochs=model.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLUSTER COMPARISON\n",
    "tp=w2v\n",
    "similarities=[]\n",
    "for i,top_wrd in enumerate(tp):\n",
    "    #if i<2:\n",
    "    #    print(f\"{i+1} - {top_wrd}\")\n",
    "    tmp=tp\n",
    "    tmp_sim=[]\n",
    "    for j,other_topic in enumerate(tmp):\n",
    "        \n",
    "        similarity=model.wv.n_similarity(top_wrd,other_topic)\n",
    "        #if i<2:\n",
    "            #print(f\"\\t{j+1} - {other_topic}\")\n",
    "            #print(f\"\\t\\t{len(top_wrd)} - {len(other_topic)}\")\n",
    "            #print(f\"\\t\\tSimilarity: {similarity}\")\n",
    "        similarity=round(similarity,4)\n",
    "        tmp_sim.append(similarity)\n",
    "    similarities.append(tmp_sim)\n",
    "    #print(f\"{i+1} - {len(tmp_sim)} - {len(top_wrd)} - {top_wrd}\")\n",
    "#print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"darkgrid\"):\n",
    "    mask = np.triu(np.ones_like(similarities, dtype=bool),k=1)\n",
    "    plt.figure(figsize=(25,20), dpi=100)\n",
    "    cmap=sns.diverging_palette(240, 10, n=9)\n",
    "    sns.heatmap(similarities,annot=True,mask=mask,\n",
    "              cmap=cmap,\n",
    "              linewidth=2,edgecolor=\"k\",\n",
    "              #square=True\n",
    "              vmin=-1,vmax=1,center=0\n",
    "              )\n",
    "    plt.title(\"Similarity between Topics\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLUSTER COMPARISON\n",
    "tp=w2v\n",
    "similarities2=[]\n",
    "for i,top_wrd in enumerate(tp):\n",
    "    #if i<2:\n",
    "    #    print(f\"{i+1} - {top_wrd}\")\n",
    "    tmp=tp\n",
    "    tmp_sim=[]\n",
    "    curr_v=np.zeros(300)\n",
    "    num_words=0\n",
    "    for el in top_wrd:\n",
    "        word_vec=model.wv[el]\n",
    "        curr_v+=word_vec\n",
    "        num_words+=1\n",
    "    curr_v=curr_v/num_words\n",
    "    for j,other_topic in enumerate(tmp):\n",
    "        act_v=np.zeros(300)\n",
    "        num_words=0\n",
    "        for el in other_topic:\n",
    "            word_vec=model.wv[el]\n",
    "            act_v+=word_vec\n",
    "            num_words+=1\n",
    "        act_v=act_v/num_words\n",
    "        #similarity=model.wv.similarity(curr_v,act_v)\n",
    "        similarity=spatial.distance.cosine(curr_v, act_v)\n",
    "        similarity=round(1-similarity,4)\n",
    "        #if i<2:\n",
    "        #   print(f\"\\t{j+1} - {other_topic}\")\n",
    "        #    print(f\"\\t\\t{len(top_wrd)} - {len(other_topic)}\")\n",
    "        #    print(f\"\\t\\tSimilarity: {similarity}\")\n",
    "        tmp_sim.append(similarity)\n",
    "    similarities2.append(tmp_sim)\n",
    "    #print(f\"{i+1} - {len(tmp_sim)} - {len(top_wrd)} - {top_wrd}\")\n",
    "#print(similarities2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"darkgrid\"):\n",
    "    mask = np.triu(np.ones_like(similarities2, dtype=bool),k=1)\n",
    "    plt.figure(figsize=(20,20), dpi=80)\n",
    "    cmap=sns.diverging_palette(240, 10, n=9)\n",
    "    sns.heatmap(similarities2,annot=True,mask=mask,\n",
    "              cmap=cmap,\n",
    "              linewidth=2,edgecolor=\"k\",\n",
    "              #square=True\n",
    "              vmin=-1,vmax=1,center=0\n",
    "              )\n",
    "    plt.title(\"Similarity between Topics\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(similarities),type(similarities2))\n",
    "similarities=np.array(similarities,dtype=np.float32)\n",
    "similarities2=np.array(similarities2,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "for el1,el2 in zip(similarities,similarities2):\n",
    "    if el1.tolist()!=el2.tolist():\n",
    "        print(idx,el1)\n",
    "        print(idx,el2)\n",
    "        print(type(el1),type(el2))\n",
    "        idx2=0\n",
    "        for n1,n2 in zip(el1,el2):\n",
    "            if n1!=n2:\n",
    "                print(idx2,n1,n2, type(n1),type(n2))\n",
    "            idx2+=1\n",
    "        print(\"\\n\")\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.array_equal(similarities,similarities2):\n",
    "    print(\"SAME W2V\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities2=similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities=pd.DataFrame(similarities)\n",
    "upper_tri = similarities.where(np.triu(np.ones(similarities.shape),k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.85)]\n",
    "similarities_dropped=similarities.drop(columns=to_drop)\n",
    "print(\"Features to be dropped:\\n\", to_drop)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
