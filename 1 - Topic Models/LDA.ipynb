{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psutil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:                      Intel(R) Xeon(R) Gold 6140 CPU @ 2.30GHz\n"
     ]
    }
   ],
   "source": [
    "!lscpu |grep 'Model name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Socket(s):                       2\n"
     ]
    }
   ],
   "source": [
    "!lscpu | grep 'Socket(s):'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core(s) per socket:              18\n"
     ]
    }
   ],
   "source": [
    "!lscpu | grep 'Core(s) per socket:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scpustats(ctx_switches=21073151792, interrupts=38202371841, soft_interrupts=35070448199, syscalls=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: wordcloud in /home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages (1.8.0)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from wordcloud) (8.1.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from wordcloud) (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages (from wordcloud) (1.19.5)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages/numpy-1.19.5.dist-info/METADATA'\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/envs/bigdatalab_cpu_202101/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: pyldavis in /home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages (3.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from pyldavis) (51.0.0.post20201207)\n",
      "Requirement already satisfied: sklearn in /home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages (from pyldavis) (0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.7/site-packages (from pyldavis) (0.23.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.7/site-packages (from pyldavis) (1.5.2)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages (from pyldavis) (1.3.3)\n",
      "Requirement already satisfied: gensim in /home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages (from pyldavis) (4.1.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.7/site-packages (from pyldavis) (2.11.2)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.7/site-packages (from pyldavis) (1.0.0)\n",
      "Requirement already satisfied: future in /opt/anaconda3/lib/python3.7/site-packages (from pyldavis) (0.18.2)\n",
      "Requirement already satisfied: numexpr in /opt/anaconda3/lib/python3.7/site-packages (from pyldavis) (2.7.1)\n",
      "Collecting numpy>=1.20.0\n",
      "  Using cached numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Requirement already satisfied: funcy in /home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages (from pyldavis) (1.16)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/lib/python3.7/site-packages (from pandas>=1.2.0->pyldavis) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.7/site-packages (from pandas>=1.2.0->pyldavis) (2.8.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages (from gensim->pyldavis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.7/site-packages (from jinja2->pyldavis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->pyldavis) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyldavis) (1.15.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Error parsing requirements for numpy: [Errno 2] No such file or directory: '/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages/numpy-1.19.5.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: numpy 1.19.5\n",
      "\u001b[31mERROR: Cannot uninstall numpy 1.19.5, RECORD file not found. You might be able to recover from this via: 'pip install --force-reinstall --no-deps numpy==1.19.5'.\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement gensim==4.1.2#3.8.3 (from versions: 0.2, 0.3.0, 0.4, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.5, 0.4.6, 0.4.7, 0.5.0, 0.6.0, 0.7.0rc2, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7, 0.7.8, 0.8.0rc1, 0.8.0, 0.8.1, 0.8.2, 0.8.3, 0.8.4, 0.8.5, 0.8.6, 0.8.7, 0.8.8, 0.8.9, 0.9.0, 0.9.1, 0.10.0rc1, 0.10.0, 0.10.1, 0.10.2, 0.10.3, 0.11.1, 0.11.1.post1, 0.12.0rc1, 0.12.0, 0.12.1rc1, 0.12.1, 0.12.2, 0.12.3rc1, 0.12.3, 0.12.4, 0.13.0rc1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.13.4, 0.13.4.1, 1.0.0rc1, 1.0.0rc2, 1.0.0, 1.0.1, 2.0.0, 2.1.0, 2.2.0, 2.3.0, 3.0.0, 3.0.1, 3.1.0, 3.2.0, 3.3.0, 3.4.0, 3.5.0, 3.6.0, 3.7.0, 3.7.1, 3.7.2, 3.7.3, 3.8.0, 3.8.1, 3.8.2, 3.8.3, 4.0.0, 4.0.1, 4.1.0, 4.1.1, 4.1.2)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for gensim==4.1.2#3.8.3\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gensim==4.1.2#3.8.3 #ISSUES->SOLVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: lda2vec in /home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages (0.16.10)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Error parsing requirements for numpy: [Errno 2] No such file or directory: '/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages/numpy-1.19.5.dist-info/METADATA'\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install lda2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: numpy in /home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages (1.19.5)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Error parsing requirements for numpy: [Errno 2] No such file or directory: '/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages/numpy-1.19.5.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: numpy 1.19.5\n",
      "\u001b[31mERROR: Cannot uninstall numpy 1.19.5, RECORD file not found. You might be able to recover from this via: 'pip install --force-reinstall --no-deps numpy==1.19.5'.\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/envs/bigdatalab_cpu_202101/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: yellowbrick in /home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages (1.3.post1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from yellowbrick) (0.23.2)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from yellowbrick) (0.10.0)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from yellowbrick) (3.3.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from yellowbrick) (1.5.2)\n",
      "Requirement already satisfied: numpy<1.20,>=1.16.0 in /home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages (from yellowbrick) (1.19.5)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from cycler>=0.10.0->yellowbrick) (1.15.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.2.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (8.1.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2021.5.30)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.1)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages/numpy-1.19.5.dist-info/METADATA'\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/envs/bigdatalab_cpu_202101/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install yellowbrick "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: ipykernel in /home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages (6.9.0)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages (from ipykernel) (0.1.3)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages (from ipykernel) (1.5.0)\n",
      "Requirement already satisfied: traitlets<6.0,>=5.1.0 in /home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages (from ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from ipykernel) (6.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from ipykernel) (1.4.3)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages (from ipykernel) (7.28.0)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from ipykernel) (6.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from ipython>=7.23.1->ipykernel) (0.17.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from ipython>=7.23.1->ipykernel) (4.8.0)\n",
      "Requirement already satisfied: pygments in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from ipython>=7.23.1->ipykernel) (2.7.4)\n",
      "Requirement already satisfied: backcall in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from ipython>=7.23.1->ipykernel) (50.3.2.post20201027)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from ipython>=7.23.1->ipykernel) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from ipython>=7.23.1->ipykernel) (3.0.8)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel) (17.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel) (2.8.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel) (4.7.1)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel) (1.15.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Error parsing requirements for numpy: [Errno 2] No such file or directory: '/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages/numpy-1.19.5.dist-info/METADATA'\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/bigdata-01QYD/s270005/.local/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/envs/bigdatalab_cpu_202101/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import unicodedata\n",
    "import re\n",
    "import csv\n",
    "import pickle \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.7 from \"/opt/anaconda3/envs/bigdatalab_cpu_202101/bin/python\"\n  * The NumPy version is: \"1.19.5\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: libopenblasp-r0-09e95953.3.13.so: cannot open shared object file: No such file or directory\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/core/multiarray.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_multiarray_umath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m from numpy.core._multiarray_umath import (\n\u001b[0m\u001b[1;32m      8\u001b[0m     add_docstring, implement_array_function, _get_implementing_args)\n",
      "\u001b[0;31mImportError\u001b[0m: libopenblasp-r0-09e95953.3.13.so: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_945283/94671785.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;31m# cbook must import matplotlib only within function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;31m# definitions, so it is safe to import from it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcsetup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m from matplotlib.cbook import (\n\u001b[1;32m    140\u001b[0m     MatplotlibDeprecationWarning, dedent, get_label, sanitize_sequence)\n",
      "\u001b[0;32m/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mweakref\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWeakMethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \"\"\" % (sys.version_info[0], sys.version_info[1], sys.executable,\n\u001b[1;32m     47\u001b[0m         __version__, exc)\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0menvkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_added\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.7 from \"/opt/anaconda3/envs/bigdatalab_cpu_202101/bin/python\"\n  * The NumPy version is: \"1.19.5\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: libopenblasp-r0-09e95953.3.13.so: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import yellowbrick\n",
    "from yellowbrick.text import FreqDistVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "#from gensim.models.wrappers import DtmModel\n",
    "#from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models.nmf import Nmf\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords') #GUARDARE PIU AVANZATE -> NN\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = [f for f in os.listdir('.') if os.path.isfile(f)]\n",
    "for f in files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Origin=True\n",
    "name=\"HDFS\"\n",
    "#name=\"Spark\"\n",
    "method=\"MESSAGE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LOAD DATA\")\n",
    "if Origin:\n",
    "    print(\"\\tCONSIDER ORIGIN\")\n",
    "    filename = filename = f\"/home/bigdata-01QYD/s270005/Tesi/Final/LDA/{name}_InputData_{method}.txt\"\n",
    "else:\n",
    "    print(\"\\tNOT CONSIDER ORIGIN\")\n",
    "    filename = f\"{name}_InputData_Message.txt\"\n",
    "print(f\"READ FILE: {filename}\")\n",
    "data=[]\n",
    "with open(filename, 'r') as csvfile:\n",
    "    rows = csv.reader(csvfile, delimiter='\\t')\n",
    "    for i,row in enumerate(rows):\n",
    "        tmp = row[0].replace(\"[\", \"\")\n",
    "        tmp = tmp.replace(\"]\", \"\")\n",
    "        tmp = tmp.replace(\"'\", \"\")\n",
    "        row_l = list(tmp.split(\", \"))\n",
    "        if i<30:\n",
    "            print(i, row_l)\n",
    "        data.append(row_l)\n",
    "print(\"END LOAD DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOPIC=len(data)\n",
    "print(MAX_TOPIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"LDA - MULTICORE\")\n",
    "topicsLDA=list(np.arange(10, 41, 1)) #STEP DI 5 PER WS\n",
    "print(f\"TOPICS: {topicsLDA}\")\n",
    "\n",
    "tmp=[0.01,0.05]\n",
    "tmp=tmp+list(np.arange(0.1, 1.1, 0.1))\n",
    "#tmp=tmp+list(np.arange(0.1, 11, 0.5))\n",
    "alpha=[]\n",
    "for el in tmp:\n",
    "    alpha.append(round(el,3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric') #-> topic_idex\n",
    "#alpha.append('auto')\n",
    "print(f\"ALPHA: {alpha}\")\n",
    "\n",
    "#'''\n",
    "tmp=[0.01, 0.05]\n",
    "tmp = tmp+list(np.arange(0.1, 1.1, 0.1))\n",
    "#tmp=tmp+list(np.arange(0.1, 11, 0.5))\n",
    "beta=[]\n",
    "for el in tmp:\n",
    "    beta.append(round(el,3))\n",
    "beta.append('symmetric')\n",
    "beta.append(\"auto\")\n",
    "print(f\"BETA: {beta}\")    \n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word=corpora.Dictionary(data)\n",
    "corpus=[id2word.doc2bow(word) for word in data]\n",
    "print(len(corpus), len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT \n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import gensim.downloader\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from sklearn.tree import export_graphviz\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODELS\n",
    "#model_g= api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(f\"/home/bigdata-01QYD/s270005/Tesi/Final/LDA/{name}_w2vdict_{method}.pkl\",\"rb\")\n",
    "modelW2V=pickle.load(f)#Word2Vec.load(pathname+\"WORD2VEC/word2vec_SPLIT.model\")\n",
    "#print(modelW2V)\n",
    "print(type(modelW2V))\n",
    "print(modelW2V.keys())\n",
    "print(len(modelW2V.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "def CLUSTERSIMILARITY2(tp, modelW2V, idx):\n",
    "    similarities2=[]\n",
    "    max=[]\n",
    "    for i,top_wrd in enumerate(tp):\n",
    "        tmp=tp\n",
    "        tmp_sim=[]\n",
    "        curr_v=np.zeros(300)\n",
    "        num_words=0\n",
    "        for el in top_wrd:\n",
    "            word_vec=modelW2V[el]\n",
    "            curr_v+=word_vec\n",
    "            num_words+=1\n",
    "        curr_v=curr_v/num_words\n",
    "        for j,other_topic in enumerate(tmp):\n",
    "            act_v=np.zeros(300)\n",
    "            num_words=0\n",
    "            for el in other_topic:\n",
    "                word_vec=modelW2V[el]\n",
    "                act_v+=word_vec\n",
    "                num_words+=1\n",
    "            act_v=act_v/num_words\n",
    "            similarity=spatial.distance.cosine(curr_v, act_v)\n",
    "            similarity=round(1-similarity,4)\n",
    "            flag=0\n",
    "            for el in top_wrd:\n",
    "                if el in other_topic:\n",
    "                    flag+=1\n",
    "            if flag==len(top_wrd) and i!=j and \"supergroup\" in top_wrd:\n",
    "                print(f\"ALL ELEMENT OF TOPIC {i} ARE INSIDE TOPIC {j}\")\n",
    "                print(i,top_wrd)\n",
    "                print(j,other_topic)\n",
    "                tmp_sim.append(-1)\n",
    "            else:\n",
    "                tmp_sim.append(similarity)\n",
    "            if j!=i:\n",
    "                max.append(similarity)\n",
    "        similarities2.append(tmp_sim)\n",
    "\n",
    "    max=np.array(max)\n",
    "    max=-np.sort(-max)\n",
    "    max5=max[:5]\n",
    "    similarities2=pd.DataFrame(similarities2)\n",
    "    upper_tri = similarities2.where(np.triu(np.ones(similarities2.shape),k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.85)] #MORE THAN 90% of WORD IN COMMON\n",
    "    similarities2_dropped=similarities2.drop(columns=to_drop)\n",
    "    delta_k=len(to_drop)\n",
    "    print(f\"{idx}) Features to be dropped:{to_drop} - {len(to_drop)} - max: {max5}\")\n",
    "    \n",
    "    return delta_k,to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS\n",
    "def TRAINLDA(topics):\n",
    "    start_time = datetime.now()\n",
    "    best_coherence=0\n",
    "    best_alpha=0\n",
    "    best_beta=0\n",
    "    best_k=MAX_TOPIC+1\n",
    "    elapsed_time=datetime.now()\n",
    "    prev_time=start_time\n",
    "    elapsed_time-=prev_time\n",
    "    \n",
    "    df_log=pd.DataFrame(columns=[\"k\",\"alpha\",\"eta\",\"coherence\",\"delta_k\"])\n",
    "    idx=0\n",
    "    for k in tqdm(topics):\n",
    "        print(f\"Topic - Alpha - Eta - Coherence\") #[0,1]')\n",
    "        for a in alpha:\n",
    "            print(\"-\"*80)\n",
    "            for b in beta:\n",
    "                ldaM_model=LdaMulticore(corpus=corpus, id2word=id2word, num_topics=k, iterations=100, passes=10, random_state=0, alpha=a, eta=b)\n",
    "                cm = CoherenceModel(model=ldaM_model, dictionary=id2word, texts=data, coherence='c_v') #110\n",
    "                coherence = cm.get_coherence() \n",
    "                topics = ldaM_model.show_topics(num_topics=k,num_words=20,formatted=False)\n",
    "                topics_words=[]\n",
    "                for i in range(k):\n",
    "                    topic_words = dict(topics[i][1])\n",
    "                    tmp=list(topic_words.keys())\n",
    "                    tmp2=[el.replace(\" \",\"_\") for el in tmp]\n",
    "                    topics_words.append(tmp2)\n",
    "                delta_k,to_drop=CLUSTERSIMILARITY2(topics_words, modelW2V,idx)\n",
    "                if coherence>=best_coherence:\n",
    "                    best_coherence=coherence\n",
    "                    best_k=k\n",
    "                    best_alpha=a\n",
    "                    best_beta=b\n",
    "                    print(f\"{idx}) --> [BEST]: {int(k):02}    -    {a}    -    {b}    -    {round(coherence,5)}\") #[0,1]')\n",
    "                else:\n",
    "                    print(f\"{idx}) {int(k):02}    -    {a}    -    {b}    -    {round(coherence,5)}\") #[0,1]')\n",
    "                #print(\"\\n\")\n",
    "                df_log.loc[idx]=[k,a,b,coherence,delta_k]\n",
    "                idx+=1\n",
    "                tmp_time=datetime.now()\n",
    "                delta_time=tmp_time-prev_time\n",
    "                elapsed_time+=delta_time\n",
    "                prev_time=tmp_time\n",
    "        print(f\"ELAPSED TIME: {elapsed_time}\\n\")\n",
    "    end_time = datetime.now()\n",
    "    print('Duration TRAIN: {}'.format(end_time - start_time))\n",
    "    \n",
    "    return best_k, best_alpha, best_beta, df_log\n",
    "\n",
    "def WORDCLOUD(best_k, best_lda_model, best_alpha, best_beta):\n",
    "    tmp = [color for name, color in mcolors.XKCD_COLORS.items()]#mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "    offset=int(len(tmp)/best_k)\n",
    "    #print(offset)\n",
    "    cols=tmp[1::offset]\n",
    "    #print(cols)\n",
    "\n",
    "    cloud = WordCloud(background_color='white',\n",
    "                      width=2500,\n",
    "                      height=1800,\n",
    "                      max_words=20,\n",
    "                      colormap='tab10',\n",
    "                      color_func=lambda *args, **kwargs: cols[i],\n",
    "                      prefer_horizontal=1.0)\n",
    "\n",
    "    topics = best_lda_model.show_topics(num_topics=best_k,num_words=20,formatted=False)\n",
    "    \n",
    "    plotx=np.floor(np.sqrt(best_k))\n",
    "    ploty=plotx\n",
    "    flag=True\n",
    "    while plotx*ploty < best_k:\n",
    "        if flag:\n",
    "            ploty+=1\n",
    "            flag=False\n",
    "        else:\n",
    "            plotx+=1\n",
    "\n",
    "    fig, axes = plt.subplots(int(plotx), int(ploty), figsize=(30,30), sharex=True, sharey=True)\n",
    "    axes=axes.flatten()\n",
    "    topics_words=[]\n",
    "    j=0\n",
    "    for i in range(best_k):\n",
    "        fig.add_subplot(axes[i])\n",
    "        topic_words = dict(topics[i][1])\n",
    "        topics_words.append(list(topic_words.keys()))\n",
    "        cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "        plt.gca().imshow(cloud)\n",
    "        plt.gca().set_title(f'Topic {j+i}', fontdict=dict(size=16))\n",
    "        plt.gca().axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=15, hspace=15)\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{name}_Best_Score_k={best_k}_a={best_alpha}_b={best_beta}_WC.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    return topics_words\n",
    "\n",
    "def W2VMODEL(topics_words):\n",
    "    w2v=[]\n",
    "    for tw in topics_words:\n",
    "        tmp=[]\n",
    "        for el in tw:\n",
    "            splits=el.split(\" \")\n",
    "            tmp.append(\"_\".join(splits))\n",
    "        #print(tmp)\n",
    "        w2v.append(tmp)\n",
    "\n",
    "    model = Word2Vec(min_count=1, vector_size=300)\n",
    "    model.build_vocab(w2v)\n",
    "    model.wv.vectors_lockf = np.ones(len(model.wv))\n",
    "    training_examples_count = model.corpus_count\n",
    "    # below line will make it 1, so saving it before\n",
    "    model.build_vocab([list(model_g.key_to_index)], update=True)\n",
    "    model.wv.vectors_lockf = np.ones(len(model.wv))\n",
    "    model.wv.intersect_word2vec_format(\"GoogleNews-vectors-negative300.bin\",binary=True)\n",
    "    model.train(w2v, total_examples=training_examples_count, epochs=model.epochs)\n",
    "    \n",
    "    return model, w2v\n",
    "\n",
    "def CLUSTERSIMILARITY(tp, model, best_alpha, best_beta):\n",
    "    similarities=[]\n",
    "    for i,top_wrd in enumerate(tp):\n",
    "        print(f\"{i+1} - {top_wrd}\")\n",
    "        tmp=tp\n",
    "        tmp_sim=[]\n",
    "        for j,other_topic in enumerate(tmp):\n",
    "            similarity=model.wv.n_similarity(top_wrd,other_topic)\n",
    "            tmp_sim.append(similarity)\n",
    "        similarities.append(tmp_sim)\n",
    "\n",
    "    with sns.axes_style(\"darkgrid\"):\n",
    "        mask = np.triu(np.ones_like(similarities, dtype=bool),k=1)\n",
    "        plt.figure(figsize=(20,20), dpi=80)\n",
    "        cmap=sns.diverging_palette(240, 10, n=9)\n",
    "        sns.heatmap(similarities,annot=True,mask=mask,cmap=cmap,linewidth=2,edgecolor=\"k\",vmin=-1,vmax=1,center=0)\n",
    "        plt.title(\"Similarity between Topics\")\n",
    "        plt.savefig(f\"{name}_Best_Score_k={best_k}_a={best_alpha}_b={best_beta}_SM.png\")\n",
    "        plt.show()\n",
    "\n",
    "    similarities=pd.DataFrame(similarities)\n",
    "    upper_tri = similarities.where(np.triu(np.ones(similarities.shape),k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)] #MORE THAN 90% of WORD IN COMMON\n",
    "    similarities_dropped=similarities.drop(columns=to_drop)\n",
    "    delta_k=len(to_drop)\n",
    "    print(f\"Features to be dropped:{to_drop} - {len(to_drop)}\\n\")\n",
    "    \n",
    "    return delta_k,to_drop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k,best_alpha,best_beta,df_log=TRAINLDA(topicsLDA)\n",
    "df_log.to_csv(f\"/home/bigdata-01QYD/s270005/Tesi/Final/LDA/{name}_Train_History_{method}_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname=f\"/home/bigdata-01QYD/s270005/Tesi/Final/LDA/{name}_Best_Score_Results_Step_{min(topics)}_{max(topics)}.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "res_df=pd.read_csv(f\"{name}_Train_History.csv\")#pd.DataFrame(dummy_res,columns=[\"num_topics\",\"alpha\",\"eta\",\"coherence\"])\n",
    "res_df=res_df.drop(\"row\",axis=1)\n",
    "print(res_df.head())\n",
    "final_df=pd.DataFrame(columns=[\"num_topics\",\"alpha\",\"eta\",\"coherence\",\"delta_k\"])\n",
    "\n",
    "final_df=res_df.sort_values(by=['coherence'], ascending=False)\n",
    "#trial_df=final_df.drop(\"coherence\",axis=1)\n",
    "print(final_df.head())\n",
    "final_df.to_csv(f\"{name}_Train_History_SORTED.csv\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100=final_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"darkgrid\"):\n",
    "    pd.DataFrame.hist(top100, figsize = [25,10], bins=50, color=\"purple\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(top100)\n",
    "print(type(top100), top100.shape)\n",
    "print(top100.describe())\n",
    "Q1 = top100[\"k\"].quantile(0.25)\n",
    "Q3 = top100[\"k\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "LB=np.ceil((Q1 - 1.5 * IQR))\n",
    "UB=np.ceil((Q3 + 1.5 * IQR))\n",
    "print(Q1,Q3,IQR,LB,UB)\n",
    "df = top100.drop(top100[top100.k <= LB].index)\n",
    "print(df.shape)\n",
    "df = df.drop(df[df.k >= UB].index)\n",
    "print(df.shape)\n",
    "final_df=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"darkgrid\"):\n",
    "    pd.DataFrame.hist(final_df, figsize = [25,10], bins=50, color=\"purple\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0,23):\n",
    "    row=trial_df.iloc[[idx]]\n",
    "    #print(row)\n",
    "    dict_params=row.to_dict('r')[0]\n",
    "    #print(dict_params)\n",
    "    best_k=int(dict_params[\"k\"])\n",
    "    best_alpha=dict_params[\"alpha\"]\n",
    "    best_beta=dict_params[\"eta\"]\n",
    "    best_cohe=dict_params[\"coherence\"]\n",
    "    if len(best_alpha)<5:\n",
    "        best_alpha=float(dict_params[\"alpha\"])\n",
    "    if len(best_beta)<5:\n",
    "        best_beta=float(dict_params[\"eta\"])\n",
    "        \n",
    "    print(f\"Num_topic={best_k}, alpha={best_alpha}, eta={best_beta}, coherence={round(best_cohe,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k,best_alpha,best_beta,df_log=TRAINLDA(topics)\n",
    "df_log.to_csv(f\"{name}_Train_History_{min(topics)}_{max(topics)}_SPLIT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "delta_k=1\n",
    "train=True\n",
    "dummy_res=[[15,0.1,0.1,0.5],[15,0.3,0.4,0.65],[15,0.8,0.5,0.55],[16,0.9,0.9,0.7]]\n",
    "res_df=pd.DataFrame(dummy_res,columns=[\"num_topics\",\"alpha\",\"eta\",\"coherence\"])\n",
    "#print(res_df)\n",
    "final_df=pd.DataFrame(columns=[\"num_topics\",\"alpha\",\"eta\",\"coherence\"])\n",
    "\n",
    "#f = open(fname, \"w\")\n",
    "    \n",
    "if train:\n",
    "    print(\"TRAIN\")\n",
    "    best_k,best_alpha,best_beta,df_log=TRAINLDA(topics)\n",
    "    df_log.to_csv(f\"{name}_Train_History_{min(topics)}_{max(topics)}_SPLIT.csv\")\n",
    "    res_df=df_log\n",
    "    train=False\n",
    "\n",
    "final_df=res_df.sort_values(by=['coherence'], ascending=False)\n",
    "trial_df=final_df.drop(\"coherence\",axis=1)\n",
    "print(trial_df)\n",
    "'''\n",
    "f=open(fname,\"w\")\n",
    "trial_df=final_df\n",
    "for idx in range(11,trial_df.shape[0]):\n",
    "    row=trial_df.iloc[[idx]]\n",
    "    print(row)\n",
    "    dict_params=row.to_dict('r')[0]\n",
    "    print(dict_params)\n",
    "    best_k=int(dict_params[\"k\"])\n",
    "    best_alpha=dict_params[\"alpha\"]\n",
    "    best_beta=dict_params[\"eta\"]\n",
    "    \n",
    "    if len(best_alpha)<5:\n",
    "        best_alpha=float(dict_params[\"alpha\"])\n",
    "    if len(best_beta)<5:\n",
    "        best_beta=float(dict_params[\"eta\"])\n",
    "        \n",
    "    print(best_k, best_alpha, best_beta)\n",
    "    best_lda_model = LdaMulticore(corpus=corpus, id2word=id2word,  num_topics=best_k, iterations=100, passes=10, random_state=42, alpha=best_alpha, eta=best_beta)\n",
    "\n",
    "    cm = CoherenceModel(model=best_lda_model, dictionary=id2word, texts=data, coherence='c_v') \n",
    "    coherence = cm.get_coherence() \n",
    "    print(f\"\\nCOHERENCE: {coherence}\")\n",
    "\n",
    "    print(\"\\nWORDCLOUDS\")\n",
    "    topics_words=WORDCLOUD(best_k,best_lda_model,best_alpha,best_beta)\n",
    "\n",
    "    print(\"\\nWORD2VEC\")\n",
    "    model, w2v=W2VMODEL(topics_words)\n",
    "\n",
    "    print(\"\\nCLUSTER SIMILARITY\")\n",
    "    delta_k, to_drop=CLUSTERSIMILARITY(w2v, model,best_alpha,best_beta)\n",
    "    print(f\"k={best_k}, alpha={best_alpha}, eta={best_beta}, coherence={coherence}, to_drop={to_drop}\",file=f)\n",
    "    if delta_k == 0:\n",
    "        break\n",
    "        \n",
    "f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_k, best_alpha, best_beta)\n",
    "#best_lda_model = LdaMulticore(corpus=corpus, id2word=id2word, num_topics=best_k, iterations=100, passes=10, random_state=42, alpha=best_alpha, eta=best_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topics_words)\n",
    "input_matrix_sim=[]\n",
    "for el in topics_words:\n",
    "    tmp_2=[]\n",
    "    for el2 in el:\n",
    "        tmp_2.append((el2,1))\n",
    "    input_matrix_sim.append(tmp_2)\n",
    "print(corpus)\n",
    "print(input_matrix_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(best_lda_model[input_matrix_sim]) \n",
    "index.save('/tmp/deerwester.index')\n",
    "index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')\n",
    "\n",
    "similarities_2=[]\n",
    "for n,tp in enumerate(topics_words):\n",
    "    #print(f\"{n+1} - {tp}\")\n",
    "    vec_bow = id2word.doc2bow(tp)\n",
    "    vec_lda = best_lda_model[vec_bow]  # convert the query to LSI space\n",
    "    #print(vec_lsi)\n",
    "    sims = index[vec_lda] \n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "    tmp_sim=[]\n",
    "    for doc_position, doc_score in sims:\n",
    "        #print(f\"\\t{doc_score, input_data[doc_position]}\")\n",
    "        tmp_sim.append(doc_score)\n",
    "    print(f\"{n+1} - {len(tmp_sim)} - {tp}\")\n",
    "    similarities_2.append(tmp_sim)\n",
    "\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    mask = np.triu(np.ones_like(similarities_2, dtype=bool),k=1)\n",
    "    plt.figure(figsize=(20,20), dpi=80)\n",
    "    cmap=sns.diverging_palette(240, 10, n=9)\n",
    "    sns.heatmap(similarities_2,annot=True,mask=mask,\n",
    "                  cmap=cmap,\n",
    "                  linewidth=2,edgecolor=\"k\",\n",
    "                  #square=True\n",
    "                  vmin=-1,vmax=1,center=0\n",
    "              )\n",
    "    plt.title(\"Similarity between Topics\")\n",
    "    plt.show()\n",
    "\n",
    "    '''\n",
    "    similarities=pd.DataFrame(similarities)\n",
    "    upper_tri = similarities.where(np.triu(np.ones(similarities.shape),k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)] #MORE THAN 90% of WORD IN COMMON\n",
    "    similarities_dropped=similarities.drop(columns=to_drop)\n",
    "    delta_k=len(to_drop)\n",
    "    print(f\"Features to be dropped:{to_drop} - {len(to_drop)}\\n\")\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## LDA VISUALIZATION ##################################\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(best_lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## WORD COUNT AND IMPORTANCE ##########################\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "topics = best_lda_model.show_topics(num_topics=best_k,formatted=False)\n",
    "data_flat = [w for w_list in data for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "plot_size=int(np.ceil(np.sqrt(best_k)))\n",
    "fig, axes = plt.subplots(plot_size, plot_size, figsize=(30,30), sharey=True, dpi=160)\n",
    "tmp = [color for name, color in mcolors.XKCD_COLORS.items()]#mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "offset=int(len(tmp)/best_k)\n",
    "print(offset)\n",
    "cols=tmp[1::offset]\n",
    "print(cols)\n",
    "\n",
    "axes=axes.flatten()\n",
    "for i in range(best_k):\n",
    "    ax=axes[i]\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### SENTENCE TOPIC DOMINANCE ###############################\n",
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=best_lda_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.to_csv(\"Spark Results\")\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### SENTENCE CHART TOPIC COLOR ######################################\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def sentences_chart(lda_model=best_lda_model, corpus=corpus, start = 0, end = 39):\n",
    "    corp = corpus[start:end]\n",
    "    tmp = [color for name, color in mcolors.XKCD_COLORS.items()]#mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "    offset=int(len(tmp)/best_k)\n",
    "    print(offset)\n",
    "    mycolors=tmp[1::offset]\n",
    "    print(cols)\n",
    "\n",
    "    fig, axes = plt.subplots(end-start, 1, figsize=(30, (end-start)*0.95), dpi=150)       \n",
    "    axes[0].axis('off')\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.axis(\"off\")\n",
    "        if i > 0:\n",
    "            corp_cur = corp[i-1] \n",
    "            #print(corp_cur)\n",
    "            topic_percs = lda_model[corp_cur]\n",
    "            #print(topic_percs)\n",
    "            #wordid_topics = \n",
    "            word_dominanttopic=[]\n",
    "            for id,f in corp_cur:\n",
    "                w=lda_model.id2word[id]\n",
    "                t=lda_model.get_term_topics(id)\n",
    "                #print(w,f,t)\n",
    "                #print(\"\\n\")\n",
    "                if len(t)>0:\n",
    "                    word_dominanttopic.append((w,t[0][0]))\n",
    "            #print(lda_model.get_document_topics(corp_cur))\n",
    "            #word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n",
    "            ax.text(0.01, 0.5, \"Doc \" + str(i-1) + \": \", verticalalignment='center',\n",
    "                    fontsize=16, color='black', transform=ax.transAxes, fontweight=700)\n",
    "\n",
    "            # Draw Rectange\n",
    "            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n",
    "            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill=None, alpha=1, \n",
    "                                   color=mycolors[topic_percs_sorted[0][0]], linewidth=2))\n",
    "\n",
    "            word_pos = 0.06\n",
    "            for j, (word, topics) in enumerate(word_dominanttopic):\n",
    "                if j < 14:\n",
    "                    ax.text(word_pos, 0.3, word,\n",
    "                            horizontalalignment='left',\n",
    "                            verticalalignment='center',\n",
    "                            fontsize=16, color=mycolors[topics],\n",
    "                            transform=ax.transAxes, fontweight=700)\n",
    "                    mul=len(word)\n",
    "                    if len(word)>15:\n",
    "                        mul=mul/1.1\n",
    "                    word_pos += .005 * mul  # to move the word for the next iter\n",
    "                    ax.axis('off')\n",
    "            ax.text(word_pos, 0.5, '. . .',\n",
    "                    horizontalalignment='left',\n",
    "                    verticalalignment='center',\n",
    "                    fontsize=16, color='black',\n",
    "                    transform=ax.transAxes)       \n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.suptitle('Sentence Topic Coloring for Documents: ' + str(start) + ' to ' + str(end-2), fontsize=22, y=0.99, x=0.5, fontweight=700)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "sentences_chart()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "reload(bokeh.io.output_notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### T-SNE  ###################################\n",
    "\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(best_lda_model[corpus]):\n",
    "    #print(row_list)\n",
    "    topic_weights.append([w for i, w in row_list])\n",
    "\n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "print(arr.shape)\n",
    "\n",
    "# Keep the well separated points (optional)\n",
    "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "print(arr.shape)\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "print(tsne_lda.shape)\n",
    "print(type(tsne_lda)\n",
    "\n",
    "# Plot the Topic Clusters using Bokeh\n",
    "output_notebook()\n",
    "n_topics = best_k\n",
    "\n",
    "tmp = [color for name, color in mcolors.XKCD_COLORS.items()]#mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "offset=int(len(tmp)/best_k)\n",
    "#print(offset)\n",
    "mycolors=tmp[1::offset]\n",
    "#print(cols)\n",
    "\n",
    "\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), plot_width=900, plot_height=700)\n",
    "plot.scatter(x=np.array(tsne_lda[:,0]), y=np.array(tsne_lda[:,1]), color=np.array(mycolors[topic_num]))\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from gensim import interfaces\n",
    "from gensim.topic_coherence import (segmentation, probability_estimation, direct_confirmation_measure, indirect_confirmation_measure, aggregation)\n",
    "from gensim.matutils import argsort\n",
    "from gensim.utils import is_corpus, FakeDict\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.wrappers import LdaVowpalWabbit, LdaMallet\n",
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "boolean_document_based = ['u_mass']\n",
    "sliding_window_based = ['c_v', 'c_uci', 'c_npmi']\n",
    "make_pipeline = namedtuple('Coherence_Measure', 'seg, prob, conf, aggr')\n",
    "\n",
    "coherence_dict = {\n",
    "    'c_v': make_pipeline(segmentation.s_one_set,\n",
    "                         probability_estimation.p_boolean_sliding_window,\n",
    "                         indirect_confirmation_measure.cosine_similarity,\n",
    "                         aggregation.arithmetic_mean),\n",
    "}\n",
    "\n",
    "sliding_windows_dict = {\n",
    "    'c_v': 110,\n",
    "    'c_uci': 10,\n",
    "    'c_npmi': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCoherenceModel(interfaces.TransformationABC):\n",
    "\n",
    "    def __init__(self, model=None, topics=None, texts=None, corpus=None, dictionary=None, window_size=None, coherence='c_v', topn=10):\n",
    "\n",
    "        if model is None and topics is None:\n",
    "            raise ValueError(\"One of model or topics has to be provided.\")\n",
    "        elif topics is not None and dictionary is None:\n",
    "            raise ValueError(\"dictionary has to be provided if topics are to be used.\")\n",
    "        if texts is None and corpus is None:\n",
    "            raise ValueError(\"One of texts or corpus has to be provided.\")\n",
    "        # Check if associated dictionary is provided.\n",
    "        if dictionary is None:\n",
    "            if isinstance(model.id2word, FakeDict):\n",
    "                raise ValueError(\"The associated dictionary should be provided with the corpus or 'id2word' for topic model\"\n",
    "                                 \" should be set as the associated dictionary.\")\n",
    "            else:\n",
    "                self.dictionary = model.id2word\n",
    "        else:\n",
    "            self.dictionary = dictionary\n",
    "        \n",
    "        # Check for correct inputs for u_mass coherence measure.\n",
    "        if coherence in boolean_document_based:\n",
    "            if is_corpus(corpus)[0]:\n",
    "                self.corpus = corpus\n",
    "            elif texts is not None:\n",
    "                self.texts = texts\n",
    "                self.corpus = [self.dictionary.doc2bow(text) for text in self.texts]\n",
    "            else:\n",
    "                raise ValueError(\"Either 'corpus' with 'dictionary' or 'texts' should be provided for %s coherence.\" % coherence)\n",
    "        # Check for correct inputs for c_v coherence measure.\n",
    "        elif coherence in sliding_window_based:\n",
    "            print(\"C_V\")\n",
    "            self.window_size = window_size\n",
    "            if texts is None:\n",
    "                raise ValueError(\"'texts' should be provided for %s coherence.\" % coherence)\n",
    "            else:\n",
    "                self.texts = texts\n",
    "        else:\n",
    "            raise ValueError(\"%s coherence is not currently supported.\" % coherence)\n",
    "        \n",
    "        print(f\"window_size: {self.window_size}\")\n",
    "        self.topn = topn\n",
    "        self.model = model\n",
    "        if model is not None:\n",
    "            self.topics = self._get_topics()\n",
    "        elif topics is not None:\n",
    "            self.topics = []\n",
    "            for topic in topics:\n",
    "                t_i = []\n",
    "                for n, _ in enumerate(topic):\n",
    "                    t_i.append(dictionary.token2id[topic[n]])\n",
    "                self.topics.append(np.array(t_i))\n",
    "        self.coherence = coherence\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return coherence_dict[self.coherence].__str__()\n",
    "\n",
    "\n",
    "    def _get_topics(self):\n",
    "        \"\"\"Internal helper function to return topics from a trained topic model.\"\"\"\n",
    "        topics = []\n",
    "\n",
    "        if isinstance(self.model, LdaModel):\n",
    "            for topic in self.model.state.get_lambda():\n",
    "                bestn = argsort(topic, topn=self.topn, reverse=True)\n",
    "                #print(f\"{len(topic)} - for {topic} -> bestn: {bestn}\")\n",
    "                print(f\"{len(topic)} -> bestn: {bestn}\")\n",
    "                topics.append(bestn)\n",
    "        else:\n",
    "            raise ValueError(\"This topic model is not currently supported. Supported topic models are \"\"LdaModel, LdaVowpalWabbit and LdaMallet.\")\n",
    "\n",
    "        print(f\"topics: {topics}\")\n",
    "        return topics\n",
    "\n",
    "\n",
    "    def get_coherence(self):\n",
    "        \"\"\"\n",
    "        Return coherence value based on pipeline parameters.\n",
    "        \"\"\"\n",
    "        measure = coherence_dict[self.coherence]\n",
    "        print(f\"\\nmeasure: {measure}\")\n",
    "        segmented_topics = measure.seg(self.topics)\n",
    "\n",
    "        print(f\"\\nsegmented_topics: {len(segmented_topics)}\")\n",
    "        for i,el in enumerate(segmented_topics):\n",
    "            print(f\" {i} - {len(el)} - {el}\")\n",
    "        \n",
    "        if self.coherence in sliding_window_based:  #C_V\n",
    "            if self.window_size is None:\n",
    "                self.window_size = sliding_windows_dict[self.coherence]  #PRENDO LA SLIDING_WINDOW DI QUELLA COERENCE METRIC\n",
    "            \n",
    "            print(f\"\\nwindow_size: {self.window_size}\")\n",
    "            \n",
    "            print(f\"\\ntexts: {len(self.texts)}\")\n",
    "            \n",
    "            print(f\"\\ndictionary: {self.dictionary}\")\n",
    "            \n",
    "            #DEBUG\n",
    "            accumulator=measure.prob(texts=self.texts, segmented_topics=segmented_topics, dictionary=self.dictionary, window_size=self.window_size) #PIPELINE\n",
    "            print(f\"\\naccumulator: {accumulator}\")\n",
    "            \n",
    "            #per_topic_postings, num_windows = measure.prob(texts=self.texts, segmented_topics=segmented_topics, dictionary=self.dictionary, window_size=self.window_size) #PIPELINE\n",
    "\n",
    "            #print(f\"\\nper_topic_postings: {result.per_topic_postings}\")\n",
    "            #print(f\"\\nnum_windows: {result.num_windows}\")\n",
    "\n",
    "            #confirmed_measures = measure.conf(self.topics, segmented_topics, per_topic_postings, 'nlr', 1, num_windows) #PIPELINE\n",
    "            confirmed_measures = measure.conf(segmented_topics=segmented_topics, accumulator=accumulator, topics=self.topics, measure='nlr', gamma=1) #PIPELINE\n",
    "            \n",
    "            print(f\"\\nconfirmed_measures: {confirmed_measures}\")\n",
    "\n",
    "            result=measure.aggr(confirmed_measures)\n",
    "            print(f\"\\nresult: {result}\")\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = MyCoherenceModel(model=best_lda_model, dictionary=id2word, texts=data, coherence='c_v')\n",
    "coherence = cm.get_coherence() \n",
    "print(f\"MY COHERENCE: {coherence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(model=best_lda_model, dictionary=id2word, texts=data, coherence='c_v')\n",
    "coherence = cm.get_coherence() \n",
    "print(f\"COHERENCE: {coherence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1) MY SCORE FUNCTION -> SE CI SONO IDEE E TEMPO\n",
    "2) ESPLORAZIONE -> NOW WORKING\n",
    "\n",
    "MODELLI:\n",
    "-LDA\n",
    "-LDA MULTICORE\n",
    "-LDA_MALLET -> TODO\n",
    "-HIERARCHICAL LDA -> TODO\n",
    "-PACHINKO LDA -> TODO\n",
    "-LSA\n",
    "-NMF\n",
    "-HDP -> TODO\n",
    "\n",
    "- fastTEXT\n",
    "- word2vec\n",
    "- sen2vec\n",
    "- lda2vec -> IMPLEMENTAZIONE IN CORSO\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('time', '', \"#LDA MALLET\\nmallet_path = 'C:/mallet/bin/mallet' # update this path\\nlda_mallet = LdaMallet(mallet_path, corpus=corpus, num_topics=5, id2word=id2word, random_seed=100)\\ncm = CoherenceModel(model=lda_mallet, dictionary=id2word, texts=input_data, coherence='c_v')\\ncoherence = cm.get_coherence()  # get coherence value\\nprint(coherence) #[0,1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared = gensimvis.prepare(lda_mallet, corpus, id2word)\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('time', '', 'path_to_dtm_binary = \"C:/Users/david/Desktop/Tesi/dtm-win64.exe\"\\ndtm_model = gensim.models.wrappers.DtmModel(path_to_dtm_binary, corpus=corpus, id2word=id2word, time_slices=[1] * len(corpus), num_topics=20)\\ncm = CoherenceModel(model=dtm_model, dictionary=id2word, texts=input_data, coherence=\\'c_v\\')\\ncoherence = cm.get_coherence()  # get coherence value\\nprint(coherence) #[0,1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for k in range(5, 100, 5)\n",
    "    nmf_model=Nmf(corpus=corpus, id2word=id2word, num_topics=k, random_state=100) # PARAMETER K\n",
    "    cm = CoherenceModel(model=nmf_model, dictionary=id2word, texts=input_data, coherence='c_v')\n",
    "    coherence = cm.get_coherence()  # get coherence value\\n    \n",
    "    print(f\"{k} - {coherence}\") #[0,1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "# Visualize the topics<br>\n",
    "nmf_model=Nmf(corpus=corpus, id2word=id2word, num_topics=5, random_state=100) # PARAMETER K -> VISUALIZZARE <br>\n",
    "pyLDAvis.enable_notebook()<br>\n",
    "LDAvis_prepared = gensimvis.prepare(nmf_model, corpus, id2word)<br>\n",
    "LDAvis_prepared<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model=Nmf(corpus=corpus, id2word=id2word, num_topics=4, random_state=100) # PARAMETER K\n",
    "cm = CoherenceModel(model=nmf_model, dictionary=id2word, texts=input_data, coherence='c_v')\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "print(f\"{k} - {coherence}\") #[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF -> 5, LDA-> 5, LSA -> 10 -->5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "erms, sizes = getTermsAndSizes(topics_display_list[0])<br>\n",
    "284 -> 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 15\n",
    "fontsize_base = 15 #/ np.max(sizes) # font size for word with largest share in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ig, ax = plt.subplots(1, 5, figsize=(6, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(num_topics):\n",
    "    tt=nmf_model.show_topic(t,15)\n",
    "    plt.plot(figsize=(6, 12))\n",
    "    plt.ylim(-1, num_top_words+1)\n",
    "    plt.xlim(-1,150)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title('Topic #{}'.format(t))\n",
    "    for i, (word, share) in enumerate(tt):\n",
    "        word = word + \" (\" + str(share) + \")\"\n",
    "        plt.text(0.3, num_top_words-i-1.0, word, fontsize=fontsize_base)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "java datanode start <br>\n",
    "p(riga|t) -> gt<br>\n",
    "0.739 -> Improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "K=4<br>\n",
    "TOPIC 0: receive, dest, datanode<br>\n",
    "TOPIC 1: byte, write, clienttrace<br>\n",
    "TOPIC 2: <br>\n",
    "TOPIC 3: data, current, disk, file<br>\n",
    "K=5<br>\n",
    "TOPIC 0: data, current, disk, file<br>\n",
    "TOPIC 1: java(TROPPO GENERICO), client, socket<br>\n",
    "TOPIC 2: byte, write, clienttrace<br>\n",
    "TOPIC 3: downstream, pipeline, terminating<br>\n",
    "TOPIC 4: downstream, pipeline, terminating<br>\n",
    "-> TERMINI ANCORA GENERICI, TOPIC 3 e 4 TROPPO SIMILI<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('time', '', 'for k in range(5, 100, 5):\\n    lsi_model = models.LsiModel(corpus=corpus, id2word=id2word, num_topics=k)  \\n    cm = CoherenceModel(model=lsi_model, dictionary=id2word, texts=input_data, coherence=\\'c_v\\')\\n    coherence = cm.get_coherence()  # get coherence value\\n    print(f\"{k} - {coherence}\") #[0,1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "5 - 0.6183807033327369<br>\n",
    "10 - 0.47864429391788443<br>\n",
    "15 - 0.6316040601926622<br>\n",
    "20 - 0.6231298550022704<br>\n",
    "25 - 0.5305429381892516<br>\n",
    "30 - 0.49587954732967043<br>\n",
    "35 - 0.5453331013907984<br>\n",
    "40 - 0.48655687340827153<br>\n",
    "45 - 0.5251982445357273<br>\n",
    "50 - 0.46653940710977365<br>\n",
    "55 - 0.5179579496637581<br>\n",
    "60 - 0.48332900081640634<br>\n",
    "65 - 0.48332900081640634<br>\n",
    "70 - 0.5289831245637493<br>\n",
    "75 - 0.5420460749491068<br>\n",
    "80 - 0.5637945593518081<br>\n",
    "85 - 0.5833787205134665<br>\n",
    "90 - 0.5833787205134665<br>\n",
    "95 - 0.5833787205134665<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lda2vec_model import LDA2Vec\n",
    "from lda2vec import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp=Corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = len(corpus)\n",
    "# Number of unique words in the vocabulary\n",
    "n_vocab = len(id2word)\n",
    "# 'Strength' of the dircihlet prior; 200.0 seems to work well\n",
    "clambda = 200.0\n",
    "# Number of topics to fit\n",
    "n_topics = 5\n",
    "batchsize = 4096\n",
    "# Power for neg sampling\n",
    "power = 0.75\n",
    "# Intialize with pretrained word vectors\n",
    "pretrained = True\n",
    "# Sampling temperature\n",
    "temperature = 1\n",
    "# Number of dimensions in a single word vector\n",
    "n_units = 300\n",
    "# Get the string representation for every compact key\n",
    "words = corpus.word_list(vocab)[:n_vocab]\n",
    "# How many tokens are in each document\n",
    "doc_idx, lengths = np.unique(doc_ids, return_counts=True)\n",
    "doc_lengths = np.zeros(doc_ids.max() + 1, dtype='int32')\n",
    "doc_lengths[doc_idx] = lengths\n",
    "# Count all token frequencies\n",
    "tok_idx, freq = np.unique(flattened, return_counts=True)\n",
    "term_frequency = np.zeros(n_vocab, dtype='int32')\n",
    "term_frequency[tok_idx] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LDA2Vec(n_words, max_length, n_hidden, counts)\n",
    "model.add_component(n_docs, n_topics, name='document id')\n",
    "model.fit(clean, components=[doc_ids])\n",
    "While visualizing the feature is similarly straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = model.prepare_topics('document_id', vocab)\n",
    "prepared = pyLDAvis.prepare(topics)\n",
    "pyLDAvis.display(prepared)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
