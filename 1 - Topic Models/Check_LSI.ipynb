{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "protective-local",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT \n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import gensim.downloader\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from sklearn.tree import export_graphviz\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import graphviz\n",
    "import gensim\n",
    "from gensim.models.nmf import Nmf\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import operator\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "statistical-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "solved-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "running-passenger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bigdata-01QYD/s278561/270005/RESULTS\n"
     ]
    }
   ],
   "source": [
    "currdir=os.getcwd()\n",
    "print(currdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "delayed-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/LSI/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "worst-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"HDFS\"\n",
    "tipo=\"SPLIT\"\n",
    "method=tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "automated-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log=pd.read_csv(currdir+path+f\"{name}_LSI_{tipo}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "adjacent-elimination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n"
     ]
    }
   ],
   "source": [
    "print(len(df_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "average-ideal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "      seed     k  coherence\n",
      "7283  80.0  93.0   0.467234\n",
      "803    8.0  93.0   0.467155\n",
      "8543  94.0  93.0   0.466683\n",
      "1074  11.0  94.0   0.466426\n",
      "352    3.0  92.0   0.466371\n",
      "4403  48.0  93.0   0.466031\n",
      "83     0.0  93.0   0.465970\n",
      "3148  34.0  98.0   0.465809\n",
      "1793  19.0  93.0   0.465541\n",
      "7553  83.0  93.0   0.465487\n",
      "8002  88.0  92.0   0.465442\n",
      "7373  81.0  93.0   0.465338\n",
      "4043  44.0  93.0   0.465310\n",
      "5572  61.0  92.0   0.465277\n",
      "5663  62.0  93.0   0.465218\n",
      "4402  48.0  92.0   0.465187\n",
      "353    3.0  93.0   0.465171\n",
      "5935  65.0  95.0   0.465169\n",
      "2155  23.0  95.0   0.465120\n",
      "2964  32.0  94.0   0.465044\n",
      "3683  40.0  93.0   0.465015\n",
      "5488  60.0  98.0   0.464992\n",
      "3053  33.0  93.0   0.464971\n",
      "8629  95.0  89.0   0.464940\n",
      "2248  24.0  98.0   0.464903\n",
      "1978  21.0  98.0   0.464882\n",
      "5570  61.0  90.0   0.464875\n",
      "7195  79.0  95.0   0.464867\n",
      "3779  41.0  99.0   0.464787\n",
      "2158  23.0  98.0   0.464775\n",
      "4855  53.0  95.0   0.464741\n",
      "2699  29.0  99.0   0.464704\n",
      "1073  11.0  93.0   0.464699\n",
      "6020  66.0  90.0   0.464663\n",
      "2878  31.0  98.0   0.464617\n",
      "6382  70.0  92.0   0.464590\n",
      "7460  82.0  90.0   0.464571\n",
      "2608  28.0  98.0   0.464557\n",
      "3232  35.0  92.0   0.464545\n",
      "4769  52.0  99.0   0.464501\n",
      "4132  45.0  92.0   0.464497\n",
      "5393  59.0  93.0   0.464465\n",
      "7829  86.0  99.0   0.464448\n",
      "4310  47.0  90.0   0.464390\n",
      "6925  76.0  95.0   0.464382\n",
      "5123  56.0  93.0   0.464371\n",
      "5304  58.0  94.0   0.464370\n",
      "2514  27.0  94.0   0.464360\n",
      "6563  72.0  93.0   0.464336\n",
      "7284  80.0  94.0   0.464328\n"
     ]
    }
   ],
   "source": [
    "final_df0=df_log[df_log['k']>=80]\n",
    "#final_df0=final_df0[final_df0['coherence']>0.7]\n",
    "#final_df0=final_df0[final_df0['kappa']==0.1]\n",
    "final_df0=final_df0.sort_values(by=['coherence'], ascending=False)\n",
    "print(len(final_df0))\n",
    "print(final_df0.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "demographic-palace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n",
      "      seed     k  coherence\n",
      "1354  15.0  14.0   0.693774\n",
      "3424  38.0  14.0   0.693774\n",
      "3964  44.0  14.0   0.693774\n",
      "7654  85.0  14.0   0.693774\n",
      "3787  42.0  17.0   0.693459\n",
      "8467  94.0  17.0   0.693459\n",
      "3157  35.0  17.0   0.693459\n",
      "8827  98.0  17.0   0.693459\n",
      "3247  36.0  17.0   0.693459\n",
      "4507  50.0  17.0   0.693459\n",
      "996   11.0  16.0   0.692272\n",
      "6666  74.0  16.0   0.692272\n",
      "1986  22.0  16.0   0.692272\n",
      "6216  69.0  16.0   0.692272\n",
      "1626  18.0  16.0   0.692272\n",
      "7116  79.0  16.0   0.692272\n",
      "6936  77.0  16.0   0.692272\n",
      "7206  80.0  16.0   0.692272\n",
      "7746  86.0  16.0   0.692272\n",
      "5678  63.0  18.0   0.689563\n",
      "5858  65.0  18.0   0.689563\n",
      "2888  32.0  18.0   0.689563\n",
      "2521  28.0  11.0   0.683591\n",
      "6751  75.0  11.0   0.683591\n",
      "7471  83.0  11.0   0.683591\n",
      "4861  54.0  11.0   0.683591\n",
      "3961  44.0  11.0   0.683591\n",
      "5670  63.0  10.0   0.682347\n",
      "6210  69.0  10.0   0.682347\n",
      "1620  18.0  10.0   0.682347\n",
      "2340  26.0  10.0   0.682347\n",
      "6480  72.0  10.0   0.681807\n",
      "6570  73.0  10.0   0.681807\n",
      "8100  90.0  10.0   0.681807\n",
      "900   10.0  10.0   0.681807\n",
      "6660  74.0  10.0   0.681807\n",
      "5760  64.0  10.0   0.681807\n",
      "4230  47.0  10.0   0.681807\n",
      "7481  83.0  21.0   0.679394\n",
      "2441  27.0  21.0   0.679394\n",
      "6221  69.0  21.0   0.679394\n",
      "7391  82.0  21.0   0.679394\n",
      "371    4.0  21.0   0.679394\n",
      "101    1.0  21.0   0.679394\n",
      "5861  65.0  21.0   0.679394\n",
      "1181  13.0  21.0   0.679394\n",
      "2261  25.0  21.0   0.679394\n",
      "280    3.0  20.0   0.678369\n",
      "4150  46.0  20.0   0.678369\n",
      "1990  22.0  20.0   0.678369\n"
     ]
    }
   ],
   "source": [
    "final_df0=df_log\n",
    "final_df0=final_df0.sort_values(by=['coherence'], ascending=False)\n",
    "print(len(final_df0))\n",
    "print(final_df0.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "valuable-defeat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "      seed     k  coherence\n",
      "185    2.0  15.0   0.672380\n",
      "2975  33.0  15.0   0.672380\n",
      "2165  24.0  15.0   0.672380\n",
      "7835  87.0  15.0   0.672380\n",
      "3425  38.0  15.0   0.656066\n",
      "8555  95.0  15.0   0.656066\n",
      "6845  76.0  15.0   0.656066\n",
      "2885  32.0  15.0   0.656066\n",
      "8285  92.0  15.0   0.656066\n",
      "1715  19.0  15.0   0.656066\n",
      "8015  89.0  15.0   0.656066\n",
      "7025  78.0  15.0   0.656066\n",
      "7205  80.0  15.0   0.656066\n",
      "4325  48.0  15.0   0.656066\n",
      "995   11.0  15.0   0.639367\n",
      "1085  12.0  15.0   0.639367\n",
      "4415  49.0  15.0   0.639367\n",
      "2795  31.0  15.0   0.639367\n",
      "2705  30.0  15.0   0.639367\n",
      "5765  64.0  15.0   0.639367\n",
      "2525  28.0  15.0   0.639367\n",
      "2255  25.0  15.0   0.639367\n",
      "635    7.0  15.0   0.639367\n",
      "3695  41.0  15.0   0.639367\n",
      "8465  94.0  15.0   0.639367\n",
      "6035  67.0  15.0   0.639367\n",
      "5585  62.0  15.0   0.639367\n",
      "1535  17.0  15.0   0.639367\n",
      "5135  57.0  15.0   0.639367\n",
      "5315  59.0  15.0   0.639367\n",
      "1265  14.0  15.0   0.639367\n",
      "3785  42.0  15.0   0.622886\n",
      "3515  39.0  15.0   0.622886\n",
      "7565  84.0  15.0   0.622886\n",
      "4865  54.0  15.0   0.622886\n",
      "5405  60.0  15.0   0.622886\n",
      "2345  26.0  15.0   0.622886\n",
      "4775  53.0  15.0   0.622886\n",
      "8375  93.0  15.0   0.622886\n",
      "7295  81.0  15.0   0.622886\n",
      "8645  96.0  15.0   0.622886\n",
      "6485  72.0  15.0   0.622886\n",
      "1175  13.0  15.0   0.622335\n",
      "3605  40.0  15.0   0.622335\n",
      "8105  90.0  15.0   0.605992\n",
      "95     1.0  15.0   0.605992\n",
      "3065  34.0  15.0   0.598479\n",
      "1985  22.0  15.0   0.598479\n",
      "1625  18.0  15.0   0.598479\n",
      "8735  97.0  15.0   0.598479\n"
     ]
    }
   ],
   "source": [
    "final_df0=df_log[df_log['k']==15]\n",
    "#final_df0=final_df0[final_df0['kappa']==0.1]\n",
    "final_df0=final_df0.sort_values(by=['coherence'], ascending=False)\n",
    "print(len(final_df0))\n",
    "print(final_df0.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-postcard",
   "metadata": {},
   "source": [
    "# BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "abstract-bandwidth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD DATA\n",
      "READ FILE: /home/bigdata-01QYD/s278561/270005/RESULTS/INPUT/HDFS/SPLIT/HDFS_InputData_SPLIT.txt\n",
      "0 ['cause', 'java', 'io', 'eof', 'exception', 'java io', 'io eof', 'eof exception', 'java io eof', 'io eof exception']\n",
      "1 ['cause', 'java', 'io', 'io', 'exception', 'connection', 'reset', 'peer', 'java io', 'io exception', 'exception connection', 'connection reset', 'reset peer', 'io exception connection', 'exception connection reset', 'connection reset peer']\n",
      "2 ['cause', 'java', 'net', 'socket', 'timeout', 'exception', 'millis', 'timeout', 'wait', 'channel', 'ready', 'read', 'java', 'channel', 'socket', 'channel', 'java net', 'net socket', 'socket timeout', 'timeout exception', 'exception millis', 'millis timeout', 'timeout wait', 'wait channel', 'channel ready', 'ready read', 'read java', 'java channel', 'channel socket', 'channel socket', 'java net socket', 'net socket timeout', 'socket timeout exception', 'timeout exception millis', 'exception millis timeout', 'millis timeout wait', 'timeout wait channel', 'wait channel ready', 'channel ready read', 'ready read java', 'read java channel', 'java channel socket']\n",
      "3 ['java', 'io', 'eof', 'exception', 'java io', 'io eof', 'eof exception', 'java io eof', 'io eof exception']\n",
      "4 ['java', 'io', 'eof', 'exception', 'end', 'file', 'exception', 'local', 'host', 'destination', 'host', 'java', 'io', 'eof', 'exception', 'detail', 'see', 'end file', 'file exception', 'exception local', 'local host', 'host destination', 'host destination', 'host java', 'java io', 'io eof', 'eof exception', 'exception detail', 'detail see', 'end file exception', 'file exception local', 'exception local host', 'local host destination', 'destination host java', 'host java io', 'java io eof', 'io eof exception', 'eof exception detail', 'exception detail see']\n",
      "5 ['java', 'io', 'io', 'exception', 'fail', 'local', 'exception', 'java', 'io', 'io', 'exception', 'connection', 'reset', 'peer', 'host', 'detail', 'local', 'host', 'destination', 'host', 'fail local', 'exception local', 'exception java', 'java io', 'io exception', 'exception connection', 'connection reset', 'reset peer', 'peer host', 'host detail', 'detail local', 'local host', 'host destination', 'host destination', 'fail local exception', 'local exception java', 'exception java io', 'io exception connection', 'exception connection reset', 'connection reset peer', 'reset peer host', 'peer host detail', 'host detail local', 'detail local host', 'local host destination']\n",
      "6 ['java', 'null', 'pointer', 'exception', 'java null', 'null pointer', 'pointer exception', 'java null pointer', 'null pointer exception']\n",
      "7 ['java', 'net', 'socket', 'timeout', 'exception', 'call', 'fail', 'socket', 'timeout', 'exception', 'java', 'net', 'socket', 'timeout', 'exception', 'millis', 'timeout', 'wait', 'channel', 'ready', 'read', 'java', 'channel', 'socket', 'channel', 'detail', 'see', 'call fail', 'fail socket', 'socket timeout', 'timeout exception', 'exception java', 'java net', 'net socket', 'socket timeout', 'timeout exception', 'exception millis', 'millis timeout', 'timeout wait', 'wait channel', 'channel ready', 'ready read', 'read java', 'java channel', 'channel socket', 'channel socket', 'channel detail', 'detail see', 'call fail socket', 'fail socket timeout', 'socket timeout exception', 'timeout exception java', 'exception java net', 'java net socket', 'net socket timeout', 'socket timeout exception', 'timeout exception millis', 'exception millis timeout', 'millis timeout wait', 'timeout wait channel', 'wait channel ready', 'channel ready read', 'ready read java', 'read java channel', 'java channel socket', 'socket channel detail', 'channel detail see']\n",
      "8 ['http', 'http', 'request', 'http', 'request', 'http', 'request', 'not', 'define', 'http request', 'http request', 'http request', 'request datanode', 'datanode not', 'not define', 'http request datanode', 'request datanode not', 'datanode not define']\n",
      "9 ['http', 'http', 'add', 'filter', 'static', 'user', 'filter', 'context', 'add filter', 'filter static', 'static user', 'user filter', 'filter context', 'context datanode', 'add filter static', 'filter static user', 'static user filter', 'user filter context', 'filter context datanode']\n",
      "99\n",
      "END LOAD DATA\n"
     ]
    }
   ],
   "source": [
    "print(\"LOAD DATA\")\n",
    "filename = f\"/home/bigdata-01QYD/s278561/270005/RESULTS/INPUT/{name}/{method}/{name}_InputData_{method}.txt\"\n",
    "print(f\"READ FILE: {filename}\")\n",
    "data=[]\n",
    "with open(filename, 'r') as csvfile:\n",
    "    rows = csv.reader(csvfile, delimiter='\\t')\n",
    "    for i,row in enumerate(rows):\n",
    "        tmp = row[0].replace(\"[\", \"\")\n",
    "        tmp = tmp.replace(\"]\", \"\")\n",
    "        tmp = tmp.replace(\"'\", \"\")\n",
    "        row_l = list(tmp.split(\", \"))\n",
    "        if i<10:\n",
    "            print(i, row_l)\n",
    "        data.append(row_l)\n",
    "print(len(data))\n",
    "print(\"END LOAD DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "korean-knitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 782\n"
     ]
    }
   ],
   "source": [
    "id2word=corpora.Dictionary(data)\n",
    "corpus=[id2word.doc2bow(word) for word in data]\n",
    "print(len(corpus), len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "documentary-vegetable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "782\n"
     ]
    }
   ],
   "source": [
    "f=open(f\"/home/bigdata-01QYD/s278561/270005/RESULTS/INPUT/{name}/{method}/{name}_w2vdict_{method}.pkl\",\"rb\")\n",
    "modelW2V=pickle.load(f)#Word2Vec.load(pathname+\"WORD2VEC/word2vec_SPLIT.model\")\n",
    "#print(modelW2V)\n",
    "print(type(modelW2V))\n",
    "#print(modelW2V.keys())\n",
    "print(len(modelW2V.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-metallic",
   "metadata": {},
   "source": [
    "## LSI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cohe=0\n",
    "it=0\n",
    "for index, row in final_df0.iterrows():\n",
    "        print(\"-\"*113)\n",
    "        seed=int(row['seed'])\n",
    "        k=int(row['k'])\n",
    "        np.random.seed(seed)\n",
    "        lsa_model=LsiModel(corpus=corpus, id2word=id2word, num_topics=k) \n",
    "        cm = CoherenceModel(model=lsa_model, dictionary=id2word, texts=data, coherence='c_v') #110\n",
    "        coherence=cm.get_coherence()\n",
    "        #print(coherence)\n",
    "        topics = best_nmf_model.show_topics(num_topics=k,num_words=20,formatted=False)\n",
    "        topics_words=[]\n",
    "        j=0\n",
    "        for i in range(k):\n",
    "            topic_words = dict(topics[i][1])\n",
    "            topics_words.append(list(topic_words.keys()))\n",
    "        w2v=[]\n",
    "        for tw in topics_words:\n",
    "            tmp=[]\n",
    "            for el in tw:\n",
    "                splits=el.split(\" \")\n",
    "                tmp.append(\"_\".join(splits))\n",
    "            #print(tmp)\n",
    "            w2v.append(tmp)\n",
    "        delta_k, to_drop,similardf=CLUSTERSIMILARITY(w2v, modelW2V, k)\n",
    "        if delta_k==0 and coherence>best_cohe:\n",
    "            best_cohe=coherence\n",
    "            print(f\"--> [BEST] k={k}, seed={seed}, coherence={coherence}, to_drop={to_drop}\")\n",
    "        else:\n",
    "            print(f\"k={k}, seed={seed}, coherence={coherence}, to_drop={to_drop}\")\n",
    "        it+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "imported-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "k=17\n",
    "np.random.seed(seed)\n",
    "best_nmf_model=LsiModel(corpus=corpus, id2word=id2word, num_topics=k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "basic-battle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69346 0.69378 0.67411\n"
     ]
    }
   ],
   "source": [
    "metric10 = CoherenceModel(model=best_nmf_model, dictionary=id2word, texts=data, coherence='c_v',topn=10) # Initialize metric\n",
    "coherence_cv10 = metric10.get_coherence()\n",
    "metric15 = CoherenceModel(model=best_nmf_model, dictionary=id2word, texts=data, coherence='c_v',topn=15) # Initialize metric\n",
    "coherence_cv15 = metric15.get_coherence()\n",
    "metric = CoherenceModel(model=best_nmf_model, dictionary=id2word, texts=data, coherence='c_v',topn=20) # Initialize metric\n",
    "coherence_cv = metric.get_coherence()\n",
    "print(round(coherence_cv,5), round(coherence_cv15,5), round(coherence_cv10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "pacific-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = best_nmf_model.show_topics(num_topics=k,num_words=20,formatted=False)\n",
    "topics_words=[]\n",
    "j=0\n",
    "for i in range(k):\n",
    "    topic_words = dict(topics[i][1])\n",
    "    topics_words.append(list(topic_words.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "domestic-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v=[]\n",
    "for tw in topics_words:\n",
    "    tmp=[]\n",
    "    for el in tw:\n",
    "        splits=el.split(\" \")\n",
    "        tmp.append(\"_\".join(splits))\n",
    "    #print(tmp)\n",
    "    w2v.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "activated-exercise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to be dropped:[19] - 1 - max: [0.8616 0.8616 0.796  0.796  0.7919]\n",
      "k=22, coherence=0.6365040044186577, to_drop=[19]\n"
     ]
    }
   ],
   "source": [
    "delta_k, to_drop,similardf=CLUSTERSIMILARITY(w2v, modelW2V, k)\n",
    "print(f\"k={k}, coherence={coherence}, to_drop={to_drop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "every-result",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TO_DROP [3, 8, 19]\n",
      "TO_MERGE\n",
      "[1, 3]\n",
      "[4, 8]\n",
      "[17, 19]\n",
      "STARTING LEN 22\n",
      "ADDING 0\n",
      "1 3\n",
      "1 True\n",
      "LIST PROCESSED [0]\n",
      "LAST LIST PROCESSED [1, 3]\n",
      "LEN NEW TP 20\n",
      "ACT NEW TP ['io', 'exception', 'host', 'java_io', 'host_destination', 'local', 'socket', 'timeout', 'channel', 'io_exception', 'java', 'java_io_eof', 'io_eof', 'eof_exception', 'eof', 'io_eof_exception', 'timeout_exception', 'socket_timeout', 'socket_timeout_exception', 'exception_local']\n",
      "LEN TOT 20\n",
      "CURR LEN MERGED_TOPICS: 1\n",
      "CURR LEN MERGED_TOPICS: 2 AFTER ADDED 1\n",
      "ADDING 2\n",
      "3 False\n",
      "4 8\n",
      "4 True\n",
      "LIST PROCESSED [0, 1]\n",
      "LAST LIST PROCESSED [4, 8]\n",
      "LEN NEW TP 20\n",
      "ACT NEW TP ['http_request', 'request', 'filter', 'http', 'static', 'not', 'storage', 'add', 'user', 'http_add', 'filter_static', 'context', 'add_filter_static', 'filter_static_user', 'static_user', 'user_filter_context', 'static_user_filter', 'user_filter', 'http_add_filter', 'filter_context']\n",
      "LEN TOT 20\n",
      "CURR LEN MERGED_TOPICS: 3\n",
      "CURR LEN MERGED_TOPICS: 4 AFTER ADDED 4\n",
      "ADDING 5\n",
      "ADDING 6\n",
      "ADDING 7\n",
      "8 False\n",
      "ADDING 9\n",
      "ADDING 10\n",
      "ADDING 11\n",
      "ADDING 12\n",
      "ADDING 13\n",
      "ADDING 14\n",
      "ADDING 15\n",
      "ADDING 16\n",
      "17 19\n",
      "17 True\n",
      "LIST PROCESSED [0, 1, 2]\n",
      "LAST LIST PROCESSED [17, 19]\n",
      "LEN NEW TP 20\n",
      "ACT NEW TP ['src', 'src_dest', 'dest', 'slow', 'datanode_slow', 'receive', 'datanode_receive', 'byte', 'block_receiver_write', 'receiver', 'block_receiver', 'receiver_write', 'slow_block', 'slow_block_receiver', 'datanode_slow_block', 'receive_src', 'datanode_receive_src', 'receive_src_dest', 'take', 'start']\n",
      "LEN TOT 20\n",
      "CURR LEN MERGED_TOPICS: 15\n",
      "CURR LEN MERGED_TOPICS: 16 AFTER ADDED 17\n",
      "ADDING 18\n",
      "19 False\n",
      "ADDING 20\n",
      "ADDING 21\n",
      "19\n",
      "COHERENCE - 19: 0.5432718836666985\n",
      "Features to be dropped:[] - 0 - max: [0.7939 0.7939 0.7775 0.7775 0.7684]\n"
     ]
    }
   ],
   "source": [
    "delta_k=1\n",
    "while delta_k!=0:\n",
    "    print(\"\\nTO_DROP\",to_drop)\n",
    "    to_merge=TO_MERGE(similardf)\n",
    "    merged_topics=MERGED_TOPICS(w2v,to_merge,to_drop,modelW2V)\n",
    "    cm = CoherenceModel(topics=merged_topics, texts = data, corpus=corpus, dictionary=id2word, coherence = \"c_v\")\n",
    "    coherence = cm.get_coherence() \n",
    "    print(f\"COHERENCE - {len(merged_topics)}: {coherence}\")\n",
    "    w2v=merged_topics\n",
    "    delta_k, to_drop,similardf=CLUSTERSIMILARITY(w2v, modelW2V, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "billion-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WORDCLOUD(best_k, best_nmf_model,k, flag=False):\n",
    "    tmp = [color for name, color in mcolors.XKCD_COLORS.items()]#mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "    offset=int(len(tmp)/best_k)\n",
    "    #print(offset)\n",
    "    cols=tmp[1::offset]\n",
    "    #print(cols)\n",
    "\n",
    "    cloud = WordCloud(background_color='white',\n",
    "                      width=2500,\n",
    "                      height=1800,\n",
    "                      max_words=20,\n",
    "                      colormap='tab10',\n",
    "                      color_func=lambda *args, **kwargs: cols[i],\n",
    "                      prefer_horizontal=1.0)\n",
    "\n",
    "    topics = best_nmf_model.show_topics(num_topics=best_k,num_words=20,formatted=False)\n",
    "    \n",
    "    plotx=np.floor(np.sqrt(best_k))\n",
    "    ploty=plotx\n",
    "    flag=True\n",
    "    while plotx*ploty < best_k:\n",
    "        if flag:\n",
    "            ploty+=1\n",
    "            flag=False\n",
    "        else:\n",
    "            plotx+=1\n",
    "\n",
    "    fig, axes = plt.subplots(int(plotx), int(ploty), figsize=(30,30), sharex=True, sharey=True)\n",
    "    axes=axes.flatten()\n",
    "    topics_words=[]\n",
    "    j=0\n",
    "    for i in range(best_k):\n",
    "        fig.add_subplot(axes[i])\n",
    "        topic_words = dict(topics[i][1])\n",
    "        topics_words.append(list(topic_words.keys()))\n",
    "        cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "        plt.gca().imshow(cloud)\n",
    "        plt.gca().set_title(f'Topic {j+i}', fontdict=dict(size=16))\n",
    "        plt.gca().axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=5, hspace=5)\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(currdir+path+f\"{name}_{tipo}_Best_Score_k={k}_WC.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    return topics_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "decimal-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CLUSTERSIMILARITY(tp, modelW2V, k,flag=False):\n",
    "    similarities2=[]\n",
    "    max=[]\n",
    "    for i,top_wrd in enumerate(tp):\n",
    "        tmp=tp\n",
    "        tmp_sim=[]\n",
    "        curr_v=np.zeros(300)\n",
    "        num_words=0\n",
    "        for el in top_wrd:\n",
    "            if el in modelW2V.keys():\n",
    "                word_vec=modelW2V[el]\n",
    "                curr_v+=word_vec\n",
    "                num_words+=1\n",
    "            else:\n",
    "                print(\"MISSING:\",el)\n",
    "        curr_v=curr_v/num_words\n",
    "        for j,other_topic in enumerate(tmp):\n",
    "            act_v=np.zeros(300)\n",
    "            num_words=0\n",
    "            for el in other_topic:\n",
    "                if el in modelW2V.keys():\n",
    "                    word_vec=modelW2V[el]\n",
    "                    act_v+=word_vec\n",
    "                    num_words+=1\n",
    "                else:\n",
    "                    print(\"MISSING:\",el)\n",
    "            act_v=act_v/num_words\n",
    "            similarity=spatial.distance.cosine(curr_v, act_v)\n",
    "            similarity=round(1-similarity,4)\n",
    "            flag=0\n",
    "            for el in top_wrd:\n",
    "                if el in other_topic:\n",
    "                    flag+=1\n",
    "            if flag==len(top_wrd) and i!=j and \"supergroup\" in top_wrd:\n",
    "                print(f\"ALL ELEMENT OF TOPIC {i} ARE INSIDE TOPIC {j}\")\n",
    "                print(i,top_wrd)\n",
    "                print(j,other_topic)\n",
    "                tmp_sim.append(-1)\n",
    "            else:\n",
    "                tmp_sim.append(similarity)\n",
    "            if j!=i:\n",
    "                max.append(similarity)\n",
    "        similarities2.append(tmp_sim)\n",
    "    \n",
    "#     with sns.axes_style(\"darkgrid\"):\n",
    "#         mask = np.triu(np.ones_like(similarities2, dtype=bool),k=1)\n",
    "#         plt.figure(figsize=(20,20), dpi=80)\n",
    "#         cmap=sns.diverging_palette(240, 10, n=9)\n",
    "#         sns.heatmap(similarities2,annot=True,mask=mask,cmap=cmap,linewidth=2,edgecolor=\"k\",vmin=-1,vmax=1,center=0)\n",
    "#         plt.title(\"Similarity between Topics\")\n",
    "#         #plt.savefig(currdir+path+f\"{name}_{tipo}_Best_Score_k={k}_SM_{len(tp)}.png\")\n",
    "#         #plt.show()\n",
    "\n",
    "    max=np.array(max)\n",
    "    max=-np.sort(-max)\n",
    "    max5=max[:5]\n",
    "    similarities2=pd.DataFrame(similarities2)\n",
    "    upper_tri = similarities2.where(np.triu(np.ones(similarities2.shape),k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.85)] #MORE THAN 90% of WORD IN COMMON\n",
    "    similarities2_dropped=similarities2.drop(columns=to_drop)\n",
    "    delta_k=len(to_drop)\n",
    "    print(f\"Features to be dropped:{to_drop} - {len(to_drop)} - max: {max5}\")\n",
    "\n",
    "    \n",
    "    return delta_k,to_drop,similarities2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_words=WORDCLOUD(k,best_nmf_model,k,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-hacker",
   "metadata": {},
   "source": [
    "### MERGE TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aerial-ground",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TO_MERGE(similardf):\n",
    "    sim=similardf.values\n",
    "    el_merge={}\n",
    "    for i in range(len(sim)):\n",
    "        el_merge[i]=[]\n",
    "    for i,row in enumerate(sim):\n",
    "        #print(i,\"ROW\")\n",
    "        sim_val=[]\n",
    "        for j,el in enumerate(row):\n",
    "            if el>0.85 and j<i:\n",
    "                #print(\"\\t\",i,\"-\",j,el)\n",
    "                sim_val.append(j)\n",
    "        for el1 in sim_val:\n",
    "            #print(\"\\t\\t\",el1)\n",
    "            #if len(el_merge[el1])==0:\n",
    "            el_merge[el1].append(i)\n",
    "            #print(\"\\t\\tACT STATE\",el1,el_merge[el1])\n",
    "            break\n",
    "    print(\"TO_MERGE\")\n",
    "    to_merge=[]\n",
    "    for k,v in el_merge.items():\n",
    "        if len(v)>0:\n",
    "            v.append(k)\n",
    "            v=sorted(v)\n",
    "            to_merge.append((v))\n",
    "            print((v))\n",
    "    return to_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "israeli-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MERGED_TOPICS(w2v,to_merge,to_drop,modelW2V):\n",
    "    merged_topics=[]\n",
    "    tm_processed=[]\n",
    "    tot_merge=[]\n",
    "    for el in to_merge:\n",
    "        tot_merge=tot_merge+el\n",
    "    print(\"STARTING LEN\",len(w2v))\n",
    "    for i,tp in enumerate(w2v):\n",
    "        if i not in tot_merge:\n",
    "            print(\"ADDING\",i)\n",
    "            merged_topics.append(tp)\n",
    "        else:\n",
    "            new_topic=[]\n",
    "            to_add={}\n",
    "            flag=False\n",
    "            for j,tm in enumerate(to_merge):\n",
    "                if i in tm and j not in tm_processed: #VEDO SE TOPIC i E' DA UNIRE E SE LA LISTA DI TO_MERGE NON e' GIa' STATA PROCESSATA\n",
    "                    flag=True\n",
    "                    for topic_idx in tm:\n",
    "                        if i!=topic_idx:\n",
    "                            print(i,topic_idx)\n",
    "                            for word in w2v[topic_idx]:\n",
    "                                if word not in to_add.keys():\n",
    "                                    #print(word,1)\n",
    "                                    to_add[word]=1\n",
    "                                else:\n",
    "                                    #print(word,to_add[word]+1)\n",
    "                                    to_add[word]+=1\n",
    "                    tm_processed.append(j) #INDICO QUELLA LISTA PROCESSATA\n",
    "            print(i,flag)\n",
    "            if flag:\n",
    "                print(\"LIST PROCESSED\",tm_processed)\n",
    "                last_proc=to_merge[tm_processed[-1]]\n",
    "                print(\"LAST LIST PROCESSED\",last_proc)\n",
    "                #print(\"----------------------------------\")\n",
    "                #print(\"TO_ADD:\")\n",
    "                for k,v in to_add.items():\n",
    "                    #print(k,v)\n",
    "                    if v+1==len(last_proc):\n",
    "                        new_topic.append(k)\n",
    "                #print(\"----------------------------------\")\n",
    "                print(\"LEN NEW TP\",len(new_topic))\n",
    "                print(\"ACT NEW TP\",new_topic)\n",
    "                num_el_still_add=20-len(new_topic)\n",
    "                print(\"LEN TOT\", len(to_add))\n",
    "                \n",
    "                if num_el_still_add == 0:\n",
    "                    print(f\"CURR LEN MERGED_TOPICS: {len(merged_topics)}\")\n",
    "                    merged_topics.append(w2v[i])\n",
    "                    print(f\"CURR LEN MERGED_TOPICS: {len(merged_topics)} AFTER ADDED {i}\")\n",
    "                else:\n",
    "                    diz_keep={}\n",
    "                    for k,v in to_add.items():\n",
    "                        #print(k,v)\n",
    "                        if v+1!=len(last_proc):\n",
    "                            tot_sim=0\n",
    "                            curr_v=modelW2V[k]\n",
    "                            for word in new_topic:\n",
    "                                act_v=modelW2V[word]\n",
    "                                similarity=spatial.distance.cosine(curr_v, act_v)\n",
    "                                tot_sim+=similarity\n",
    "                            if k not in diz_keep.keys():\n",
    "                                diz_keep[k]=tot_sim\n",
    "\n",
    "                    sort_diz = dict( sorted(diz_keep.items(), key=operator.itemgetter(1),reverse=True))\n",
    "                    print(sort_diz)\n",
    "                    it=0\n",
    "                    added_el=[]\n",
    "                    for k,v in sort_diz.items():\n",
    "                        print(k,v)\n",
    "                        if it<num_el_still_add:\n",
    "                            print(\"ADDING\",k)\n",
    "                            added_el.append(k)\n",
    "                            new_topic.append(k)\n",
    "                        it+=1\n",
    "                    print(\"ADDED ELEMENT:\",added_el)\n",
    "                    print(\"FINAL NEW_TOPIC:\",new_topic, len(new_topic))\n",
    "                    merged_topics.append(new_topic)\n",
    "                    print(\"\\n\")\n",
    "\n",
    "    print(len(merged_topics))\n",
    "    return merged_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v=[]\n",
    "for tw in topics_words:\n",
    "    tmp=[]\n",
    "    for el in tw:\n",
    "        splits=el.split(\" \")\n",
    "        tmp.append(\"_\".join(splits))\n",
    "    #print(tmp)\n",
    "    w2v.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_k, to_drop,similardf=CLUSTERSIMILARITY(w2v, modelW2V, k)\n",
    "print(f\"k={k}, coherence={coherence}, to_drop={to_drop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_k=1\n",
    "while delta_k!=0:\n",
    "    print(\"\\nTO_DROP\",to_drop)\n",
    "    to_merge=TO_MERGE(similardf)\n",
    "    merged_topics=MERGED_TOPICS(w2v,to_merge,to_drop,modelW2V)\n",
    "    cm = CoherenceModel(topics=merged_topics, texts = data, corpus=corpus, dictionary=id2word, coherence = \"c_v\")\n",
    "    coherence = cm.get_coherence() \n",
    "    print(f\"COHERENCE - {len(merged_topics)}: {coherence}\")\n",
    "    w2v=merged_topics\n",
    "    delta_k, to_drop,similardf=CLUSTERSIMILARITY(w2v, modelW2V, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## WORD COUNT AND IMPORTANCE ##########################\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "topics = best_nmf_model.show_topics(num_topics=best_k,formatted=False)\n",
    "data_flat = [w for w_list in data for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "#print(df)\n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "plot_size=int(np.ceil(np.sqrt(best_k)))\n",
    "fig, axes = plt.subplots(plot_size, plot_size, figsize=(30,30), sharey=True, dpi=160)\n",
    "tmp = [color for name, color in mcolors.XKCD_COLORS.items()]#mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "offset=int(len(tmp)/best_k)\n",
    "print(offset)\n",
    "cols=tmp[1::offset]\n",
    "print(cols)\n",
    "\n",
    "axes=axes.flatten()\n",
    "for i in range(best_k):\n",
    "    ax=axes[i]\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.010); ax.set_ylim(0, 1000)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=1)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.savefig(currdir+path+f\"{name}_{tipo}_WORD_COUNT_IMPORTANCE.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "laden-stylus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caused by  java.io.EOFException\n",
      "Caused by  java.io.IOException: Connection reset by peer\n",
      "Caused by  java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.10.34.42:48476 remote=mesos-master-1/10.10.34.11:9000]\n",
      " java.io.EOFException\n",
      "java.io.EOFException  End of File Exception between local host is: \"mesos-slave-32/127.0.1.1\"; destination host is: \"mesos-master-1\":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException\n",
      "java.io.IOException  Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: \"mesos-slave-32/127.0.1.1\"; destination host is: \"mesos-master-1\":9000; \n",
      " java.lang.NullPointerException\n",
      "java.net.SocketTimeoutException  Call From mesos-slave-32/127.0.1.1 to mesos-master-1:9000 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.10.34.42:48476 remote=mesos-master-1/10.10.34.11:9000]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout\n",
      "org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined\n",
      "org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\n",
      "org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\n",
      "org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\n",
      "org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
      "org.apache.hadoop.http.HttpServer2: Jetty bound to port 60199\n",
      "org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue\n",
      "org.apache.hadoop.ipc.Client: Retrying connect to server: mesos-master-1/10.10.34.11:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting\n",
      "org.apache.hadoop.ipc.Server: IPC Server Responder: starting\n",
      "org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020\n",
      "org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n",
      "org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n",
      "org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n",
      "org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\n",
      "org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-108841162-10.10.34.11-1440074360971\n",
      "org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /tmp/hadoop-hdfs/dfs/data/current/BP-108841162-10.10.34.11-1440074360971 is not formatted for BP-108841162-10.10.34.11-1440074360971\n",
      "org.apache.hadoop.hdfs.server.common.Storage: Formatting ...\n",
      "org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-108841162-10.10.34.11-1440074360971 directory /tmp/hadoop-hdfs/dfs/data/current/BP-108841162-10.10.34.11-1440074360971/current\n",
      "org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-hdfs/dfs/data/in_use.lock acquired by nodename 17707@mesos-slave-32\n",
      "org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /tmp/hadoop-hdfs/dfs/data/current/BP-108841162-10.10.34.11-1440074360971\n",
      "org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n",
      "org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-hdfs/dfs/data is not formatted for BP-108841162-10.10.34.11-1440074360971\n",
      "org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-108841162-10.10.34.11-1440074360971 (Datanode Uuid 78bbd6f1-d006-423d-9385-6de0f2676b45) service to mesos-master-1/10.10.34.11:9000\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-108841162-10.10.34.11-1440074360971 (Datanode Uuid null) service to mesos-master-1/10.10.34.11:9000 successfully registered with NN\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to mesos-master-1/10.10.34.11:9000 starting to offer service\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-108841162-10.10.34.11-1440074360971 (Datanode Uuid null) service to mesos-master-1/10.10.34.11:9000 beginning handshake with NN\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.34.11:59129, dest: /10.10.34.42:50010, bytes: 174053, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-402940418_1, offset: 0, srvID: ac6cb715-a2bc-4644-aaa4-10fcbd1c390e, blockid: BP-108841162-10.10.34.11-1440074360971:blk_1073743809_2985, duration: 19802122\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is mesos-slave-32\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-108841162-10.10.34.11-1440074360971:blk_1073891770_150946 (numBytes=31235) to /10.10.34.41:50010\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from mesos-master-1/10.10.34.11:9000 with active state\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(10.10.34.42:50010, datanodeUuid=ac6cb715-a2bc-4644-aaa4-10fcbd1c390e, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-254fec23-fa57-4394-9764-92d5cf14cc2e;nsid=560345747;c=0) Starting thread to transfer BP-108841162-10.10.34.11-1440074360971:blk_1073891770_150946 to 10.10.34.41:50010 \n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hdfs\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: mesos-slave-32:50010:DataXceiver error processing unknown operation  src: /10.10.34.12:40042 dst: /10.10.34.42:50010\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 78bbd6f1-d006-423d-9385-6de0f2676b45\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-108841162-10.10.34.11-1440074360971\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-108841162-10.10.34.11-1440074360971 (Datanode Uuid 78bbd6f1-d006-423d-9385-6de0f2676b45) service to mesos-master-1/10.10.34.11:9000 trying to claim ACTIVE state with txid=11992\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode mesos-master-1/10.10.34.11:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Can't replicate block BP-108841162-10.10.34.11-1440074360971:blk_1074051283_310459 because on-disk length 565 is shorter than NameNode recorded length 9223372036854775807\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-108841162-10.10.34.11-1440074360971:blk_1073743809_2985, type=LAST_IN_PIPELINE, downstreams=0:[] terminating\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-108841162-10.10.34.11-1440074360971:blk_1074009166_268342 src: /10.10.34.30:37128 dest: /10.10.34.42:50010 of size 155454\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-108841162-10.10.34.11-1440074360971:blk_1073743809_2985 src: /10.10.34.11:59129 dest: /10.10.34.42:50010\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=560345747;bpid=BP-108841162-10.10.34.11-1440074360971;lv=-56;nsInfo=lv=-63;cid=CID-254fec23-fa57-4394-9764-92d5cf14cc2e;nsid=560345747;c=0;bpid=BP-108841162-10.10.34.11-1440074360971;dnuuid=null\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:807ms (threshold=300ms)\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 4450ms (threshold=300ms)\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Slow manageWriterOsCache took 5299ms (threshold=300ms)\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6640da15314c3,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 2 msec to generate and 73 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = hdfsgroup\n",
      "org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup\n",
      "org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-108841162-10.10.34.11-1440074360971 Total blocks: 64, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0\n",
      "org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1440139511127 with interval 21600000\n",
      "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-108841162-10.10.34.11-1440074360971 blk_1073744003_3179 file /opt/hdfs/data/current/BP-108841162-10.10.34.11-1440074360971/current/finalized/subdir0/subdir8/blk_1073744003\n",
      "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073744003_3179 file /opt/hdfs/data/current/BP-108841162-10.10.34.11-1440074360971/current/finalized/subdir0/subdir8/blk_1073744003 for deletion\n",
      "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-7922da36-b08a-4046-b03e-1a669a082235\n",
      "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-hdfs/dfs/data/current, StorageType: DISK\n",
      "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-108841162-10.10.34.11-1440074360971\n",
      "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-108841162-10.10.34.11-1440074360971 on volume /tmp/hadoop-hdfs/dfs/data/current...\n",
      "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /opt/hdfs/data/current/BP-108841162-10.10.34.11-1440074360971/current: 1567772672\n",
      "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean\n",
      "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-108841162-10.10.34.11-1440074360971 on volume /tmp/hadoop-hdfs/dfs/data/current...\n",
      "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-108841162-10.10.34.11-1440074360971 on volume /tmp/hadoop-hdfs/dfs/data/current: 0ms\n",
      "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-108841162-10.10.34.11-1440074360971 on /tmp/hadoop-hdfs/dfs/data/current: 7ms\n",
      "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 0ms\n",
      "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-108841162-10.10.34.11-1440074360971: 8ms\n",
      "org.apache.hadoop.hdfs.server.datanode.VolumeScanner: FileNotFound while finding block BP-108841162-10.10.34.11-1440074360971:blk_1073768048_0 on volume /opt/hdfs/data\n",
      "org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now rescanning bpid BP-108841162-10.10.34.11-1440074360971 on volume /opt/hdfs/data, after more than 504 hour(s)\n",
      "org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-108841162-10.10.34.11-1440074360971 on volume /tmp/hadoop-hdfs/dfs/data\n",
      "org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/opt/hdfs/data, DS-ae5d34ab-b7b0-43b0-9cdd-5d7ccee7b2df) exiting.\n",
      "org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/opt/hdfs/data, DS-ae5d34ab-b7b0-43b0-9cdd-5d7ccee7b2df) exiting because of exception \n",
      "org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-hdfs/dfs/data, DS-7922da36-b08a-4046-b03e-1a669a082235): finished scanning block pool BP-108841162-10.10.34.11-1440074360971\n",
      "org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/opt/hdfs/data, DS-ae5d34ab-b7b0-43b0-9cdd-5d7ccee7b2df): Not scheduling suspect block BP-108841162-10.10.34.11-1440074360971:blk_1073747924_7100 for rescanning, because we rescanned it recently.\n",
      "org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-hdfs/dfs/data, DS-7922da36-b08a-4046-b03e-1a669a082235): no suitable block pools found to scan.  Waiting 1814399962 ms.\n",
      "org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/opt/hdfs/data, DS-ae5d34ab-b7b0-43b0-9cdd-5d7ccee7b2df): Scheduling suspect block BP-108841162-10.10.34.11-1440074360971:blk_1073747924_7100 for rescanning.\n",
      "org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/opt/hdfs/data, DS-ae5d34ab-b7b0-43b0-9cdd-5d7ccee7b2df): suspect block BP-108841162-10.10.34.11-1440074360971:blk_1074051196_310372 is already queued for rescanning.\n",
      "org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075\n",
      "org.mortbay.log: jetty-6.1.26\n",
      "org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n",
      "org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:60199\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "f=open(currdir+f\"/INPUT/{name}/{tipo}/{name}_Raw_{tipo}.txt\",\"r\")\n",
    "lines=f.readlines()\n",
    "originals=[]\n",
    "\n",
    "for i,line in enumerate(lines):\n",
    "    line=line.rstrip(\"\\n\")\n",
    "    print(line)\n",
    "    originals.append(line)\n",
    "f.close()\n",
    "print(len(originals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "coordinated-aging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Raw</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.2428</td>\n",
       "      <td>eof, java io eof, io eof, io eof exception, eo...</td>\n",
       "      <td>Caused by  java.io.EOFException</td>\n",
       "      <td>[cause, java, io, eof, exception, java io, io ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4683</td>\n",
       "      <td>exception, java, socket, timeout, channel, cha...</td>\n",
       "      <td>Caused by  java.io.IOException: Connection res...</td>\n",
       "      <td>[cause, java, io, io, exception, connection, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.3906</td>\n",
       "      <td>exception, java, socket, timeout, channel, cha...</td>\n",
       "      <td>Caused by  java.net.SocketTimeoutException: 60...</td>\n",
       "      <td>[cause, java, net, socket, timeout, exception,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.2655</td>\n",
       "      <td>eof, java io eof, io eof, io eof exception, eo...</td>\n",
       "      <td>java.io.EOFException</td>\n",
       "      <td>[java, io, eof, exception, java io, io eof, eo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.7696</td>\n",
       "      <td>eof, java io eof, io eof, io eof exception, eo...</td>\n",
       "      <td>java.io.EOFException  End of File Exception be...</td>\n",
       "      <td>[java, io, eof, exception, end, file, exceptio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.1258</td>\n",
       "      <td>exception, java, socket, timeout, channel, cha...</td>\n",
       "      <td>java.io.IOException  Failed on local exception...</td>\n",
       "      <td>[java, io, io, exception, fail, local, excepti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7449</td>\n",
       "      <td>exception, java, socket, timeout, channel, cha...</td>\n",
       "      <td>java.lang.NullPointerException</td>\n",
       "      <td>[java, null, pointer, exception, java null, nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.1812</td>\n",
       "      <td>exception, java, socket, timeout, channel, cha...</td>\n",
       "      <td>java.net.SocketTimeoutException  Call From mes...</td>\n",
       "      <td>[java, net, socket, timeout, exception, call, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.7406</td>\n",
       "      <td>http, filter, static, add, user, filter contex...</td>\n",
       "      <td>org.apache.hadoop.http.HttpRequestLog: Http re...</td>\n",
       "      <td>[http, http, request, http, request, http, req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.2992</td>\n",
       "      <td>http, filter, static, add, user, filter contex...</td>\n",
       "      <td>org.apache.hadoop.http.HttpServer2: Added filt...</td>\n",
       "      <td>[http, http, add, filter, static, user, filter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.3554</td>\n",
       "      <td>http, filter, static, add, user, filter contex...</td>\n",
       "      <td>org.apache.hadoop.http.HttpServer2: Added filt...</td>\n",
       "      <td>[http, http, add, filter, static, user, filter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5249</td>\n",
       "      <td>http, filter, static, add, user, filter contex...</td>\n",
       "      <td>org.apache.hadoop.http.HttpServer2: Added filt...</td>\n",
       "      <td>[http, http, add, filter, static, user, filter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.9801</td>\n",
       "      <td>http, filter, static, add, user, filter contex...</td>\n",
       "      <td>org.apache.hadoop.http.HttpServer2: Added glob...</td>\n",
       "      <td>[http, http, add, global, filter, safety, add ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3623</td>\n",
       "      <td>http, filter, static, add, user, filter contex...</td>\n",
       "      <td>org.apache.hadoop.http.HttpServer2: Jetty boun...</td>\n",
       "      <td>[http, http, jetty, bound, port, jetty bound, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.3963</td>\n",
       "      <td>retry, queue, ipc, storage, retry policy, call...</td>\n",
       "      <td>org.apache.hadoop.ipc.CallQueueManager: Using ...</td>\n",
       "      <td>[ipc, call, queue, manager, use, call, queue, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.9093</td>\n",
       "      <td>retry, queue, ipc, storage, retry policy, call...</td>\n",
       "      <td>org.apache.hadoop.ipc.Client: Retrying connect...</td>\n",
       "      <td>[ipc, client, retry, connect, try, time, retry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.6386</td>\n",
       "      <td>retry, queue, ipc, storage, retry policy, call...</td>\n",
       "      <td>org.apache.hadoop.ipc.Server: IPC Server liste...</td>\n",
       "      <td>[ipc, ipc, listener, start, ipc listener, list...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.6391</td>\n",
       "      <td>retry, queue, ipc, storage, retry policy, call...</td>\n",
       "      <td>org.apache.hadoop.ipc.Server: IPC Server Respo...</td>\n",
       "      <td>[ipc, ipc, responder, start, ipc responder, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.4020</td>\n",
       "      <td>metric, system, start, impl, fsdataset, datano...</td>\n",
       "      <td>org.apache.hadoop.ipc.Server: Starting Socket ...</td>\n",
       "      <td>[ipc, start, socket, reader, port, start socke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.7444</td>\n",
       "      <td>metric, system, start, impl, fsdataset, datano...</td>\n",
       "      <td>org.apache.hadoop.metrics2.impl.MetricsConfig:...</td>\n",
       "      <td>[metric, impl, metric, config, load, property,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.9265</td>\n",
       "      <td>metric, system, start, impl, fsdataset, datano...</td>\n",
       "      <td>org.apache.hadoop.metrics2.impl.MetricsSystemI...</td>\n",
       "      <td>[metric, impl, metric, system, impl, metric, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.9199</td>\n",
       "      <td>metric, system, start, impl, fsdataset, datano...</td>\n",
       "      <td>org.apache.hadoop.metrics2.impl.MetricsSystemI...</td>\n",
       "      <td>[metric, impl, metric, system, impl, schedule,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0506</td>\n",
       "      <td>authentication, secret, file, initialize, init...</td>\n",
       "      <td>org.apache.hadoop.security.authentication.serv...</td>\n",
       "      <td>[security, authentication, authentication, fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.8335</td>\n",
       "      <td>storage, common, directory, service, queue, st...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.common.Storage: ...</td>\n",
       "      <td>[common, storage, analyze, storage, directory,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.2075</td>\n",
       "      <td>storage, common, directory, service, queue, st...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.common.Storage: ...</td>\n",
       "      <td>[common, storage, pool, storage, directory, no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.8616</td>\n",
       "      <td>storage, common, directory, service, queue, st...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.common.Storage: ...</td>\n",
       "      <td>[common, storage, format]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.4678</td>\n",
       "      <td>block pool, pool, block miss, service, miss, i...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.common.Storage: ...</td>\n",
       "      <td>[common, storage, format, pool, directory, for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.8536</td>\n",
       "      <td>storage, common, directory, service, queue, st...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.common.Storage: ...</td>\n",
       "      <td>[common, storage, lock, acquire, nodename, loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.7866</td>\n",
       "      <td>storage, common, directory, service, queue, st...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.common.Storage: ...</td>\n",
       "      <td>[common, storage, lock, disable, lock disable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.8917</td>\n",
       "      <td>storage, common, directory, service, queue, st...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.common.Storage: ...</td>\n",
       "      <td>[common, storage, restore, file, trash, restor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.3136</td>\n",
       "      <td>storage, common, directory, service, queue, st...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.common.Storage: ...</td>\n",
       "      <td>[common, storage, storage, directory, not, for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.2190</td>\n",
       "      <td>authentication, secret, file, initialize, init...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.BlockSc...</td>\n",
       "      <td>[scanner, initialize, scanner, target, byte, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8985</td>\n",
       "      <td>block pool, pool, block miss, service, miss, i...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[acknowledge, active, namenode, pool, service,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>disk, length, namenode, service, write, not, s...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[balance, bandwith, byte, balance bandwith, ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.2166</td>\n",
       "      <td>block pool, pool, block miss, service, miss, i...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[pool, pool, service, register, nn, block pool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.9435</td>\n",
       "      <td>block pool, pool, block miss, service, miss, i...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[pool, register, service, start, offer, servic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.7473</td>\n",
       "      <td>block pool, pool, block miss, service, miss, i...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[pool, service, begin, handshake, nn, block po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.8230</td>\n",
       "      <td>disk, length, namenode, service, write, not, s...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[clienttrace, src, dest, byte, write, offset, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0807</td>\n",
       "      <td>disk, length, namenode, service, write, not, s...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[configure, hostname, configure hostname]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.3111</td>\n",
       "      <td>report, command, finalize command, msec, repor...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[data, transfer, transmit, data transfer, tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.3626</td>\n",
       "      <td>metric, system, start, impl, fsdataset, datano...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[command, action, register, active, state, dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.1702</td>\n",
       "      <td>http, filter, static, add, user, filter contex...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[registration, start, thread, transfer, datano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.3654</td>\n",
       "      <td>disk, length, namenode, service, write, not, s...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[user, name, user name]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1206</td>\n",
       "      <td>report, command, finalize command, msec, repor...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[error, process, unknown, operation, src, dest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9111</td>\n",
       "      <td>report, command, finalize command, msec, repor...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[generate, persist, new, generate persist, per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5809</td>\n",
       "      <td>exception, java, socket, timeout, channel, cha...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[get, finalize, command, pool, get finalize, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.1428</td>\n",
       "      <td>block pool, pool, block miss, service, miss, i...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[io, exception, offer, service, io exception, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.2639</td>\n",
       "      <td>interval, queue, interval cachereport, call, r...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[namenode, pool, service, try, claim, active, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.6082</td>\n",
       "      <td>disk, length, namenode, service, write, not, s...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[namenode, use, deletereport, interval, msec, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>metric, system, start, impl, fsdataset, datano...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[not, replicate, disk, length, shorter, nameno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.2696</td>\n",
       "      <td>retry, queue, ipc, storage, retry policy, call...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[number, thread, balance, number thread, threa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>retry, queue, ipc, storage, retry policy, call...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[open, ipc, open ipc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0785</td>\n",
       "      <td>disk, length, namenode, service, write, not, s...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[open, stream, open stream]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>disk, length, namenode, service, write, not, s...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[packet, responder, terminate, packet responde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.2988</td>\n",
       "      <td>disk, length, namenode, service, write, not, s...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[receive, signal, sigterm, receive signal, sig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.2648</td>\n",
       "      <td>disk, length, namenode, service, write, not, s...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[receive, src, dest, size, receive src, src de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.4306</td>\n",
       "      <td>request, http, http request, storage, interval...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[receive, src, dest, receive src, src dest, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0969</td>\n",
       "      <td>block pool, pool, block miss, service, miss, i...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[refresh, request, receive, nameservices, null...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.4614</td>\n",
       "      <td>storage, common, directory, service, queue, st...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[register, unix, signal, handler, register uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.6255</td>\n",
       "      <td>disk, length, namenode, service, write, not, s...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[set, storage, set storage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.5671</td>\n",
       "      <td>disk, length, namenode, service, write, not, s...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[shutdown, msg, shutdown msg]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.4402</td>\n",
       "      <td>disk, length, namenode, service, write, not, s...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[slow, receiver, write, data, disk, slow block...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.7138</td>\n",
       "      <td>metric, system, start, impl, fsdataset, datano...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[slow, receiver, write, packet, mirror, take, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.3602</td>\n",
       "      <td>metric, system, start, impl, fsdataset, datano...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[slow, manage, writer, os, cache, take, slow m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.3545</td>\n",
       "      <td>report, command, finalize command, msec, repor...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.DataNod...</td>\n",
       "      <td>[start, bp, offer, service, nameservices, defa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.3723</td>\n",
       "      <td>block pool, pool, block miss, service, miss, i...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.Directo...</td>\n",
       "      <td>[start, max, lock, memory, start datanode, dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.9534</td>\n",
       "      <td>storage, common, directory, service, queue, st...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.Directo...</td>\n",
       "      <td>[startup, msg, startup msg]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.8972</td>\n",
       "      <td>disk, length, namenode, service, write, not, s...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.fsdatas...</td>\n",
       "      <td>[send, report, contain, storage, report, send,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0524</td>\n",
       "      <td>disk, length, namenode, service, write, not, s...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.fsdatas...</td>\n",
       "      <td>[supergroup, hdfsgroup, supergroup hdfsgroup]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>69</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>scanner, volume, fsdataset, impl, volume scann...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.fsdatas...</td>\n",
       "      <td>[supergroup, supergroup]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.7154</td>\n",
       "      <td>storage, common, directory, service, queue, st...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.fsdatas...</td>\n",
       "      <td>[directory, scanner, pool, total, miss, metada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0137</td>\n",
       "      <td>scanner, volume, fsdataset, impl, volume scann...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.fsdatas...</td>\n",
       "      <td>[directory, scanner, periodic, directory, tree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6932</td>\n",
       "      <td>scanner, volume, fsdataset, impl, volume scann...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.fsdatas...</td>\n",
       "      <td>[fsdataset, impl, fsdataset, async, disk, serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0322</td>\n",
       "      <td>scanner, volume, fsdataset, impl, volume scann...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.fsdatas...</td>\n",
       "      <td>[fsdataset, impl, fsdataset, async, disk, serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3809</td>\n",
       "      <td>scanner, volume, fsdataset, impl, volume scann...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.fsdatas...</td>\n",
       "      <td>[fsdataset, impl, fsdataset, impl, add, new, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3233</td>\n",
       "      <td>scanner, volume, fsdataset, impl, volume scann...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.fsdatas...</td>\n",
       "      <td>[fsdataset, impl, fsdataset, impl, add, volume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.7832</td>\n",
       "      <td>scanner, volume, fsdataset, impl, volume scann...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.fsdatas...</td>\n",
       "      <td>[fsdataset, impl, fsdataset, impl, add, pool, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9242</td>\n",
       "      <td>scanner, volume, fsdataset, impl, volume scann...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.fsdatas...</td>\n",
       "      <td>[fsdataset, impl, fsdataset, impl, add, replic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3798</td>\n",
       "      <td>scanner, volume, fsdataset, impl, volume scann...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.fsdatas...</td>\n",
       "      <td>[fsdataset, impl, fsdataset, impl, cache, dfs,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0671</td>\n",
       "      <td>scanner, volume, fsdataset, impl, volume scann...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.fsdatas...</td>\n",
       "      <td>[fsdataset, impl, fsdataset, impl, register, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.3598</td>\n",
       "      <td>authentication, secret, file, initialize, init...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.VolumeS...</td>\n",
       "      <td>[fsdataset, impl, fsdataset, impl, scan, pool,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0897</td>\n",
       "      <td>block miss, miss, storage, miss metadata, comm...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.VolumeS...</td>\n",
       "      <td>[fsdataset, impl, fsdataset, impl, time, add, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0916</td>\n",
       "      <td>block miss, miss, storage, miss metadata, comm...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.VolumeS...</td>\n",
       "      <td>[fsdataset, impl, fsdataset, impl, time, take,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0718</td>\n",
       "      <td>interval, queue, interval cachereport, call, r...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.VolumeS...</td>\n",
       "      <td>[fsdataset, impl, fsdataset, impl, total, time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4334</td>\n",
       "      <td>exception, java, socket, timeout, channel, cha...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.VolumeS...</td>\n",
       "      <td>[fsdataset, impl, fsdataset, impl, total, time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5279</td>\n",
       "      <td>block pool, pool, block miss, service, miss, i...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.VolumeS...</td>\n",
       "      <td>[volume, scanner, file, not, find, find, volum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.5121</td>\n",
       "      <td>disk, length, namenode, service, write, not, s...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.VolumeS...</td>\n",
       "      <td>[volume, scanner, rescan, bpid, volume, hour, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5776</td>\n",
       "      <td>block pool, pool, block miss, service, miss, i...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.VolumeS...</td>\n",
       "      <td>[volume, scanner, scan, bpid, volume, scan bpi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.2749</td>\n",
       "      <td>disk, length, namenode, service, write, not, s...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.VolumeS...</td>\n",
       "      <td>[volume, scanner, volume, scanner, exit, volum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.4142</td>\n",
       "      <td>retry, queue, ipc, storage, retry policy, call...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.VolumeS...</td>\n",
       "      <td>[volume, scanner, volume, scanner, exit, excep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3440</td>\n",
       "      <td>http, filter, static, add, user, filter contex...</td>\n",
       "      <td>org.apache.hadoop.hdfs.server.datanode.web.Dat...</td>\n",
       "      <td>[volume, scanner, volume, scanner, finish, sca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>metric, system, start, impl, fsdataset, datano...</td>\n",
       "      <td>org.mortbay.log: jetty-6.1.26</td>\n",
       "      <td>[volume, scanner, volume, scanner, not, schedu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0562</td>\n",
       "      <td>http, filter, static, add, user, filter contex...</td>\n",
       "      <td>org.mortbay.log: Logging to org.slf4j.impl.Log...</td>\n",
       "      <td>[volume, scanner, volume, scanner, not, suitab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.2683</td>\n",
       "      <td>metric, system, start, impl, fsdataset, datano...</td>\n",
       "      <td>org.mortbay.log: Started HttpServer2$SelectCha...</td>\n",
       "      <td>[volume, scanner, volume, scanner, schedule, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[volume, scanner, volume, scanner, suspect, qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[http, listen, http, traffic, listen http, htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[mortbay]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[mortbay, log, mortbay log]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[mortbay, start, mortbay start]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0             0            15.0              1.2428   \n",
       "1             1             0.0              1.4683   \n",
       "2             2             0.0              6.3906   \n",
       "3             3            15.0              1.2655   \n",
       "4             4            15.0              3.7696   \n",
       "5             5             0.0              4.1258   \n",
       "6             6             0.0              0.7449   \n",
       "7             7             0.0             10.1812   \n",
       "8             8             4.0              3.7406   \n",
       "9             9             4.0              4.2992   \n",
       "10           10             4.0              4.3554   \n",
       "11           11             4.0              4.5249   \n",
       "12           12             4.0              1.9801   \n",
       "13           13             4.0              1.3623   \n",
       "14           14             7.0              3.3963   \n",
       "15           15             7.0              3.9093   \n",
       "16           16             7.0              0.6386   \n",
       "17           17             7.0              0.6391   \n",
       "18           18            14.0              0.4020   \n",
       "19           19            14.0              1.7444   \n",
       "20           20            14.0              3.9265   \n",
       "21           21            14.0              2.9199   \n",
       "22           22            13.0              5.0506   \n",
       "23           23             9.0              1.8335   \n",
       "24           24             9.0              2.2075   \n",
       "25           25             9.0              0.8616   \n",
       "26           26             6.0              1.4678   \n",
       "27           27             9.0              0.8536   \n",
       "28           28             9.0              0.7866   \n",
       "29           29             9.0              0.8917   \n",
       "30           30             9.0              2.3136   \n",
       "31           31            13.0              0.2190   \n",
       "32           32             6.0              1.8985   \n",
       "33           33            16.0              0.0428   \n",
       "34           34             6.0              3.2166   \n",
       "35           35             6.0              1.9435   \n",
       "36           36             6.0              1.7473   \n",
       "37           37            16.0              0.8230   \n",
       "38           38            16.0              0.0807   \n",
       "39           39             5.0              0.3111   \n",
       "40           40            14.0              0.3626   \n",
       "41           41             4.0              0.1702   \n",
       "42           42            16.0              0.3654   \n",
       "43           43             5.0              0.1206   \n",
       "44           44             5.0              0.9111   \n",
       "45           45             0.0              0.5809   \n",
       "46           46             6.0              2.1428   \n",
       "47           47            12.0              4.2639   \n",
       "48           48            16.0              3.6082   \n",
       "49           49            14.0              0.0138   \n",
       "50           50             7.0              0.2696   \n",
       "51           51             7.0              0.0071   \n",
       "52           52            16.0              0.0785   \n",
       "53           53            16.0              0.0306   \n",
       "54           54            16.0              0.2988   \n",
       "55           55            16.0              0.2648   \n",
       "56           56            11.0              0.4306   \n",
       "57           57             6.0              0.0969   \n",
       "58           58             9.0              0.4614   \n",
       "59           59            16.0              1.6255   \n",
       "60           60            16.0              1.5671   \n",
       "61           61            16.0              0.4402   \n",
       "62           62            14.0              0.7138   \n",
       "63           63            14.0              0.3602   \n",
       "64           64             5.0              8.3545   \n",
       "65           65             6.0              3.3723   \n",
       "66           66             9.0              0.9534   \n",
       "67           67            16.0              0.8972   \n",
       "68           68            16.0              1.0524   \n",
       "69           69             3.0              0.5525   \n",
       "70           70             9.0              0.7154   \n",
       "71           71             3.0              1.0137   \n",
       "72           72             3.0              0.6932   \n",
       "73           73             3.0              1.0322   \n",
       "74           74             3.0              1.3809   \n",
       "75           75             3.0              0.3233   \n",
       "76           76             3.0              0.7832   \n",
       "77           77             3.0              0.9242   \n",
       "78           78             3.0              1.3798   \n",
       "79           79             3.0              1.0671   \n",
       "80           80            13.0              0.3598   \n",
       "81           81             8.0              0.0897   \n",
       "82           82             8.0              0.0916   \n",
       "83           83            12.0              0.0718   \n",
       "84           84             0.0              0.4334   \n",
       "85           85             6.0              0.5279   \n",
       "86           86            16.0              0.5121   \n",
       "87           87             6.0              0.5776   \n",
       "88           88            16.0              0.2749   \n",
       "89           89             7.0              0.4142   \n",
       "90           90             4.0              1.3440   \n",
       "91           91            14.0              0.0089   \n",
       "92           92             4.0              0.0562   \n",
       "93           93            14.0              0.2683   \n",
       "94           94             NaN                 NaN   \n",
       "95           95             NaN                 NaN   \n",
       "96           96             NaN                 NaN   \n",
       "97           97             NaN                 NaN   \n",
       "98           98             NaN                 NaN   \n",
       "\n",
       "                                             Keywords  \\\n",
       "0   eof, java io eof, io eof, io eof exception, eo...   \n",
       "1   exception, java, socket, timeout, channel, cha...   \n",
       "2   exception, java, socket, timeout, channel, cha...   \n",
       "3   eof, java io eof, io eof, io eof exception, eo...   \n",
       "4   eof, java io eof, io eof, io eof exception, eo...   \n",
       "5   exception, java, socket, timeout, channel, cha...   \n",
       "6   exception, java, socket, timeout, channel, cha...   \n",
       "7   exception, java, socket, timeout, channel, cha...   \n",
       "8   http, filter, static, add, user, filter contex...   \n",
       "9   http, filter, static, add, user, filter contex...   \n",
       "10  http, filter, static, add, user, filter contex...   \n",
       "11  http, filter, static, add, user, filter contex...   \n",
       "12  http, filter, static, add, user, filter contex...   \n",
       "13  http, filter, static, add, user, filter contex...   \n",
       "14  retry, queue, ipc, storage, retry policy, call...   \n",
       "15  retry, queue, ipc, storage, retry policy, call...   \n",
       "16  retry, queue, ipc, storage, retry policy, call...   \n",
       "17  retry, queue, ipc, storage, retry policy, call...   \n",
       "18  metric, system, start, impl, fsdataset, datano...   \n",
       "19  metric, system, start, impl, fsdataset, datano...   \n",
       "20  metric, system, start, impl, fsdataset, datano...   \n",
       "21  metric, system, start, impl, fsdataset, datano...   \n",
       "22  authentication, secret, file, initialize, init...   \n",
       "23  storage, common, directory, service, queue, st...   \n",
       "24  storage, common, directory, service, queue, st...   \n",
       "25  storage, common, directory, service, queue, st...   \n",
       "26  block pool, pool, block miss, service, miss, i...   \n",
       "27  storage, common, directory, service, queue, st...   \n",
       "28  storage, common, directory, service, queue, st...   \n",
       "29  storage, common, directory, service, queue, st...   \n",
       "30  storage, common, directory, service, queue, st...   \n",
       "31  authentication, secret, file, initialize, init...   \n",
       "32  block pool, pool, block miss, service, miss, i...   \n",
       "33  disk, length, namenode, service, write, not, s...   \n",
       "34  block pool, pool, block miss, service, miss, i...   \n",
       "35  block pool, pool, block miss, service, miss, i...   \n",
       "36  block pool, pool, block miss, service, miss, i...   \n",
       "37  disk, length, namenode, service, write, not, s...   \n",
       "38  disk, length, namenode, service, write, not, s...   \n",
       "39  report, command, finalize command, msec, repor...   \n",
       "40  metric, system, start, impl, fsdataset, datano...   \n",
       "41  http, filter, static, add, user, filter contex...   \n",
       "42  disk, length, namenode, service, write, not, s...   \n",
       "43  report, command, finalize command, msec, repor...   \n",
       "44  report, command, finalize command, msec, repor...   \n",
       "45  exception, java, socket, timeout, channel, cha...   \n",
       "46  block pool, pool, block miss, service, miss, i...   \n",
       "47  interval, queue, interval cachereport, call, r...   \n",
       "48  disk, length, namenode, service, write, not, s...   \n",
       "49  metric, system, start, impl, fsdataset, datano...   \n",
       "50  retry, queue, ipc, storage, retry policy, call...   \n",
       "51  retry, queue, ipc, storage, retry policy, call...   \n",
       "52  disk, length, namenode, service, write, not, s...   \n",
       "53  disk, length, namenode, service, write, not, s...   \n",
       "54  disk, length, namenode, service, write, not, s...   \n",
       "55  disk, length, namenode, service, write, not, s...   \n",
       "56  request, http, http request, storage, interval...   \n",
       "57  block pool, pool, block miss, service, miss, i...   \n",
       "58  storage, common, directory, service, queue, st...   \n",
       "59  disk, length, namenode, service, write, not, s...   \n",
       "60  disk, length, namenode, service, write, not, s...   \n",
       "61  disk, length, namenode, service, write, not, s...   \n",
       "62  metric, system, start, impl, fsdataset, datano...   \n",
       "63  metric, system, start, impl, fsdataset, datano...   \n",
       "64  report, command, finalize command, msec, repor...   \n",
       "65  block pool, pool, block miss, service, miss, i...   \n",
       "66  storage, common, directory, service, queue, st...   \n",
       "67  disk, length, namenode, service, write, not, s...   \n",
       "68  disk, length, namenode, service, write, not, s...   \n",
       "69  scanner, volume, fsdataset, impl, volume scann...   \n",
       "70  storage, common, directory, service, queue, st...   \n",
       "71  scanner, volume, fsdataset, impl, volume scann...   \n",
       "72  scanner, volume, fsdataset, impl, volume scann...   \n",
       "73  scanner, volume, fsdataset, impl, volume scann...   \n",
       "74  scanner, volume, fsdataset, impl, volume scann...   \n",
       "75  scanner, volume, fsdataset, impl, volume scann...   \n",
       "76  scanner, volume, fsdataset, impl, volume scann...   \n",
       "77  scanner, volume, fsdataset, impl, volume scann...   \n",
       "78  scanner, volume, fsdataset, impl, volume scann...   \n",
       "79  scanner, volume, fsdataset, impl, volume scann...   \n",
       "80  authentication, secret, file, initialize, init...   \n",
       "81  block miss, miss, storage, miss metadata, comm...   \n",
       "82  block miss, miss, storage, miss metadata, comm...   \n",
       "83  interval, queue, interval cachereport, call, r...   \n",
       "84  exception, java, socket, timeout, channel, cha...   \n",
       "85  block pool, pool, block miss, service, miss, i...   \n",
       "86  disk, length, namenode, service, write, not, s...   \n",
       "87  block pool, pool, block miss, service, miss, i...   \n",
       "88  disk, length, namenode, service, write, not, s...   \n",
       "89  retry, queue, ipc, storage, retry policy, call...   \n",
       "90  http, filter, static, add, user, filter contex...   \n",
       "91  metric, system, start, impl, fsdataset, datano...   \n",
       "92  http, filter, static, add, user, filter contex...   \n",
       "93  metric, system, start, impl, fsdataset, datano...   \n",
       "94                                                NaN   \n",
       "95                                                NaN   \n",
       "96                                                NaN   \n",
       "97                                                NaN   \n",
       "98                                                NaN   \n",
       "\n",
       "                                                  Raw  \\\n",
       "0                     Caused by  java.io.EOFException   \n",
       "1   Caused by  java.io.IOException: Connection res...   \n",
       "2   Caused by  java.net.SocketTimeoutException: 60...   \n",
       "3                                java.io.EOFException   \n",
       "4   java.io.EOFException  End of File Exception be...   \n",
       "5   java.io.IOException  Failed on local exception...   \n",
       "6                      java.lang.NullPointerException   \n",
       "7   java.net.SocketTimeoutException  Call From mes...   \n",
       "8   org.apache.hadoop.http.HttpRequestLog: Http re...   \n",
       "9   org.apache.hadoop.http.HttpServer2: Added filt...   \n",
       "10  org.apache.hadoop.http.HttpServer2: Added filt...   \n",
       "11  org.apache.hadoop.http.HttpServer2: Added filt...   \n",
       "12  org.apache.hadoop.http.HttpServer2: Added glob...   \n",
       "13  org.apache.hadoop.http.HttpServer2: Jetty boun...   \n",
       "14  org.apache.hadoop.ipc.CallQueueManager: Using ...   \n",
       "15  org.apache.hadoop.ipc.Client: Retrying connect...   \n",
       "16  org.apache.hadoop.ipc.Server: IPC Server liste...   \n",
       "17  org.apache.hadoop.ipc.Server: IPC Server Respo...   \n",
       "18  org.apache.hadoop.ipc.Server: Starting Socket ...   \n",
       "19  org.apache.hadoop.metrics2.impl.MetricsConfig:...   \n",
       "20  org.apache.hadoop.metrics2.impl.MetricsSystemI...   \n",
       "21  org.apache.hadoop.metrics2.impl.MetricsSystemI...   \n",
       "22  org.apache.hadoop.security.authentication.serv...   \n",
       "23  org.apache.hadoop.hdfs.server.common.Storage: ...   \n",
       "24  org.apache.hadoop.hdfs.server.common.Storage: ...   \n",
       "25  org.apache.hadoop.hdfs.server.common.Storage: ...   \n",
       "26  org.apache.hadoop.hdfs.server.common.Storage: ...   \n",
       "27  org.apache.hadoop.hdfs.server.common.Storage: ...   \n",
       "28  org.apache.hadoop.hdfs.server.common.Storage: ...   \n",
       "29  org.apache.hadoop.hdfs.server.common.Storage: ...   \n",
       "30  org.apache.hadoop.hdfs.server.common.Storage: ...   \n",
       "31  org.apache.hadoop.hdfs.server.datanode.BlockSc...   \n",
       "32  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "33  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "34  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "35  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "36  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "37  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "38  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "39  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "40  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "41  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "42  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "43  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "44  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "45  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "46  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "47  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "48  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "49  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "50  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "51  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "52  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "53  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "54  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "55  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "56  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "57  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "58  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "59  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "60  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "61  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "62  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "63  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "64  org.apache.hadoop.hdfs.server.datanode.DataNod...   \n",
       "65  org.apache.hadoop.hdfs.server.datanode.Directo...   \n",
       "66  org.apache.hadoop.hdfs.server.datanode.Directo...   \n",
       "67  org.apache.hadoop.hdfs.server.datanode.fsdatas...   \n",
       "68  org.apache.hadoop.hdfs.server.datanode.fsdatas...   \n",
       "69  org.apache.hadoop.hdfs.server.datanode.fsdatas...   \n",
       "70  org.apache.hadoop.hdfs.server.datanode.fsdatas...   \n",
       "71  org.apache.hadoop.hdfs.server.datanode.fsdatas...   \n",
       "72  org.apache.hadoop.hdfs.server.datanode.fsdatas...   \n",
       "73  org.apache.hadoop.hdfs.server.datanode.fsdatas...   \n",
       "74  org.apache.hadoop.hdfs.server.datanode.fsdatas...   \n",
       "75  org.apache.hadoop.hdfs.server.datanode.fsdatas...   \n",
       "76  org.apache.hadoop.hdfs.server.datanode.fsdatas...   \n",
       "77  org.apache.hadoop.hdfs.server.datanode.fsdatas...   \n",
       "78  org.apache.hadoop.hdfs.server.datanode.fsdatas...   \n",
       "79  org.apache.hadoop.hdfs.server.datanode.fsdatas...   \n",
       "80  org.apache.hadoop.hdfs.server.datanode.VolumeS...   \n",
       "81  org.apache.hadoop.hdfs.server.datanode.VolumeS...   \n",
       "82  org.apache.hadoop.hdfs.server.datanode.VolumeS...   \n",
       "83  org.apache.hadoop.hdfs.server.datanode.VolumeS...   \n",
       "84  org.apache.hadoop.hdfs.server.datanode.VolumeS...   \n",
       "85  org.apache.hadoop.hdfs.server.datanode.VolumeS...   \n",
       "86  org.apache.hadoop.hdfs.server.datanode.VolumeS...   \n",
       "87  org.apache.hadoop.hdfs.server.datanode.VolumeS...   \n",
       "88  org.apache.hadoop.hdfs.server.datanode.VolumeS...   \n",
       "89  org.apache.hadoop.hdfs.server.datanode.VolumeS...   \n",
       "90  org.apache.hadoop.hdfs.server.datanode.web.Dat...   \n",
       "91                      org.mortbay.log: jetty-6.1.26   \n",
       "92  org.mortbay.log: Logging to org.slf4j.impl.Log...   \n",
       "93  org.mortbay.log: Started HttpServer2$SelectCha...   \n",
       "94                                                NaN   \n",
       "95                                                NaN   \n",
       "96                                                NaN   \n",
       "97                                                NaN   \n",
       "98                                                NaN   \n",
       "\n",
       "                                                 Text  \n",
       "0   [cause, java, io, eof, exception, java io, io ...  \n",
       "1   [cause, java, io, io, exception, connection, r...  \n",
       "2   [cause, java, net, socket, timeout, exception,...  \n",
       "3   [java, io, eof, exception, java io, io eof, eo...  \n",
       "4   [java, io, eof, exception, end, file, exceptio...  \n",
       "5   [java, io, io, exception, fail, local, excepti...  \n",
       "6   [java, null, pointer, exception, java null, nu...  \n",
       "7   [java, net, socket, timeout, exception, call, ...  \n",
       "8   [http, http, request, http, request, http, req...  \n",
       "9   [http, http, add, filter, static, user, filter...  \n",
       "10  [http, http, add, filter, static, user, filter...  \n",
       "11  [http, http, add, filter, static, user, filter...  \n",
       "12  [http, http, add, global, filter, safety, add ...  \n",
       "13  [http, http, jetty, bound, port, jetty bound, ...  \n",
       "14  [ipc, call, queue, manager, use, call, queue, ...  \n",
       "15  [ipc, client, retry, connect, try, time, retry...  \n",
       "16  [ipc, ipc, listener, start, ipc listener, list...  \n",
       "17  [ipc, ipc, responder, start, ipc responder, re...  \n",
       "18  [ipc, start, socket, reader, port, start socke...  \n",
       "19  [metric, impl, metric, config, load, property,...  \n",
       "20  [metric, impl, metric, system, impl, metric, s...  \n",
       "21  [metric, impl, metric, system, impl, schedule,...  \n",
       "22  [security, authentication, authentication, fil...  \n",
       "23  [common, storage, analyze, storage, directory,...  \n",
       "24  [common, storage, pool, storage, directory, no...  \n",
       "25                          [common, storage, format]  \n",
       "26  [common, storage, format, pool, directory, for...  \n",
       "27  [common, storage, lock, acquire, nodename, loc...  \n",
       "28     [common, storage, lock, disable, lock disable]  \n",
       "29  [common, storage, restore, file, trash, restor...  \n",
       "30  [common, storage, storage, directory, not, for...  \n",
       "31  [scanner, initialize, scanner, target, byte, i...  \n",
       "32  [acknowledge, active, namenode, pool, service,...  \n",
       "33  [balance, bandwith, byte, balance bandwith, ba...  \n",
       "34  [pool, pool, service, register, nn, block pool...  \n",
       "35  [pool, register, service, start, offer, servic...  \n",
       "36  [pool, service, begin, handshake, nn, block po...  \n",
       "37  [clienttrace, src, dest, byte, write, offset, ...  \n",
       "38          [configure, hostname, configure hostname]  \n",
       "39  [data, transfer, transmit, data transfer, tran...  \n",
       "40  [command, action, register, active, state, dat...  \n",
       "41  [registration, start, thread, transfer, datano...  \n",
       "42                            [user, name, user name]  \n",
       "43  [error, process, unknown, operation, src, dest...  \n",
       "44  [generate, persist, new, generate persist, per...  \n",
       "45  [get, finalize, command, pool, get finalize, f...  \n",
       "46  [io, exception, offer, service, io exception, ...  \n",
       "47  [namenode, pool, service, try, claim, active, ...  \n",
       "48  [namenode, use, deletereport, interval, msec, ...  \n",
       "49  [not, replicate, disk, length, shorter, nameno...  \n",
       "50  [number, thread, balance, number thread, threa...  \n",
       "51                              [open, ipc, open ipc]  \n",
       "52                        [open, stream, open stream]  \n",
       "53  [packet, responder, terminate, packet responde...  \n",
       "54  [receive, signal, sigterm, receive signal, sig...  \n",
       "55  [receive, src, dest, size, receive src, src de...  \n",
       "56  [receive, src, dest, receive src, src dest, re...  \n",
       "57  [refresh, request, receive, nameservices, null...  \n",
       "58  [register, unix, signal, handler, register uni...  \n",
       "59                        [set, storage, set storage]  \n",
       "60                      [shutdown, msg, shutdown msg]  \n",
       "61  [slow, receiver, write, data, disk, slow block...  \n",
       "62  [slow, receiver, write, packet, mirror, take, ...  \n",
       "63  [slow, manage, writer, os, cache, take, slow m...  \n",
       "64  [start, bp, offer, service, nameservices, defa...  \n",
       "65  [start, max, lock, memory, start datanode, dat...  \n",
       "66                        [startup, msg, startup msg]  \n",
       "67  [send, report, contain, storage, report, send,...  \n",
       "68      [supergroup, hdfsgroup, supergroup hdfsgroup]  \n",
       "69                           [supergroup, supergroup]  \n",
       "70  [directory, scanner, pool, total, miss, metada...  \n",
       "71  [directory, scanner, periodic, directory, tree...  \n",
       "72  [fsdataset, impl, fsdataset, async, disk, serv...  \n",
       "73  [fsdataset, impl, fsdataset, async, disk, serv...  \n",
       "74  [fsdataset, impl, fsdataset, impl, add, new, v...  \n",
       "75  [fsdataset, impl, fsdataset, impl, add, volume...  \n",
       "76  [fsdataset, impl, fsdataset, impl, add, pool, ...  \n",
       "77  [fsdataset, impl, fsdataset, impl, add, replic...  \n",
       "78  [fsdataset, impl, fsdataset, impl, cache, dfs,...  \n",
       "79  [fsdataset, impl, fsdataset, impl, register, f...  \n",
       "80  [fsdataset, impl, fsdataset, impl, scan, pool,...  \n",
       "81  [fsdataset, impl, fsdataset, impl, time, add, ...  \n",
       "82  [fsdataset, impl, fsdataset, impl, time, take,...  \n",
       "83  [fsdataset, impl, fsdataset, impl, total, time...  \n",
       "84  [fsdataset, impl, fsdataset, impl, total, time...  \n",
       "85  [volume, scanner, file, not, find, find, volum...  \n",
       "86  [volume, scanner, rescan, bpid, volume, hour, ...  \n",
       "87  [volume, scanner, scan, bpid, volume, scan bpi...  \n",
       "88  [volume, scanner, volume, scanner, exit, volum...  \n",
       "89  [volume, scanner, volume, scanner, exit, excep...  \n",
       "90  [volume, scanner, volume, scanner, finish, sca...  \n",
       "91  [volume, scanner, volume, scanner, not, schedu...  \n",
       "92  [volume, scanner, volume, scanner, not, suitab...  \n",
       "93  [volume, scanner, volume, scanner, schedule, s...  \n",
       "94  [volume, scanner, volume, scanner, suspect, qu...  \n",
       "95  [http, listen, http, traffic, listen http, htt...  \n",
       "96                                          [mortbay]  \n",
       "97                        [mortbay, log, mortbay log]  \n",
       "98                    [mortbay, start, mortbay start]  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################### SENTENCE TOPIC DOMINANCE ###############################\n",
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data, raw=originals):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list#[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                orig=raw[i]\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords, orig]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords','Raw']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=best_nmf_model, corpus=corpus, texts=data, raw=originals)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords','Raw','Text']\n",
    "df_dominant_topic.to_csv(currdir+path+f\"{name}_RESULTS_{tipo}.csv\")\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fifth-investing",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_873881/1106837561.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0msentences_chart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_873881/1106837561.py\u001b[0m in \u001b[0;36msentences_chart\u001b[0;34m(lda_model, corpus, start, end)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcorp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXKCD_COLORS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbest_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmycolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_k' is not defined"
     ]
    }
   ],
   "source": [
    "##################### SENTENCE CHART TOPIC COLOR ######################################\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def sentences_chart(lda_model=best_nmf_model, corpus=corpus, start = 0, end = 45):\n",
    "    corp = corpus[start:end]\n",
    "    tmp = [color for name, color in mcolors.XKCD_COLORS.items()]#mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "    offset=int(len(tmp)/best_k)\n",
    "    print(offset)\n",
    "    mycolors=tmp[1::offset]\n",
    "    print(cols)\n",
    "\n",
    "    fig, axes = plt.subplots(end-start, 1, figsize=(30, (end-start)*0.95), dpi=150)       \n",
    "    axes[0].axis('off')\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.axis(\"off\")\n",
    "        if i > 0:\n",
    "            corp_cur = corp[i-1] \n",
    "            #print(corp_cur)\n",
    "            topic_percs = lda_model[corp_cur]\n",
    "            #print(topic_percs)\n",
    "            #wordid_topics = \n",
    "            word_dominanttopic=[]\n",
    "            for id,f in corp_cur:\n",
    "                w=lda_model.id2word[id]\n",
    "                t=lda_model.get_term_topics(id)\n",
    "                #print(w,f,t)\n",
    "                #print(\"\\n\")\n",
    "                if len(t)>0:\n",
    "                    word_dominanttopic.append((w,t[0][0]))\n",
    "            #print(lda_model.get_document_topics(corp_cur))\n",
    "            #word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n",
    "            ax.text(0.01, 0.5, \"Doc \" + str(i-1) + \": \", verticalalignment='center',\n",
    "                    fontsize=16, color='black', transform=ax.transAxes, fontweight=700)\n",
    "\n",
    "            # Draw Rectange\n",
    "            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n",
    "            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill=None, alpha=1, \n",
    "                                   color=mycolors[topic_percs_sorted[0][0]], linewidth=2))\n",
    "\n",
    "            word_pos = 0.06\n",
    "            for j, (word, topics) in enumerate(word_dominanttopic):\n",
    "                if j < 14:\n",
    "                    ax.text(word_pos, 0.3, word,\n",
    "                            horizontalalignment='left',\n",
    "                            verticalalignment='center',\n",
    "                            fontsize=16, color=mycolors[topics],\n",
    "                            transform=ax.transAxes, fontweight=700)\n",
    "                    mul=len(word)\n",
    "                    if len(word)>15:\n",
    "                        mul=mul/1.1\n",
    "                    word_pos += .005 * mul  # to move the word for the next iter\n",
    "                    ax.axis('off')\n",
    "            ax.text(word_pos, 0.5, '. . .',\n",
    "                    horizontalalignment='left',\n",
    "                    verticalalignment='center',\n",
    "                    fontsize=16, color='black',\n",
    "                    transform=ax.transAxes)       \n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.suptitle('Sentence Topic Coloring for Documents: ' + str(start) + ' to ' + str(end-2), fontsize=22, y=0.99, x=0.5, fontweight=700)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(currdir+path+f\"{name}_{tipo}_NMF_SENTENCE_CHART_TOPIC_COLOR.png\")\n",
    "    plt.show()\n",
    "\n",
    "sentences_chart()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpcs=df_dominant_topic['Dominant_Topic'].values.tolist()\n",
    "print(type(tpcs))\n",
    "topics=[]\n",
    "for tp in tpcs:\n",
    "    topics.append(int(tp))\n",
    "print(topics)\n",
    "tp_set=set(topics)\n",
    "print(tp_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### T-SNE  ###################################\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(best_lda_model[corpus]):\n",
    "    #print(row_list)\n",
    "    topic_weights.append([w for i, w in row_list])\n",
    "\n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "print(arr.shape)\n",
    "\n",
    "# Keep the well separated points (optional)\n",
    "#arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "print(arr.shape)\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "print(tsne_lda.shape)\n",
    "print(type(tsne_lda))\n",
    "\n",
    "tmp = [color for name, color in mcolors.XKCD_COLORS.items()]#mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "print(\"TMP\",len(tmp))\n",
    "offset=int(len(tmp)/100)\n",
    "#print(offset)\n",
    "mycolors=tmp[1::offset]\n",
    "customPalette=sns.set_palette(sns.color_palette(mycolors, as_cmap=True))\n",
    "print(mycolors)\n",
    "\n",
    "print(len(topics))\n",
    "df_subset=pd.DataFrame()\n",
    "df_subset['tsne-2d-one'] = tsne_lda[:,0]\n",
    "df_subset['tsne-2d-two'] = tsne_lda[:,1]\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue=topics,\n",
    "    palette=sns.color_palette(mycolors, 17),#sns.color_palette(\"PRGn\",100, as_cmap=True),\n",
    "    data=df_subset,\n",
    "    legend=\"full\",\n",
    "    alpha=1,\n",
    "    s=100\n",
    ")\n",
    "plt.savefig(f\"{name}_{tipo}_TSNE.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
